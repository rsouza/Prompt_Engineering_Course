{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c058a22-bd74-490d-a9e3-df2e2de7d144",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Quick intro to LlamaIndex  \n",
    "Sources: [1](https://lmy.medium.com/comparing-langchain-and-llamaindex-with-4-tasks-2970140edf33), [2](https://docs.llamaindex.ai/en/stable/), [3](https://github.com/run-llama/llama_index), [4](https://nanonets.com/blog/llamaindex/)  \n",
    "\n",
    "LlamaIndex is a \"data framework\" to help you build LLM apps. It provides the following tools:\n",
    "\n",
    "+ Offers data connectors to ingest your existing data sources and data formats (APIs, PDFs, docs, SQL, etc.).\n",
    "+ Provides ways to structure your data (indices, graphs) so that this data can be easily used with LLMs.\n",
    "+ Provides an advanced retrieval/query interface over your data: Feed in any LLM input prompt, get back retrieved context and knowledge-augmented output.\n",
    "+ Allows easy integrations with your outer application framework (e.g. with LangChain, Flask, Docker, ChatGPT, anything else).\n",
    "+ LlamaIndex provides tools for both beginner users and advanced users.  \n",
    "\n",
    "The high-level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code.  \n",
    "The lower-level APIs allow advanced users to customize and extend any module (data connectors, indices, retrievers, query engines, reranking modules), to fit their needs.  \n",
    "\n",
    "LlamaIndex provides the following tools:\n",
    "+ Data connectors ingest your existing data from their native source and format. These could be APIs, PDFs, SQL, and (much) more.\n",
    "+ Data indexes structure your data in intermediate representations that are easy and performant for LLMs to consume.\n",
    "+ Engines provide natural language access to your data. For example:\n",
    "+ Query engines are powerful retrieval interfaces for knowledge-augmented output.\n",
    "+ Chat engines are conversational interfaces for multi-message, “back and forth” interactions with your data.\n",
    "+ Data agents are LLM-powered knowledge workers augmented by tools, from simple helper functions to API integrations and more.\n",
    "+ Application integrations tie LlamaIndex back into the rest of your ecosystem. This could be LangChain, Flask, Docker, ChatGPT, or… anything else!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7f6931d-f17f-4b44-ad98-209eff067812",
     "showTitle": false,
     "title": "ro"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q openai==0.27.0\n",
    "#!pip install -qU llama-index            # Just the core components\n",
    "!pip install llama-index[local_models] # Installs tools useful for private LLMs, local inference, and HuggingFace models\n",
    "#!pip install llama-index[postgres]     # Is useful if you are working with Postgres, PGVector or Supabase\n",
    "#!pip install llama-index[query_tools]  # Gives you tools for hybrid search, structured outputs, and node post-processing\n",
    "# !pip install google-generativeai  #PALM\n",
    "!pip install -qU pypdf\n",
    "!pip install -qU docx2txt\n",
    "!pip install -qU sentence-transformers\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bb3df4f-2bfa-452b-b734-262f7d9abcd5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import glob\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "#import tiktoken\n",
    "#from funcy import lcat, lmap, linvoke\n",
    "#from IPython.display import display, Markdown\n",
    "import openai\n",
    "\n",
    "## LlamaIndex LLMs\n",
    "#from openai import OpenAI\n",
    "#from openai import AzureOpenAI\n",
    "from llama_index.llms import AzureOpenAI\n",
    "from llama_index.llms import ChatMessage\n",
    "from llama_index.llms import MessageRole\n",
    "from llama_index.chat_engine.condense_question import CondenseQuestionChatEngine\n",
    "#from llama_index.llms import Ollama\n",
    "#from llama_index.llms import PaLM\n",
    "\n",
    "## LlamaIndex Embeddings\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.embeddings import AzureOpenAIEmbedding\n",
    "from llama_index.embeddings import resolve_embed_model\n",
    "\n",
    "## Llamaindex readers \n",
    "from llama_index import SimpleDirectoryReader\n",
    "#from llama_index import ResponseSynthesizer\n",
    "\n",
    "## LlamaIndex Index Types\n",
    "#from llama_index import GPTListIndex             \n",
    "from llama_index import VectorStoreIndex\n",
    "#from llama_index import GPTVectorStoreIndex  \n",
    "#from llama_index import GPTTreeIndex\n",
    "#from llama_index import GPTKeywordTableIndex\n",
    "#from llama_index import GPTSimpleKeywordTableIndex\n",
    "#from llama_index import GPTDocumentSummaryIndex\n",
    "#from llama_index import GPTKnowledgeGraphIndex\n",
    "#from llama_index.indices.struct_store import GPTPandasIndex\n",
    "#from llama_index.vector_stores import ChromaVectorStore\n",
    "\n",
    "## LlamaIndex Context Managers\n",
    "from llama_index import ServiceContext\n",
    "from llama_index import StorageContext\n",
    "from llama_index import load_index_from_storage\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "#from llama_index import LLMPredictor\n",
    "\n",
    "## LlamaIndex Templates\n",
    "from llama_index.prompts import PromptTemplate\n",
    "from llama_index.prompts import ChatPromptTemplate\n",
    "\n",
    "## LlamaIndex Callbacks\n",
    "#from llama_index.callbacks import CallbackManager\n",
    "#from llama_index.callbacks import LlamaDebugHandler\n",
    "\n",
    "\n",
    "## Defining LLM Model\n",
    "llm_option = \"OpenAI\"\n",
    "if llm_option == \"OpenAI\":\n",
    "    openai.api_type = \"azure\"\n",
    "    azure_endpoint = \"https://rg-rbi-aa-aitest-dsacademy.openai.azure.com/\"\n",
    "    #azure_endpoint = \"https://chatgpt-summarization.openai.azure.com/\"\n",
    "    openai.api_version = \"2023-07-01-preview\"\n",
    "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "    deployment_name = \"model-gpt-35-turbo\"\n",
    "    openai_model_name = \"gpt-35-turbo\"\n",
    "    llm = AzureOpenAI(api_key=openai.api_key,\n",
    "                      azure_endpoint=azure_endpoint,\n",
    "                      model=openai_model_name,\n",
    "                      engine=deployment_name,\n",
    "                      api_version=openai.api_version,\n",
    "                      )\n",
    "elif llm_option == \"Local\":  #https://docs.llamaindex.ai/en/stable/module_guides/models/llms.html and https://docs.llamaindex.ai/en/stable/module_guides/models/llms/local.html\n",
    "    print(\"Make sure you have installed Local Models - !pip install llama-index[local_models]\")\n",
    "    llm = Ollama(model=\"mistral\", request_timeout=30.0)\n",
    "else:\n",
    "    raise ValueError(\"Invalid LLM Model\")\n",
    "\n",
    "## Defining Embedding Model\n",
    "emb_option = \"Local\"\n",
    "if emb_option == \"OpenAI\":\n",
    "    embed_model_name = \"text-embedding-ada-002\"\n",
    "    embed_model_deployment_name = \"model-text-embedding-ada-002\"\n",
    "    embed_model = AzureOpenAIEmbedding(model=embed_model_name,\n",
    "                                       deployment_name=embed_model_deployment_name,\n",
    "                                       api_key=openai.api_key,\n",
    "                                       azure_endpoint=azure_endpoint)\n",
    "elif emb_option == \"Local\":\n",
    "    embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\")   ## bge-m3 embedding model\n",
    "else:\n",
    "    raise ValueError(\"Invalid Embedding Model\")\n",
    "\n",
    "## Logging Optionals\n",
    "#logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "#logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ecbe217-c8a5-40eb-8228-6060db6949a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Text Completion Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87ae85fb-32ab-4230-9d6b-5a2d329408da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "resp = llm.complete(\"Paul Graham is \")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "001b0c93-3d49-431f-944f-2eadb153a61c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Chat Example Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58062725-56ee-4d14-aea1-293db9f40985",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "messages = [ChatMessage(role=\"system\", content=\"You are a pirate with a colorful personality\"),\n",
    "            ChatMessage(role=\"user\", content=\"What is your name\"),\n",
    "            ]\n",
    "resp = llm.chat(messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a1275f5-48c0-4e4f-acb9-6e0f848af08e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Quickstart: Implementing a RAG Pipeline:\n",
    "\n",
    "![](https://docs.llamaindex.ai/en/stable/_images/basic_rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca614505-2820-4fc3-894f-c6dbfa629999",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Examining Documents Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da840490-f086-4de4-ab76-ffa8abf92a50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DOCS_DIR = \"../../Data/txt/\"\n",
    "docs = os.listdir(DOCS_DIR)\n",
    "docs = [d for d in docs] # if d.endswith(\".txt\")]\n",
    "docs.sort()\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1d2f4a2-d2bc-41da-a75d-d51cfb7a68c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Setting the Service Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2856988c-f08a-45d2-adf9-945df50f3fb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm)\n",
    "set_global_service_context(service_context)\n",
    "\n",
    "#Testing Service Context\n",
    "service_context.llm.complete(\"RBI is a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "990ef080-353d-41ff-910d-cd1e31a3c4f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Creating the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39ec9553-2e9e-4f83-a9e1-e33084f93ff7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PERSIST_DIR = \"/Workspace/ds-academy-research/LLamaIndex_quick/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e82be14-3c9c-410e-a843-de9db7c05ded",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists(PERSIST_DIR):\n",
    "    shutil.rmtree(PERSIST_DIR)\n",
    "print(f\"Creating Directory {PERSIST_DIR}\")\n",
    "os.mkdir(PERSIST_DIR)\n",
    "\n",
    "if os.listdir(PERSIST_DIR) == []:\n",
    "    print(\"Loading Documents...\")\n",
    "    documents = SimpleDirectoryReader(DOCS_DIR).load_data()\n",
    "    print(\"Creating Vector Store...\")\n",
    "    index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "    print(\"Persisting Vector Store...\")\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a82211c0-d2bb-43c2-a660-a078e2933481",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Reading from existing Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e1cd3f0-570e-4788-bd0e-29c1efd6a321",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Reading from Vector Store...\")\n",
    "storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08794919-a98f-409c-ba71-14b6e1706f4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "index.ref_doc_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a128e44-0cb8-45ea-8719-afa61176cd12",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Querying your data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17998cb2-db67-478c-9078-2f9d6f3a1e7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(retriever_mode=\"embedding\", response_mode=\"accumulate\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2addd392-d887-4a92-a10f-d4e35b2672c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Who was Romeo?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3507e150-ef40-4283-9c08-baef0032ff9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Who did the proofreading of Pride and Prejudice?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd9c532a-f8ae-43ca-bb3d-929c8df0f432",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Who is the publisher of Pride and Prejudice?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd975ab9-b40c-4f0a-b475-b501b928150d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Chat with your Data  \n",
    "\n",
    "[Available Chat Modes](https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/usage_pattern.html)  \n",
    "+ best - Turn the query engine into a tool, for use with a ReAct data agent or an OpenAI data agent, depending on what your LLM supports. OpenAI data agents require gpt-3.5-turbo or gpt-4 as they use the function calling API from OpenAI.\n",
    "+ condense_question - Look at the chat history and re-write the user message to be a query for the index. Return the response after reading the response from the query engine.\n",
    "+ context - Retrieve nodes from the index using every user message. The retrieved text is inserted into the system prompt, so that the chat engine can either respond naturally or use the context from the query engine.\n",
    "+ condense_plus_context - A combination of condense_question and context. Look at the chat history and re-write the user message to be a retrieval query for the index. The retrieved text is inserted into the system prompt, so that the chat engine can either respond naturally or use the context from the query engine.\n",
    "+ simple - A simple chat with the LLM directly, no query engine involved.\n",
    "+ react - Same as best, but forces a ReAct data agent.\n",
    "+ openai - Same as best, but forces an OpenAI data agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cfcddcf-c5d7-479a-bcad-3e4de6e5dd08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chat_engine = index.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
    "chat_engine.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "619ba842-8357-4b98-aa55-bfa726c005fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\"What did happen to Romeo?\")\n",
    "print(response)\n",
    "response = chat_engine.chat(\"When was that?\")\n",
    "print(response)\n",
    "response = chat_engine.chat(\"What kind of poison?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c3d6bb1-c153-42de-945e-04144b335598",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Using  REPL interface  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2441412-b0fb-4532-9a35-8f5da8ba0e24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chat_engine.chat_repl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ac70a01-d36a-40a8-aa2b-a370f13caff1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Using [Prompt Templates](https://docs.llamaindex.ai/en/stable/examples/customization/prompts/completion_prompts.html)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc53e408-8fca-4712-b552-8bf6a816c650",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "With Text Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3a110d5-71ec-4238-88f3-de34981578c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "text_qa_template_str = (\n",
    "    \"Context information is\"\n",
    "    \" below.\\n---------------------\\n{context_str}\\n---------------------\\nUsing\"\n",
    "    \" both the context information and also using your own knowledge, answer\"\n",
    "    \" the question: {query_str}\\nIf the context isn't helpful, you can also\"\n",
    "    \" answer the question on your own.\\n\"\n",
    ")\n",
    "text_qa_template = PromptTemplate(text_qa_template_str)\n",
    "\n",
    "refine_template_str = (\n",
    "    \"The original question is as follows: {query_str}\\nWe have provided an\"\n",
    "    \" existing answer: {existing_answer}\\nWe have the opportunity to refine\"\n",
    "    \" the existing answer (only if needed) with some more context\"\n",
    "    \" below.\\n------------\\n{context_msg}\\n------------\\nUsing both the new\"\n",
    "    \" context and your own knowledge, update or repeat the existing answer.\\n\"\n",
    ")\n",
    "refine_template = PromptTemplate(refine_template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c6ca808-13aa-4f81-b58e-9844c6a33bbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(index.as_query_engine().query(\"Who is Bill Gates?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a54598a-e399-4fe4-98b7-fa951c7d99b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(index.as_query_engine(text_qa_template=text_qa_template, \n",
    "                            refine_template=refine_template).query(\"Who is Bill Gates?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "612813df-a969-4493-9d86-52e43cfcc30e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "With Chat Engine  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "172fc964-7e58-4394-b68c-67fe7201d0da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "custom_prompt = PromptTemplate(\n",
    "    \"\"\"\\\n",
    "Given a conversation (between Human and Assistant) and a follow up message from Human, \\\n",
    "rewrite the message to be a standalone question that captures all relevant context \\\n",
    "from the conversation.\n",
    "\n",
    "<Chat History>\n",
    "{chat_history}\n",
    "\n",
    "<Follow Up Message>\n",
    "{question}\n",
    "\n",
    "<Standalone question>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# list of `ChatMessage` objects\n",
    "custom_chat_history = [\n",
    "    ChatMessage(role=MessageRole.USER,\n",
    "                content=\"Hello assistant, we are having a insightful discussion about two famous romances today.\",\n",
    "                ),\n",
    "    ChatMessage(role=MessageRole.ASSISTANT, content=\"Okay, sounds good.\"),\n",
    "]\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "chat_engine = CondenseQuestionChatEngine.from_defaults(query_engine=query_engine,\n",
    "                                                       condense_question_prompt=custom_prompt,\n",
    "                                                       chat_history=custom_chat_history,\n",
    "                                                       verbose=True,)\n",
    "chat_engine.chat_repl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb3507d5-8685-42d1-8379-f811cc032bc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1-Intro_LLamaIndex",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
