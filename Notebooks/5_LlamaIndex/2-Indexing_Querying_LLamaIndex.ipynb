{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30a0635d-0c62-49ec-a325-88ce54ba9852",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Intro to LlamaIndex  \n",
    "Sources: [1](https://lmy.medium.com/comparing-langchain-and-llamaindex-with-4-tasks-2970140edf33), [2](https://docs.llamaindex.ai/en/stable/), [3](https://github.com/run-llama/llama_index), [4](https://nanonets.com/blog/llamaindex/)  \n",
    "\n",
    "#### Retrieval Augmented Generation (RAG)\n",
    "LLMs are trained on enormous bodies of data but they aren’t trained on your data. Retrieval-Augmented Generation (RAG) solves this problem by adding your data to the data LLMs already have access to. You will see references to RAG frequently in this documentation.  \n",
    "In RAG, your data is loaded and prepared for queries or “indexed”. User queries act on the index, which filters your data down to the most relevant context. This context and your query then go to the LLM along with a prompt, and the LLM provides a response.  \n",
    "Even if what you’re building is a chatbot or an agent, you’ll want to know RAG techniques for getting data into your application.  \n",
    "\n",
    "#### Stages within RAG\n",
    "There are five key stages within RAG, which in turn will be a part of any larger application you build. These are:\n",
    "+ Loading: this refers to getting your data from where it lives – whether it’s text files, PDFs, another website, a database, or an API – into your pipeline. LlamaHub provides hundreds of connectors to choose from.\n",
    "+ Indexing: this means creating a data structure that allows for querying the data. For LLMs this nearly always means creating vector embeddings, numerical representations of the meaning of your data, as well as numerous other metadata strategies to make it easy to accurately find contextually relevant data.\n",
    "+ Storing: once your data is indexed you will almost always want to store your index, as well as other metadata, to avoid having to re-index it.\n",
    "+ Querying: for any given indexing strategy there are many ways you can utilize LLMs and LlamaIndex data structures to query, including sub-queries, multi-step queries and hybrid strategies.\n",
    "+ Evaluation: a critical step in any pipeline is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures of how accurate, faithful and fast your responses to queries are.\n",
    "\n",
    "#### Important concepts within each step\n",
    "There are also some terms you’ll encounter that refer to steps within each of these stages.  \n",
    "+ Loading stage\n",
    "**Nodes** and **Documents**: A Document is a container around any data source - for instance, a PDF, an API output, or retrieve data from a database.  \n",
    "A Node is the atomic unit of data in LlamaIndex and represents a “chunk” of a source Document. Nodes have metadata that relate them to the document they are in and to other nodes.  \n",
    "**Connectors**: A data connector (often called a Reader) ingests data from different data sources and data formats into Documents and Nodes.  \n",
    "\n",
    "+ Indexing Stage  \n",
    "**Indexes**: Once you’ve ingested your data, LlamaIndex will help you index the data into a structure that’s easy to retrieve. This usually involves generating vector embeddings which are stored in a specialized database called a vector store. Indexes can also store a variety of metadata about your data.  \n",
    "**Embeddings** LLMs generate numerical representations of data called embeddings. When filtering your data for relevance, LlamaIndex will convert queries into embeddings, and your vector store will find data that is numerically similar to the embedding of your query.  \n",
    "\n",
    "+ Querying Stage\n",
    "**Retrievers**: A retriever defines how to efficiently retrieve relevant context from an index when given a query. Your retrieval strategy is key to the relevancy of the data retrieved and the efficiency with which it’s done.  \n",
    "**Routers**: A router determines which retriever will be used to retrieve relevant context from the knowledge base. More specifically, the RouterRetriever class, is responsible for selecting one or multiple candidate retrievers to execute a query. They use a selector to choose the best option based on each candidate’s metadata and the query.  \n",
    "Node Postprocessors: A node postprocessor takes in a set of retrieved nodes and applies transformations, filtering, or re-ranking logic to them.  \n",
    "Response Synthesizers: A response synthesizer generates a response from an LLM, using a user query and a given set of retrieved text chunks.  \n",
    "\n",
    "#### Putting it all together\n",
    "There are endless use cases for data-backed LLM applications but they can be roughly grouped into three categories:\n",
    "\n",
    "+ Query Engines: A query engine is an end-to-end pipeline that allows you to ask questions over your data. It takes in a natural language query, and returns a response, along with reference context retrieved and passed to the LLM.\n",
    "+ Chat Engines: A chat engine is an end-to-end pipeline for having a conversation with your data (multiple back-and-forth instead of a single question-and-answer).\n",
    "+ Agents: An agent is an automated decision-maker powered by an LLM that interacts with the world via a set of tools. Agents can take an arbitrary number of steps to complete a given task, dynamically deciding on the best course of action rather than following pre-determined steps. This gives it additional flexibility to tackle more complex tasks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fabe163-b043-407c-bd24-780e277a0e98",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Installing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c66c693-d00b-4efc-a585-eb95ad7d93eb",
     "showTitle": false,
     "title": "ro"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install -q openai==0.27.0\n",
    "#!pip install -qU llama-index[local_models]  # Installs tools useful for private LLMs, local inference, and HuggingFace models\n",
    "#!pip install -q llama-index[postgres]       # Is useful if you are working with Postgres, PGVector or Supabase\n",
    "#!pip install -q llama-index[query_tools]    # Gives you tools for hybrid search, structured outputs, and node post-processing\n",
    "!pip install -q llama-index==0.9.47                 # Just the core components  ## See: https://github.com/run-llama/llama_index/issues/10636\n",
    "#!pip install -qU chromadb\n",
    "!pip install -qU pypdf\n",
    "!pip install -qU docx2txt\n",
    "!pip install -qU sentence-transformers\n",
    "#!pip install -q aa-llm-utils\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a49d493c-7c36-4a40-8a4d-53c5648b8768",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fef0c9a7-a01e-48a6-9e69-b3d9a4fc4dc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import glob\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "#import tiktoken\n",
    "#from funcy import lcat, lmap, linvoke\n",
    "#from IPython.display import Markdown, display\n",
    "import openai\n",
    "#import chromadb\n",
    "\n",
    "## LlamaIndex LLMs\n",
    "#from openai import OpenAI\n",
    "#from openai import AzureOpenAI\n",
    "from llama_index.llms import AzureOpenAI\n",
    "#from llama_index.llms import ChatMessage\n",
    "#from llama_index.llms import MessageRole\n",
    "#from llama_index.llms import Ollama\n",
    "#from llama_index.llms import PaLM\n",
    "\n",
    "## LlamaIndex Embeddings\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.embeddings import AzureOpenAIEmbedding\n",
    "from llama_index.embeddings import resolve_embed_model\n",
    "\n",
    "## Llamaindex readers \n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "## LlamaIndex Index Types\n",
    "from llama_index import GPTListIndex             \n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index import GPTVectorStoreIndex  \n",
    "from llama_index import GPTTreeIndex\n",
    "from llama_index import GPTKeywordTableIndex\n",
    "from llama_index import GPTSimpleKeywordTableIndex\n",
    "from llama_index import GPTDocumentSummaryIndex\n",
    "from llama_index import GPTKnowledgeGraphIndex\n",
    "from llama_index.indices.struct_store import GPTPandasIndex\n",
    "#from llama_index.vector_stores import ChromaVectorStore\n",
    "\n",
    "## LlamaIndex Context Managers\n",
    "from llama_index import ServiceContext\n",
    "from llama_index import StorageContext\n",
    "from llama_index import load_index_from_storage\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.response_synthesizers import ResponseMode\n",
    "from llama_index.schema import Node\n",
    "#from llama_index import LLMPredictor\n",
    "\n",
    "## LlamaIndex Templates\n",
    "#from llama_index.prompts import PromptTemplate\n",
    "#from llama_index.prompts import ChatPromptTemplate\n",
    "\n",
    "## LlamaIndex Callbacks\n",
    "from llama_index.callbacks import CallbackManager\n",
    "from llama_index.callbacks import LlamaDebugHandler\n",
    "\n",
    "#from aa_llm_utils.utils import ensure_certificates\n",
    "#ensure_certificates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc0c6ccb-fe7d-4603-8a87-afce29d981b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Defining Model and Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa023fe2-37e1-48b6-a3f8-0da93fdec0c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Defining LLM Model\n",
    "## A full guide to using and configuring LLMs available here: https://docs.llamaindex.ai/en/stable/module_guides/models/llms.html\n",
    "## Check also: https://docs.llamaindex.ai/en/stable/module_guides/models/llms/local.html\n",
    "llm_option = \"OpenAI\"\n",
    "if llm_option == \"OpenAI\":\n",
    "    openai.api_type = \"azure\"\n",
    "    azure_endpoint = \"https://rg-rbi-aa-aitest-dsacademy.openai.azure.com/\"\n",
    "    #azure_endpoint = \"https://chatgpt-summarization.openai.azure.com/\"\n",
    "    openai.api_version = \"2023-07-01-preview\"\n",
    "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "    deployment_name = \"model-gpt-35-turbo\"\n",
    "    openai_model_name = \"gpt-35-turbo\"\n",
    "    llm = AzureOpenAI(api_key=openai.api_key,\n",
    "                      azure_endpoint=azure_endpoint,\n",
    "                      model=openai_model_name,\n",
    "                      engine=deployment_name,\n",
    "                      api_version=openai.api_version,\n",
    "                      )\n",
    "elif llm_option == \"Local\":  \n",
    "    print(\"Make sure you have installed Local Models - !pip install llama-index[local_models]\")\n",
    "    llm = Ollama(model=\"mistral\", request_timeout=30.0)\n",
    "else:\n",
    "    raise ValueError(\"Invalid LLM Model\")\n",
    "\n",
    "## Defining Embedding Model\n",
    "## A full guide to using and configuring embedding models is available here. https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html\n",
    "emb_option = \"OpenAI\"\n",
    "if emb_option == \"OpenAI\":\n",
    "    embed_model_name = \"text-embedding-ada-002\"\n",
    "    embed_model_deployment_name = \"model-text-embedding-ada-002\"\n",
    "    embed_model = AzureOpenAIEmbedding(model=embed_model_name,\n",
    "                                       deployment_name=embed_model_deployment_name,\n",
    "                                       api_key=openai.api_key,\n",
    "                                       azure_endpoint=azure_endpoint)\n",
    "elif emb_option == \"Local\":\n",
    "    embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\")   ## bge-m3 embedding model\n",
    "else:\n",
    "    raise ValueError(\"Invalid Embedding Model\")\n",
    "\n",
    "## Logging Optionals\n",
    "#logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "#logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "DOCS_DIR = \"/Workspace/ds-academy-research/Docs/LLM_sample/\"\n",
    "PERSIST_DIR = \"/Workspace/ds-academy-research/LLamaIndex/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60cbe6f9-f8e0-47ae-a6f2-268ebfc17ed4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### [ServiceContext](https://docs.llamaindex.ai/en/stable/api_reference/service_context.html) defines a handful of services and configurations used across a LlamaIndex pipeline.  \n",
    "+ Embeddings  \n",
    "+ OpenAIEmbedding  \n",
    "+ HuggingFaceEmbedding\n",
    "+ OptimumEmbedding\n",
    "+ InstructorEmbedding\n",
    "+ LangchainEmbedding\n",
    "+ GoogleUnivSentEncoderEmbedding\n",
    "+ [Node Parsers](https://docs.llamaindex.ai/en/stable/api_reference/service_context/node_parser.html)\n",
    "+ PromptHelper\n",
    "+ LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "157911ee-01f1-49b8-b7d0-454d6d2b915d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm,\n",
    "                                               #prompt_helper= ,\n",
    "                                               embed_model=embed_model,\n",
    "                                               #node_parser= ,\n",
    "                                               #chunk_size=1000,                                        #Parse Documents into smaller chunks\n",
    "                                               callback_manager=callback_manager,                       #Visualize execution\n",
    "                                               #system_prompt=(Optional[str]),                          #System-wide prompt to be prepended to all input prompts, used to guide system “decision making”\n",
    "                                               #query_wrapper_prompt=(Optional[BasePromptTemplate]),    #A format to wrap passed-in input queries.\n",
    "                                               )\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "949ec290-b02c-4e50-8f6e-46a8baa43e03",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Customizing a RAG Pipeline:\n",
    "\n",
    "![](https://docs.llamaindex.ai/en/stable/_images/basic_rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65febcd0-fae7-4268-983b-7d4ff55b07f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Creating the Index    \n",
    "\n",
    "LlamaIndex is known for offering [different types of indexes](https://docs.llamaindex.ai/en/stable/module_guides/indexing/indexing.html); each one is more suited to a different purpose.\n",
    "***\n",
    "There are important aspects to regard, namely: indexation cost, and indexation time (speed).  \n",
    "+ Indexing Cost: The expense of indexing is a crucial factor to consider. This is particularly significant when dealing with massive datasets.  \n",
    "+ Indexing Speed: The second important issue is the time of document indexing, i.e. preparing the entire solution for operation. Indexation time varies but it is a one-off and also depends on the OpenAI server.  \n",
    "Usually, the pdf with 40 pages will take approximately 5 seconds. Imagine a huge dataset with more than 100k pages, it could take to several days. We can leverage the async method to reduce the indexing time.  \n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*cyRHH_0z39JmFGeLYBWFEA.png)\n",
    "\n",
    "Sources: [1](https://betterprogramming.pub/llamaindex-how-to-use-index-correctly-6f928b8944c6), [2](https://docs.llamaindex.ai/en/stable/module_guides/indexing/indexing.html), [3](https://mikulskibartosz.name/llama-index-which-index-should-you-use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbc6c9de-cb5f-4d3d-8074-c6a79501f711",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### [Storage Context](https://docs.llamaindex.ai/en/stable/api_reference/storage.html)  \n",
    "LlamaIndex offers core abstractions around storage of Nodes, indices, and vectors. A key abstraction is the StorageContext - this contains the underlying BaseDocumentStore (for nodes), BaseIndexStore (for indices), and VectorStore (for vectors).\n",
    "StorageContext defines the storage backend for where the documents, embeddings, and indexes are stored.   \n",
    "```\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"<path/to/index>\")\n",
    "```\n",
    "You can learn more about [storage](https://docs.llamaindex.ai/en/stable/module_guides/storing/storing.html) and how to [customize](https://docs.llamaindex.ai/en/stable/module_guides/storing/customization.html) it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e28f8b14-d314-46c2-b36e-ca37bc1219cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Deleting existing Indexes  \n",
    "\n",
    "(Only if you want to recreate all indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31eb3c92-f79b-4202-865d-20edc7e1c07d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(PERSIST_DIR):\n",
    "    print(f\"Creating Directory {PERSIST_DIR}\")\n",
    "    os.mkdir(PERSIST_DIR)\n",
    "else:\n",
    "    print(f\"Re-Creating Directory {PERSIST_DIR}\")\n",
    "    #shutil.rmtree(PERSIST_DIR)\n",
    "    #os.mkdir(PERSIST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de984093-b255-47fc-99a3-9bf856e9a1fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Generic Function to create indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0184692b-e68b-4585-aa64-bec14876b25a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_retrieve_index(index_path, docs_path, index_type):\n",
    "    if not os.path.exists(index_path):\n",
    "        print(f\"Creating Directory {index_path}\")\n",
    "        os.mkdir(index_path)\n",
    "    if os.listdir(index_path) == []:\n",
    "        print(\"Loading Documents...\")\n",
    "        documents = SimpleDirectoryReader(docs_path).load_data()\n",
    "        print(\"Creating Index...\")\n",
    "        index = index_type.from_documents(documents, \n",
    "                                          service_context=service_context, \n",
    "                                          show_progress=True,\n",
    "                                          )\n",
    "        print(\"Persisting Index...\")\n",
    "        index.storage_context.persist(persist_dir=index_path)\n",
    "        print(\"Done!\")\n",
    "    else:\n",
    "        print(\"Reading from Index...\")\n",
    "        index = load_index_from_storage(storage_context=StorageContext.from_defaults(persist_dir=index_path))\n",
    "        print(\"Done!\")\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9d034e8-2d70-4886-9077-aa0c04c13f30",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Some remarks:\n",
    "+ We will load documens from a directory, but you can check all integrations (readers) [here](https://llamahub.ai/?tab=loaders)  \n",
    "+ We could also transform documents in nodes and create the index directly from [nodes](https://docs.llamaindex.ai/en/stable/api_reference/service_context/node_parser.html)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b083ee2c-9f21-4dd0-a089-b438610007a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### A) Creating (or loading) List Index  \n",
    "~ 1 min   \n",
    "\n",
    "The list index is a simple data structure where nodes are stored in a sequence.  \n",
    "The document texts are chunked up, converted to nodes, and stored in a list during index construction.\n",
    "The GPTListIndex index is perfect when you don’t have many documents. Instead of trying to find the relevant data, the index concatenates all chunks and sends them all to the LLM. If the resulting text is too long, the index splits the text and asks LLM to refine the answer.  \n",
    "GPTListIndex may be a good choice when we have a few questions to answer using a handful of documents. It may give us the best answer because AI will get all the available data, but it is also quite expensive. We pay per token, so sending all the documents to the LLM may not be the best idea.  \n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/0*rBBHy019pbV9kyxh.png)\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/0*8ANcn6OBBVzIHAd0.png)\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/0*NQAUXYHPq0wh8zhw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a412ae8e-430d-4f26-9311-8fc7e9f53926",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "LISTINDEXDIR = PERSIST_DIR + 'ListIndex' \n",
    "listindex = create_retrieve_index(LISTINDEXDIR, DOCS_DIR, GPTListIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d4d11e0-7b44-4f5e-bffa-05458dba0d54",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### B) Creating (or loading) Vector Store Index  \n",
    "~ 3 min (Local) / 7 min (OpenAI)  \n",
    "\n",
    "It is most common and simple to use, allows answering a query over a large corpus of data  \n",
    "By default, LlamaIndex uses a simple in-memory vector store, but you can use [another solution](https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores.html)   \n",
    "VectorStoreIndex creates numerical vectors from the text using word embeddings and retrieves relevant documents based on the similarity of the vectors.  \n",
    "When we index the documents, the library chunks them into a number of nodes and calls the embeddings endpoint of OpenAI API by default.  Unlike list index, vector-store based indices generate embeddings during index construction  \n",
    "The number of API calls during indexing depends on the amount of data. GPTVectorStoreIndex can use the embeddings API or a Local Model.\n",
    "When we ask a question, it will create a vector from the question, retrieve relevant data, and pass the text to the LLM. The LLM will generate the answer using our question and the retrieved documents. Using GPTVectorStoreIndex, we can implement the most popular method of passing private data to LLMs which is to create vectors using word embeddings and find relevant documents based on the similarity between the documents and the question.  It has an obvious advantage. It is cheap to index and retrieve the data. We can also reuse the index to answer multiple questions without sending the documents to LLM many times. The disadvantage is that the quality of the answers depends on the quality of the embeddings. If the embeddings are not good enough, the LLM will not be able to generate a good responses.  \n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/0*IbHJovGnj38dDHsB.png)\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/0*-9QtrMEBYrAFWDMH.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d72f4459-7096-48e8-8bdf-caf1b857ddc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "VECTORINDEXDIR = PERSIST_DIR + 'VectorStoreIndex' \n",
    "vectorstoreindex = create_retrieve_index(VECTORINDEXDIR, DOCS_DIR, VectorStoreIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20a7fded-0df1-42ee-b1dc-da78a9c721d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### C) Creating (or loading) Tree Index  \n",
    "~ 3 min  \n",
    "It is useful for summarizing a collection of documents  \n",
    "The tree index is a tree-structured index, where each node is a summary of the children's nodes.  \n",
    "During index construction, the tree is constructed in a bottoms-up fashion until we end up with a set of root nodes.  \n",
    "The tree index builds a hierarchical tree from a set of Nodes (which become leaf nodes in this tree).  \n",
    "Unlike vector index, LlamaIndex won’t call LLM to generate embedding but will generate it during query time.   \n",
    "Embeddings are lazily generated and then cached (if retriever_mode=\"embedding\" is specified during query(...)), and not during index construction.  \n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/0*906uyjc0HBDfiyzw.png)  \n",
    "![](https://miro.medium.com/v2/resize:fit:1100/format:webp/0*CpUvD5VejES-JdRq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eabfd1f-79e3-4e2f-89ba-e61f938227d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TREEINDEXDIR = PERSIST_DIR + 'TreeIndex' \n",
    "treeindex = create_retrieve_index(TREEINDEXDIR, DOCS_DIR, GPTTreeIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00bf881c-bac4-4c9d-bb75-9e58e655b158",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### D) Creating (or loading) Keyword Table Indexes (embeddings)  \n",
    "~ 14 min (Local) / 12 min (OpenAI)  \n",
    "\n",
    "It is useful for routing queries to the disparate data source  \n",
    "The keyword table index extracts keywords from each Node and builds a mapping from each keyword to the corresponding Nodes of that keyword.  \n",
    "During query time, we extract relevant keywords from the query and match those with pre-extracted Node keywords to fetch the corresponding Nodes. The extracted Nodes are passed to the Response Synthesis module. GPTKeywordTableIndex use LLM to extract keywords from each document, meaning it do require LLM calls during build time. However, if you use GPTSimpleKeywordTableIndex which uses a regex keyword extractor to extract keywords from each document, it won’t call LLM during build time  \n",
    "The bulk of the work happens at the indexing time. Every node is sent to the LLM to generate keywords, and sending every document to an LLM increases the cost of indexing. Not only because we pay for the tokens but also because calls to the Completion API of OpenAI take longer than their Embeddings API. \n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/0*DUR4yHaMam-vln3t.png)\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/0*ERSNFpKoKfbIICkz.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff4a410e-c54b-4a73-ac68-518d5b0185a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "KEYWORDINDEXDIR = PERSIST_DIR + 'KeywordIndex' \n",
    "keywordindex = create_retrieve_index(KEYWORDINDEXDIR, DOCS_DIR, GPTKeywordTableIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63b37d3f-f699-4e7f-ac7f-127c85f4375e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#keywordindex.index_struct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "057342e7-b777-43f3-a078-49040d906e27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### D2) Creating (or loading) Simple Keyword Table Indexes (regex)  \n",
    "~ 1 min  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2f35c49-887a-42c5-962a-57c8a81de349",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SIMPLEKEYWORDINDEXDIR = PERSIST_DIR + 'SimpleKeywordIndex' \n",
    "simplekeywordindex = create_retrieve_index(SIMPLEKEYWORDINDEXDIR, DOCS_DIR, GPTSimpleKeywordTableIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e32d175-b174-49c4-87a5-78df7de22418",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#simplekeywordindex.index_struct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b8161b9-e090-486d-8ee5-207043f31488",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### E) Creating (or loading) Document Summary Index  \n",
    "~ 19 min (Local) / 17 min (OpenAI)  \n",
    "\n",
    "This index can extract and index an unstructured text summary for each document, which enhances retrieval performance beyond existing approaches. It contains more information than a single text chunk and carries more semantic meaning than keyword tags. It also allows for flexible retrieval, including both LLM and embedding-based approaches. During build-time, this index ingests document and use LLM to extract a summary from each document. During query time, it retrieves relevant documents to query based on summaries using the following approaches:  \n",
    "+ LLM-based Retrieval: get collections of document summaries and request LLM to identify the relevant documents + relevance score  \n",
    "+ Embedding-based Retrieval: utilize summary embedding similarity to retrieve relevant documents, and impose a top-k limit to the number of retrieved results.  \n",
    "\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/0*Sr1_53f_HAXwbsQ5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19e176d7-2898-4720-8b96-b5ad693bec7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DSUMMARYINDEXDIR = PERSIST_DIR + 'DSummaryIndex' \n",
    "dsummaryindex = create_retrieve_index(DSUMMARYINDEXDIR, DOCS_DIR, GPTDocumentSummaryIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f0966cf-61e2-4549-ba5b-150e650518c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### F) Creating (or loading) Knowledge Graph Index  \n",
    "~ 21 min  (Local) / 17 min (OpenAI)\n",
    "\n",
    "It builds a knowledge graph with keywords and relations between nodes, consuming a lot of resources.  \n",
    "The default behavior of GPTKnowledgeGraphIndex is based on keywords, but we can use embeddings by specifying the retriever_mode parameter (KGRetrieverMode.EMBEDDING)  \n",
    "It builds the index by extracting knowledge triples in the form (subject, predicate, object) over a set of docs. During the query time, it can either query using just the knowledge graph as context or leverage the underlying text from each entity as context. By leveraging the underlying text, we can ask more complicated queries with respect to the contents of the document.  \n",
    "With LlamaIndex, you have the ability to create composite indices by building indices on top of existing ones. This feature empowers you to efficiently index your complete document hierarchy and provide tailored knowledge to GPT.\n",
    "By leveraging composability, you can define indices at multiple levels, such as lower-level indices for individual documents and higher-level indices for groups of documents.  \n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*rEg1wqA7V7HXUWy4LP6zXQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35c8064e-b16c-478b-97bd-a47064e224ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "KGGRAPHINDEXDIR = PERSIST_DIR + 'KGraphIndex' \n",
    "kgraphindex = create_retrieve_index(KGGRAPHINDEXDIR, DOCS_DIR, GPTKnowledgeGraphIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9234cbe7-dbf1-4b68-9096-5748a6e2ef6c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Retrieving your data  \n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/0*XqAckrehpK4MYt35.jpg)\n",
    "\n",
    "LlamaIndex provides a high-level API that facilitates straightforward querying, ideal for common use cases.\n",
    "```\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"your_query\")\n",
    "print(response)\n",
    "```\n",
    "\n",
    "```as_query_engine``` builds a default retriever and query engine on top of the index.  \n",
    "You can check check out the query engine, chat engine and agents sections [here](https://docs.llamaindex.ai/en/stable/module_guides/querying/querying.html)\n",
    "\n",
    "We can try different [response modes](https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/response_modes.html)  \n",
    "\n",
    "+ refine: create and refine an answer by sequentially going through each retrieved text chunk. This makes a separate LLM call per Node/retrieved chunk.\n",
    "Details: the first chunk is used in a query using the text_qa_template prompt. Then the answer and the next chunk (as well as the original question) are used in another query with the refine_template prompt. And so on until all chunks have been parsed. If a chunk is too large to fit within the window (considering the prompt size), it is split using a TokenTextSplitter (allowing some text overlap between chunks) and the (new) additional chunks are considered as chunks of the original chunks collection (and thus queried with the refine_template as well). Good for more detailed answers.\n",
    "\n",
    "+ compact (default): similar to refine but compact (concatenate) the chunks beforehand, resulting in less LLM calls.\n",
    "Details: stuff as many text (concatenated/packed from the retrieved chunks) that can fit within the context window (considering the maximum prompt size between text_qa_template and refine_template). If the text is too long to fit in one prompt, it is split in as many parts as needed (using a TokenTextSplitter and thus allowing some overlap between text chunks). Each text part is considered a “chunk” and is sent to the refine synthesizer. In short, it is like refine, but with less LLM calls.\n",
    "\n",
    "+ tree_summarize: Query the LLM using the summary_template prompt as many times as needed so that all concatenated chunks have been queried, resulting in as many answers that are themselves recursively used as chunks in a tree_summarize LLM call and so on, until there’s only one chunk left, and thus only one final answer.\n",
    "Details: concatenate the chunks as much as possible to fit within the context window using the summary_template prompt, and split them if needed (again with a TokenTextSplitter and some text overlap). Then, query each resulting chunk/split against summary_template (there is no refine query !) and get as many answers. If there is only one answer (because there was only one chunk), then it’s the final answer.\n",
    "If there are more than one answer, these themselves are considered as chunks and sent recursively to the tree_summarize process (concatenated/splitted-to-fit/queried). Good for summarization purposes.\n",
    "\n",
    "+ simple_summarize: Truncates all text chunks to fit into a single LLM prompt. Good for quick summarization purposes, but may lose detail due to truncation.\n",
    "+ no_text: Only runs the retriever to fetch the nodes that would have been sent to the LLM, without actually sending them. Then can be inspected by checking response.source_nodes.\n",
    "+ accumulate: Given a set of text chunks and the query, apply the query to each text chunk while accumulating the responses into an array. Returns a concatenated string of all responses. Good for when you need to run the same query separately against each text chunk.\n",
    "+ compact_accumulate: The same as accumulate, but will “compact” each LLM prompt similar to compact, and run the same query against each text chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22ef23b2-d9d7-4d94-9c04-74c00abe7391",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### A) Retrieving from [List Index](https://docs.llamaindex.ai/en/stable/api_reference/query/retrievers/list.html)  \n",
    "~ 5 min\n",
    "\n",
    "LlamaIndex provides embedding support to list indices.  \n",
    "In addition to each node storing text, each node can optionally store an embedding. During query time, we can use embeddings to do max-similarity retrieval of nodes before calling the LLM to synthesize an answer.  \n",
    "Since similarity lookup using embeddings (e.g. using cosine similarity) does not require an LLM call, embeddings serve as a cheaper lookup mechanism instead of using LLMs to traverse nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d854cba-b107-4f44-8e54-020eefbe8ecc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_engine = listindex.as_query_engine(similarity_top_k=2,\n",
    "                                         keyword_filter=[\"Raiffeisen\", \"Generative AI\"],\n",
    "                                         response_mode=\"accumulate\",\n",
    "                                         verbose=True,\n",
    "                                         )\n",
    "response = query_engine.query(\"What industries demand upskilling regarding GenAI?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b36da93-f926-47b0-a454-1edf5bddd44c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Will GenAI create new jobs?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86a906f1-f993-4959-940b-2d66d39ccd0f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### B) Retrieving from [Vector Store Index](https://docs.llamaindex.ai/en/stable/api_reference/query/retrievers/vector_store.html)  \n",
    "~ 2 Seconds  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2be41af-ab0e-4d10-8c40-632eb1c7a610",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_engine = vectorstoreindex.as_query_engine(retriever_mode=\"embedding\",\n",
    "                                                response_mode=\"compact\",\n",
    "                                                verbose=True)\n",
    "response = query_engine.query(\"What industries demand upskilling regarding GenAI?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "721b2780-495b-4677-8feb-7b9732776157",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Will GenAI create new jobs?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e99b2a70-14eb-4f04-a52c-fac7caabc598",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### C) Retrieving from [Tree Index](https://docs.llamaindex.ai/en/stable/api_reference/query/retrievers/tree.html)\n",
    "~ 2.5 min  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e93e61b-26c2-4c2f-87ed-66a96a6444fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "query_engine = treeindex.as_query_engine(response_mode=\"tree_summarize\",  \n",
    "                                         retriever_mode=\"all_leaf\",  \n",
    "                                         child_branch_factor=1,\n",
    "                                         verbose=True\n",
    "                                        )\n",
    "response = query_engine.query(\"What industries demand upskilling regarding GenAI?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd2e4b28-2fa4-4a1d-8224-143c6fe36b4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Will GenAI create new jobs?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96392495-0554-4d67-8317-6fd591f968d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### D) Retrieving from [SimpleKeywordTableIndex and KeywordTableIndex](https://docs.llamaindex.ai/en/stable/api_reference/query/retrievers/table.html)  \n",
    "~ 3 seconds  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc34d86f-0163-43dd-bbdb-581c79aa3ad1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_engine = simplekeywordindex.as_query_engine(verbose=True,\n",
    "                                                  response_mode=\"compact\",\n",
    "                                                  )\n",
    "response = query_engine.query(\"What industries demand upskilling regarding GenAI?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec7cdad3-795a-4b3a-8cbf-abbc55f45056",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_engine = keywordindex.as_query_engine(verbose=True,\n",
    "                                            response_mode=\"compact\",\n",
    "                                        )\n",
    "response = query_engine.query(\"What industries demand upskilling regarding GenAI?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7cb1022-e36d-44bc-8594-ee2d014911b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Will GenAI create new jobs?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27f4c558-ea5c-435a-98f3-b56120a54800",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Will GenAI create new jobs?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ca7570e-3dfc-4c9d-b621-c151b4b071d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### E) Retrieving from [Document Summary Index](https://docs.llamaindex.ai/en/latest/examples/index_structs/doc_summary/DocSummary.htmll)  \n",
    "~ 1 second  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7974adfd-0e30-4a9a-947e-8a7001baca50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_engine = dsummaryindex.as_query_engine(verbose=True,\n",
    "                                           response_mode=\"compact\",\n",
    "                                           )\n",
    "response = query_engine.query(\"What industries demand upskilling regarding GenAI?\")\n",
    "print(response)\n",
    "\n",
    "#response = response_synthesizer.synthesize(\"query text\", nodes=[Node(text=\"text\"), ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5be79485-3bf7-48db-8956-50c6d029e5be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "list(dsummaryindex.ref_doc_info.keys())[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fa428d4-cd37-42a8-8c77-99dafdaff5a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dsummaryindex.get_document_summary(doc_id=list(dsummaryindex.ref_doc_info.keys())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c55e0ecb-096c-4f2d-92c1-6c629dbce2d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### F) Retrieving from [Knowledge Graph Index](https://docs.llamaindex.ai/en/stable/examples/index_structs/knowledge_graph/KnowledgeGraphDemo.html)  \n",
    "~ 1 second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca586ad9-7e37-4a11-bf12-f83fd93b955f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_engine = kgraphindex.as_query_engine(verbose=True,\n",
    "                                           response_mode=\"compact\",\n",
    "                                           )\n",
    "response = query_engine.query(\"What industries demand upskilling regarding GenAI?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3f1b7e6-9afe-455e-ab35-fbfc937dd507",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "GPTKnowledgeGraphIndex comes with an additional benefit. We can retrieve the knowledge graph, and we can even visualize it with the networkx library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e3be9d2-1703-47e6-9a89-519ae9246341",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(1, figsize=(100, 40), dpi=50)\n",
    "nx.draw_networkx(kgraphindex.get_networkx_graph(), font_size=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd7a760e-f9b8-47ab-9526-168dd8dfd0b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### F) Pandas Index\n",
    "\n",
    "Good to query Dataframes  \n",
    "Error: https://stackoverflow.com/questions/77445728/pandasqueryengine-from-llama-index-is-unable-to-execute-code-with-the-following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dc4fc4d-619a-4e96-8ddc-b24401896924",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../Data/csv/bank_data.csv\")\n",
    "index = GPTPandasIndex(df=df) #, service_context=service_context)\n",
    "query_engine = index.as_query_engine(verbose=True)\n",
    "response = query_engine.query(\"What is the size of the dataframe?\",)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebbe7430-0e34-45a3-9729-bf988a61ee77",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### G) SQL Index:\n",
    "You can attach your LLM app to your database and ask questions on top of it.  \n",
    "##### (This is a non-functional example code)  \n",
    "\n",
    "```\n",
    "!pip install wikipedia\n",
    "\n",
    "from llama_index import SimpleDirectoryReader, WikipediaReader\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, String, Integer, select, column\n",
    "\n",
    "wiki_docs = WikipediaReader().load_data(pages=['Toronto', 'Berlin', 'Tokyo'])\n",
    "\n",
    "engine = create_engine(\"sqlite:///:memory:\")\n",
    "metadata_obj = MetaData()\n",
    "\n",
    "# create city SQL table\n",
    "table_name = \"city_stats\"\n",
    "city_stats_table = Table(\n",
    "    table_name,\n",
    "    metadata_obj,\n",
    "    Column(\"city_name\", String(16), primary_key=True),\n",
    "    Column(\"population\", Integer),\n",
    "    Column(\"country\", String(16), nullable=False),\n",
    ")\n",
    "metadata_obj.create_all(engine)\n",
    "\n",
    "from llama_index import GPTSQLStructStoreIndex, SQLDatabase, ServiceContext\n",
    "from langchain import OpenAI\n",
    "from llama_index import LLMPredictor\n",
    "\n",
    "llm_predictor = LLMPredictor(llm=LLMPredictor(llm=ChatOpenAI(temperature=0, max_tokens=512, model_name='gpt-3.5-turbo')))\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
    "\n",
    "sql_database = SQLDatabase(engine, include_tables=[\"city_stats\"])\n",
    "sql_database.table_info\n",
    "\n",
    "# NOTE: the table_name specified here is the table that you\n",
    "# want to extract into from unstructured documents.\n",
    "index = GPTSQLStructStoreIndex.from_documents(\n",
    "    wiki_docs, \n",
    "    sql_database=sql_database, \n",
    "    table_name=\"city_stats\",\n",
    "    service_context=service_context\n",
    ")\n",
    "\n",
    "# view current table to verify the answer later\n",
    "stmt = select(\n",
    "    city_stats_table.c[\"city_name\", \"population\", \"country\"]\n",
    ").select_from(city_stats_table)\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    results = connection.execute(stmt).fetchall()\n",
    "    print(results)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    query_mode=\"nl\"\n",
    ")\n",
    "response = query_engine.query(\"Which city has the highest population?\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1887f9b6-15ed-4e3b-90b0-bec1951d86ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2-Indexing_Querying_LLamaIndex",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
