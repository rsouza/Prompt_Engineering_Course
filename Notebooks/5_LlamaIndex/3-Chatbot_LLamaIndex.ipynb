{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e664a6a8-404e-4a51-b017-d52a0159f34d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Intro to LlamaIndex  \n",
    "Sources: [1](https://lmy.medium.com/comparing-langchain-and-llamaindex-with-4-tasks-2970140edf33), [2](https://docs.llamaindex.ai/en/stable/), [3](https://github.com/run-llama/llama_index), [4](https://nanonets.com/blog/llamaindex/)  \n",
    "\n",
    "#### Retrieval Augmented Generation (RAG)\n",
    "LLMs are trained on enormous bodies of data but they aren’t trained on your data. Retrieval-Augmented Generation (RAG) solves this problem by adding your data to the data LLMs already have access to. You will see references to RAG frequently in this documentation.  \n",
    "In RAG, your data is loaded and prepared for queries or “indexed”. User queries act on the index, which filters your data down to the most relevant context. This context and your query then go to the LLM along with a prompt, and the LLM provides a response.  \n",
    "Even if what you’re building is a chatbot or an agent, you’ll want to know RAG techniques for getting data into your application.  \n",
    "\n",
    "#### Stages within RAG\n",
    "There are five key stages within RAG, which in turn will be a part of any larger application you build. These are:\n",
    "+ Loading: this refers to getting your data from where it lives – whether it’s text files, PDFs, another website, a database, or an API – into your pipeline. LlamaHub provides hundreds of connectors to choose from.\n",
    "+ Indexing: this means creating a data structure that allows for querying the data. For LLMs this nearly always means creating vector embeddings, numerical representations of the meaning of your data, as well as numerous other metadata strategies to make it easy to accurately find contextually relevant data.\n",
    "+ Storing: once your data is indexed you will almost always want to store your index, as well as other metadata, to avoid having to re-index it.\n",
    "+ Querying: for any given indexing strategy there are many ways you can utilize LLMs and LlamaIndex data structures to query, including sub-queries, multi-step queries and hybrid strategies.\n",
    "+ Evaluation: a critical step in any pipeline is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures of how accurate, faithful and fast your responses to queries are.\n",
    "\n",
    "#### Important concepts within each step\n",
    "There are also some terms you’ll encounter that refer to steps within each of these stages.  \n",
    "+ Loading stage\n",
    "**Nodes** and **Documents**: A Document is a container around any data source - for instance, a PDF, an API output, or retrieve data from a database.  \n",
    "A Node is the atomic unit of data in LlamaIndex and represents a “chunk” of a source Document. Nodes have metadata that relate them to the document they are in and to other nodes.  \n",
    "**Connectors**: A data connector (often called a Reader) ingests data from different data sources and data formats into Documents and Nodes.  \n",
    "\n",
    "+ Indexing Stage  \n",
    "**Indexes**: Once you’ve ingested your data, LlamaIndex will help you index the data into a structure that’s easy to retrieve. This usually involves generating vector embeddings which are stored in a specialized database called a vector store. Indexes can also store a variety of metadata about your data.  \n",
    "**Embeddings** LLMs generate numerical representations of data called embeddings. When filtering your data for relevance, LlamaIndex will convert queries into embeddings, and your vector store will find data that is numerically similar to the embedding of your query.  \n",
    "\n",
    "+ Querying Stage\n",
    "**Retrievers**: A retriever defines how to efficiently retrieve relevant context from an index when given a query. Your retrieval strategy is key to the relevancy of the data retrieved and the efficiency with which it’s done.  \n",
    "**Routers**: A router determines which retriever will be used to retrieve relevant context from the knowledge base. More specifically, the RouterRetriever class, is responsible for selecting one or multiple candidate retrievers to execute a query. They use a selector to choose the best option based on each candidate’s metadata and the query.  \n",
    "Node Postprocessors: A node postprocessor takes in a set of retrieved nodes and applies transformations, filtering, or re-ranking logic to them.  \n",
    "Response Synthesizers: A response synthesizer generates a response from an LLM, using a user query and a given set of retrieved text chunks.  \n",
    "\n",
    "#### Putting it all together\n",
    "There are endless use cases for data-backed LLM applications but they can be roughly grouped into three categories:\n",
    "\n",
    "+ Query Engines: A query engine is an end-to-end pipeline that allows you to ask questions over your data. It takes in a natural language query, and returns a response, along with reference context retrieved and passed to the LLM.\n",
    "+ Chat Engines: A chat engine is an end-to-end pipeline for having a conversation with your data (multiple back-and-forth instead of a single question-and-answer).\n",
    "+ Agents: An agent is an automated decision-maker powered by an LLM that interacts with the world via a set of tools. Agents can take an arbitrary number of steps to complete a given task, dynamically deciding on the best course of action rather than following pre-determined steps. This gives it additional flexibility to tackle more complex tasks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3870825c-294f-4209-b7fd-0b575e426efe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Installing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ff6e701-2f86-4569-a844-87e281df3140",
     "showTitle": false,
     "title": "ro"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatabricks-feature-engineering 0.2.0 requires pyspark<4,>=3.1.2, which is not installed.\ngoogle-auth 2.21.0 requires urllib3<2.0, but you have urllib3 2.2.0 which is incompatible.\ndatabricks-sdk 0.1.6 requires requests<2.29.0,>=2.28.1, but you have requests 2.31.0 which is incompatible.\nbotocore 1.27.96 requires urllib3<1.27,>=1.25.4, but you have urllib3 2.2.0 which is incompatible.\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install -q openai==0.27.0\n",
    "!pip install -qU llama-index            # Just the core components\n",
    "#!pip install -qU llama-index[local_models] # Installs tools useful for private LLMs, local inference, and HuggingFace models\n",
    "#!pip install -q llama-index[postgres]     # Is useful if you are working with Postgres, PGVector or Supabase\n",
    "#!pip install -q llama-index[query_tools]  # Gives you tools for hybrid search, structured outputs, and node post-processing\n",
    "!pip install -q llama-hub \n",
    "#!pip install -qU chromadb\n",
    "!pip install -qU pypdf\n",
    "!pip install -qU docx2txt\n",
    "!pip install -qU sentence-transformers\n",
    "!pip install -q unstructured\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "656e5f42-85bb-4cd0-b856-49150d512ee1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7f5e7dc-ad27-4669-9d7f-f4e3fa9b893d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import glob\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "#import tiktoken\n",
    "#from funcy import lcat, lmap, linvoke\n",
    "#from IPython.display import Markdown, display\n",
    "import openai\n",
    "#import chromadb\n",
    "\n",
    "#OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]  #It has to be defined before importing LlamaIndex modules\n",
    "\n",
    "## LlamaIndex LLMs\n",
    "#from openai import OpenAI\n",
    "#from openai import AzureOpenAI\n",
    "from llama_index.llms import AzureOpenAI\n",
    "#from llama_index.llms import Ollama\n",
    "#from llama_index.llms import PaLM\n",
    "\n",
    "## LlamaIndex Embeddings\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.embeddings import AzureOpenAIEmbedding\n",
    "from llama_index.embeddings import resolve_embed_model\n",
    "\n",
    "## Llamaindex readers \n",
    "#from llama_index import SimpleDirectoryReader\n",
    "from llama_hub.file.unstructured.base import UnstructuredReader\n",
    "\n",
    "## LlamaIndex Index Types\n",
    "#from llama_index import GPTListIndex             \n",
    "from llama_index import VectorStoreIndex\n",
    "#from llama_index import GPTVectorStoreIndex  \n",
    "#from llama_index import GPTTreeIndex\n",
    "#from llama_index import GPTKeywordTableIndex\n",
    "#from llama_index import GPTSimpleKeywordTableIndex\n",
    "#from llama_index import GPTDocumentSummaryIndex\n",
    "#from llama_index import GPTKnowledgeGraphIndex\n",
    "#from llama_index.indices.struct_store import GPTPandasIndex\n",
    "#from llama_index.vector_stores import ChromaVectorStore\n",
    "\n",
    "## LlamaIndex Context Managers\n",
    "from llama_index import ServiceContext\n",
    "from llama_index import StorageContext\n",
    "from llama_index import load_index_from_storage\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.response_synthesizers import ResponseMode\n",
    "from llama_index.schema import Node\n",
    "#from llama_index import LLMPredictor\n",
    "\n",
    "## LlamaIndex Tools\n",
    "from llama_index.tools import QueryEngineTool\n",
    "from llama_index.tools import ToolMetadata\n",
    "from llama_index.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.chat_engine import SimpleChatEngine\n",
    "\n",
    "## LlamaIndex Agents\n",
    "from llama_index.agent import OpenAIAgent\n",
    "\n",
    "## LlamaIndex Callbacks\n",
    "from llama_index.callbacks import CallbackManager\n",
    "from llama_index.callbacks import LlamaDebugHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cf9b236-7b3b-4c86-a12f-28ffa334d575",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Defining Model and Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f2ba122-e919-4768-840a-03db5ccc0e8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Defining LLM Model\n",
    "## A full guide to using and configuring LLMs available here: https://docs.llamaindex.ai/en/stable/module_guides/models/llms.html\n",
    "## Check also: https://docs.llamaindex.ai/en/stable/module_guides/models/llms/local.html\n",
    "llm_option = \"OpenAI\"\n",
    "if llm_option == \"OpenAI\":\n",
    "    openai.api_type = \"azure\"\n",
    "    azure_endpoint = \"https://rg-rbi-aa-aitest-dsacademy.openai.azure.com/\"\n",
    "    #azure_endpoint = \"https://chatgpt-summarization.openai.azure.com/\"\n",
    "    openai.api_version = \"2023-07-01-preview\"\n",
    "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "    deployment_name = \"model-gpt-35-turbo\"\n",
    "    openai_model_name = \"gpt-35-turbo\"\n",
    "    llm = AzureOpenAI(api_key=openai.api_key,\n",
    "                      azure_endpoint=azure_endpoint,\n",
    "                      model=openai_model_name,\n",
    "                      engine=deployment_name,\n",
    "                      api_version=openai.api_version,\n",
    "                      )\n",
    "elif llm_option == \"Local\":  \n",
    "    print(\"Make sure you have installed Local Models - !pip install llama-index[local_models]\")\n",
    "    llm = Ollama(model=\"mistral\", request_timeout=30.0)\n",
    "else:\n",
    "    raise ValueError(\"Invalid LLM Model\")\n",
    "\n",
    "## Defining Embedding Model\n",
    "## A full guide to using and configuring embedding models is available here. https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html\n",
    "emb_option = \"OpenAI\"\n",
    "if emb_option == \"OpenAI\":\n",
    "    embed_model_name = \"text-embedding-ada-002\"\n",
    "    embed_model_deployment_name = \"model-text-embedding-ada-002\"\n",
    "    embed_model = AzureOpenAIEmbedding(model=embed_model_name,\n",
    "                                       deployment_name=embed_model_deployment_name,\n",
    "                                       api_key=openai.api_key,\n",
    "                                       azure_endpoint=azure_endpoint)\n",
    "elif emb_option == \"Local\":\n",
    "    embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\")   ## bge-m3 embedding model\n",
    "else:\n",
    "    raise ValueError(\"Invalid Embedding Model\")\n",
    "\n",
    "## Logging Optionals\n",
    "#logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "#logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "PERSIST_DIR = \"/Workspace/ds-academy-research/LLamaIndex/VectorStoreIndex/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c74520b-5c61-43fe-a212-c96128d0e5d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm,\n",
    "                                               #prompt_helper= ,\n",
    "                                               embed_model=embed_model,\n",
    "                                               #node_parser= ,\n",
    "                                               #chunk_size=1000,                                        #Parse Documents into smaller chunks\n",
    "                                               callback_manager=callback_manager,                       #Visualize execution\n",
    "                                               #system_prompt=(Optional[str]),                          #System-wide prompt to be prepended to all input prompts, used to guide system “decision making”\n",
    "                                               #query_wrapper_prompt=(Optional[BasePromptTemplate]),    #A format to wrap passed-in input queries.\n",
    "                                               )\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a7e70de-4c59-4b75-98e5-6c19cb7d0b79",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### [Storage Context](https://docs.llamaindex.ai/en/stable/api_reference/storage.html)  \n",
    "LlamaIndex offers core abstractions around storage of Nodes, indices, and vectors. A key abstraction is the StorageContext - this contains the underlying BaseDocumentStore (for nodes), BaseIndexStore (for indices), and VectorStore (for vectors).\n",
    "StorageContext defines the storage backend for where the documents, embeddings, and indexes are stored.   \n",
    "```\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"<path/to/index>\")\n",
    "```\n",
    "You can learn more about [storage](https://docs.llamaindex.ai/en/stable/module_guides/storing/storing.html) and how to [customize](https://docs.llamaindex.ai/en/stable/module_guides/storing/customization.html) it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "542f7f6a-3d03-46e7-9cc5-8a276b4cdc0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Reading [Vector Store Index](https://docs.llamaindex.ai/en/stable/api_reference/query/retrievers/vector_store.html)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfb7d30d-dd4e-419d-a332-368e14a0fb1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\nTrace: index_construction\n**********\n"
     ]
    }
   ],
   "source": [
    "vectorstoreindex = load_index_from_storage(storage_context=StorageContext.from_defaults(persist_dir=PERSIST_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "446fdf10-cbff-4719-9502-30af18b8e2c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Querying Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0284771-615d-4ff7-a0a0-963aeb044546",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\nTrace: query\n    |_query ->  0.270917 seconds\n      |_retrieve ->  0.27081 seconds\n        |_embedding ->  0.255206 seconds\n**********\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-851970462462462>, line 4\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m query_engine \u001B[38;5;241m=\u001B[39m vectorstoreindex\u001B[38;5;241m.\u001B[39mas_query_engine(retriever_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124membedding\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m      2\u001B[0m                                                 response_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompact\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m      3\u001B[0m                                                 verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\u001B[0;32m----> 4\u001B[0m response \u001B[38;5;241m=\u001B[39m query_engine\u001B[38;5;241m.\u001B[39mquery(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWill GenAI create new jobs?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(response)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-66f8e693-e286-42f4-98bb-4aa22059fe50/lib/python3.10/site-packages/llama_index/core/base_query_engine.py:40\u001B[0m, in \u001B[0;36mBaseQueryEngine.query\u001B[0;34m(self, str_or_query_bundle)\u001B[0m\n",
       "\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(str_or_query_bundle, \u001B[38;5;28mstr\u001B[39m):\n",
       "\u001B[1;32m     39\u001B[0m     str_or_query_bundle \u001B[38;5;241m=\u001B[39m QueryBundle(str_or_query_bundle)\n",
       "\u001B[0;32m---> 40\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstr_or_query_bundle\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-66f8e693-e286-42f4-98bb-4aa22059fe50/lib/python3.10/site-packages/llama_index/query_engine/retriever_query_engine.py:171\u001B[0m, in \u001B[0;36mRetrieverQueryEngine._query\u001B[0;34m(self, query_bundle)\u001B[0m\n",
       "\u001B[1;32m    167\u001B[0m \u001B[38;5;124;03m\"\"\"Answer a query.\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mevent(\n",
       "\u001B[1;32m    169\u001B[0m     CBEventType\u001B[38;5;241m.\u001B[39mQUERY, payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mQUERY_STR: query_bundle\u001B[38;5;241m.\u001B[39mquery_str}\n",
       "\u001B[1;32m    170\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m query_event:\n",
       "\u001B[0;32m--> 171\u001B[0m     nodes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mretrieve\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_bundle\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    172\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_response_synthesizer\u001B[38;5;241m.\u001B[39msynthesize(\n",
       "\u001B[1;32m    173\u001B[0m         query\u001B[38;5;241m=\u001B[39mquery_bundle,\n",
       "\u001B[1;32m    174\u001B[0m         nodes\u001B[38;5;241m=\u001B[39mnodes,\n",
       "\u001B[1;32m    175\u001B[0m     )\n",
       "\u001B[1;32m    177\u001B[0m     query_event\u001B[38;5;241m.\u001B[39mon_end(payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mRESPONSE: response})\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-66f8e693-e286-42f4-98bb-4aa22059fe50/lib/python3.10/site-packages/llama_index/query_engine/retriever_query_engine.py:127\u001B[0m, in \u001B[0;36mRetrieverQueryEngine.retrieve\u001B[0;34m(self, query_bundle)\u001B[0m\n",
       "\u001B[1;32m    126\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mretrieve\u001B[39m(\u001B[38;5;28mself\u001B[39m, query_bundle: QueryBundle) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[NodeWithScore]:\n",
       "\u001B[0;32m--> 127\u001B[0m     nodes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_retriever\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mretrieve\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_bundle\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    128\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply_node_postprocessors(nodes, query_bundle\u001B[38;5;241m=\u001B[39mquery_bundle)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-66f8e693-e286-42f4-98bb-4aa22059fe50/lib/python3.10/site-packages/llama_index/core/base_retriever.py:224\u001B[0m, in \u001B[0;36mBaseRetriever.retrieve\u001B[0;34m(self, str_or_query_bundle)\u001B[0m\n",
       "\u001B[1;32m    219\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mas_trace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquery\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\u001B[1;32m    220\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mevent(\n",
       "\u001B[1;32m    221\u001B[0m         CBEventType\u001B[38;5;241m.\u001B[39mRETRIEVE,\n",
       "\u001B[1;32m    222\u001B[0m         payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mQUERY_STR: query_bundle\u001B[38;5;241m.\u001B[39mquery_str},\n",
       "\u001B[1;32m    223\u001B[0m     ) \u001B[38;5;28;01mas\u001B[39;00m retrieve_event:\n",
       "\u001B[0;32m--> 224\u001B[0m         nodes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_retrieve\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_bundle\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    225\u001B[0m         nodes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_recursive_retrieval(query_bundle, nodes)\n",
       "\u001B[1;32m    226\u001B[0m         retrieve_event\u001B[38;5;241m.\u001B[39mon_end(\n",
       "\u001B[1;32m    227\u001B[0m             payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mNODES: nodes},\n",
       "\u001B[1;32m    228\u001B[0m         )\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-66f8e693-e286-42f4-98bb-4aa22059fe50/lib/python3.10/site-packages/llama_index/indices/vector_store/retrievers/retriever.py:92\u001B[0m, in \u001B[0;36mVectorIndexRetriever._retrieve\u001B[0;34m(self, query_bundle)\u001B[0m\n",
       "\u001B[1;32m     86\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m query_bundle\u001B[38;5;241m.\u001B[39membedding \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(query_bundle\u001B[38;5;241m.\u001B[39membedding_strs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\u001B[1;32m     87\u001B[0m         query_bundle\u001B[38;5;241m.\u001B[39membedding \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[1;32m     88\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_service_context\u001B[38;5;241m.\u001B[39membed_model\u001B[38;5;241m.\u001B[39mget_agg_embedding_from_queries(\n",
       "\u001B[1;32m     89\u001B[0m                 query_bundle\u001B[38;5;241m.\u001B[39membedding_strs\n",
       "\u001B[1;32m     90\u001B[0m             )\n",
       "\u001B[1;32m     91\u001B[0m         )\n",
       "\u001B[0;32m---> 92\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_nodes_with_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_bundle\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-66f8e693-e286-42f4-98bb-4aa22059fe50/lib/python3.10/site-packages/llama_index/indices/vector_store/retrievers/retriever.py:168\u001B[0m, in \u001B[0;36mVectorIndexRetriever._get_nodes_with_embeddings\u001B[0;34m(self, query_bundle_with_embeddings)\u001B[0m\n",
       "\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_nodes_with_embeddings\u001B[39m(\n",
       "\u001B[1;32m    165\u001B[0m     \u001B[38;5;28mself\u001B[39m, query_bundle_with_embeddings: QueryBundle\n",
       "\u001B[1;32m    166\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[NodeWithScore]:\n",
       "\u001B[1;32m    167\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_vector_store_query(query_bundle_with_embeddings)\n",
       "\u001B[0;32m--> 168\u001B[0m     query_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_vector_store\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    169\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_node_list_from_query_result(query_result)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-66f8e693-e286-42f4-98bb-4aa22059fe50/lib/python3.10/site-packages/llama_index/vector_stores/simple.py:274\u001B[0m, in \u001B[0;36mSimpleVectorStore.query\u001B[0;34m(self, query, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    266\u001B[0m     top_similarities, top_ids \u001B[38;5;241m=\u001B[39m get_top_k_mmr_embeddings(\n",
       "\u001B[1;32m    267\u001B[0m         query_embedding,\n",
       "\u001B[1;32m    268\u001B[0m         embeddings,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    271\u001B[0m         mmr_threshold\u001B[38;5;241m=\u001B[39mmmr_threshold,\n",
       "\u001B[1;32m    272\u001B[0m     )\n",
       "\u001B[1;32m    273\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m query\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m VectorStoreQueryMode\u001B[38;5;241m.\u001B[39mDEFAULT:\n",
       "\u001B[0;32m--> 274\u001B[0m     top_similarities, top_ids \u001B[38;5;241m=\u001B[39m \u001B[43mget_top_k_embeddings\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m    275\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery_embedding\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    276\u001B[0m \u001B[43m        \u001B[49m\u001B[43membeddings\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    277\u001B[0m \u001B[43m        \u001B[49m\u001B[43msimilarity_top_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimilarity_top_k\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    278\u001B[0m \u001B[43m        \u001B[49m\u001B[43membedding_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnode_ids\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    279\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    280\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    281\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid query mode: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquery\u001B[38;5;241m.\u001B[39mmode\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-66f8e693-e286-42f4-98bb-4aa22059fe50/lib/python3.10/site-packages/llama_index/indices/query/embedding_utils.py:31\u001B[0m, in \u001B[0;36mget_top_k_embeddings\u001B[0;34m(query_embedding, embeddings, similarity_fn, similarity_top_k, embedding_ids, similarity_cutoff)\u001B[0m\n",
       "\u001B[1;32m     29\u001B[0m similarity_heap: List[Tuple[\u001B[38;5;28mfloat\u001B[39m, Any]] \u001B[38;5;241m=\u001B[39m []\n",
       "\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, emb \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(embeddings_np):\n",
       "\u001B[0;32m---> 31\u001B[0m     similarity \u001B[38;5;241m=\u001B[39m \u001B[43msimilarity_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_embedding_np\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43memb\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m similarity_cutoff \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m similarity \u001B[38;5;241m>\u001B[39m similarity_cutoff:\n",
       "\u001B[1;32m     33\u001B[0m         heapq\u001B[38;5;241m.\u001B[39mheappush(similarity_heap, (similarity, embedding_ids[i]))\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-66f8e693-e286-42f4-98bb-4aa22059fe50/lib/python3.10/site-packages/llama_index/core/embeddings/base.py:48\u001B[0m, in \u001B[0;36msimilarity\u001B[0;34m(embedding1, embedding2, mode)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mdot(embedding1, embedding2)\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     product \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43membedding1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding2\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     norm \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(embedding1) \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(embedding2)\n",
       "\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m product \u001B[38;5;241m/\u001B[39m norm\n",
       "\n",
       "File \u001B[0;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mdot\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: shapes (1536,) and (384,) not aligned: 1536 (dim 0) != 384 (dim 0)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m<command-851970462462462>, line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m query_engine \u001B[38;5;241m=\u001B[39m vectorstoreindex\u001B[38;5;241m.\u001B[39mas_query_engine(retriever_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124membedding\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      2\u001B[0m                                                 response_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompact\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      3\u001B[0m                                                 verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m----> 4\u001B[0m response \u001B[38;5;241m=\u001B[39m query_engine\u001B[38;5;241m.\u001B[39mquery(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWill GenAI create new jobs?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(response)\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-66f8e693-e286-42f4-98bb-4aa22059fe50/lib/python3.10/site-packages/llama_index/core/base_query_engine.py:40\u001B[0m, in \u001B[0;36mBaseQueryEngine.query\u001B[0;34m(self, str_or_query_bundle)\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(str_or_query_bundle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m     39\u001B[0m     str_or_query_bundle \u001B[38;5;241m=\u001B[39m QueryBundle(str_or_query_bundle)\n\u001B[0;32m---> 40\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstr_or_query_bundle\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-66f8e693-e286-42f4-98bb-4aa22059fe50/lib/python3.10/site-packages/llama_index/query_engine/retriever_query_engine.py:171\u001B[0m, in \u001B[0;36mRetrieverQueryEngine._query\u001B[0;34m(self, query_bundle)\u001B[0m\n\u001B[1;32m    167\u001B[0m \u001B[38;5;124;03m\"\"\"Answer a query.\"\"\"\u001B[39;00m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mevent(\n\u001B[1;32m    169\u001B[0m     CBEventType\u001B[38;5;241m.\u001B[39mQUERY, payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mQUERY_STR: query_bundle\u001B[38;5;241m.\u001B[39mquery_str}\n\u001B[1;32m    170\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m query_event:\n\u001B[0;32m--> 171\u001B[0m     nodes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mretrieve\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_bundle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    172\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_response_synthesizer\u001B[38;5;241m.\u001B[39msynthesize(\n\u001B[1;32m    173\u001B[0m         query\u001B[38;5;241m=\u001B[39mquery_bundle,\n\u001B[1;32m    174\u001B[0m         nodes\u001B[38;5;241m=\u001B[39mnodes,\n\u001B[1;32m    175\u001B[0m     )\n\u001B[1;32m    177\u001B[0m     query_event\u001B[38;5;241m.\u001B[39mon_end(payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mRESPONSE: response})\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-66f8e693-e286-42f4-98bb-4aa22059fe50/lib/python3.10/site-packages/llama_index/query_engine/retriever_query_engine.py:127\u001B[0m, in \u001B[0;36mRetrieverQueryEngine.retrieve\u001B[0;34m(self, query_bundle)\u001B[0m\n\u001B[1;32m    126\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mretrieve\u001B[39m(\u001B[38;5;28mself\u001B[39m, query_bundle: QueryBundle) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[NodeWithScore]:\n\u001B[0;32m--> 127\u001B[0m     nodes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_retriever\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mretrieve\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_bundle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    128\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply_node_postprocessors(nodes, query_bundle\u001B[38;5;241m=\u001B[39mquery_bundle)\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-66f8e693-e286-42f4-98bb-4aa22059fe50/lib/python3.10/site-packages/llama_index/core/base_retriever.py:224\u001B[0m, in \u001B[0;36mBaseRetriever.retrieve\u001B[0;34m(self, str_or_query_bundle)\u001B[0m\n\u001B[1;32m    219\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mas_trace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquery\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    220\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mevent(\n\u001B[1;32m    221\u001B[0m         CBEventType\u001B[38;5;241m.\u001B[39mRETRIEVE,\n\u001B[1;32m    222\u001B[0m         payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mQUERY_STR: query_bundle\u001B[38;5;241m.\u001B[39mquery_str},\n\u001B[1;32m    223\u001B[0m     ) \u001B[38;5;28;01mas\u001B[39;00m retrieve_event:\n\u001B[0;32m--> 224\u001B[0m         nodes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_retrieve\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_bundle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    225\u001B[0m         nodes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_recursive_retrieval(query_bundle, nodes)\n\u001B[1;32m    226\u001B[0m         retrieve_event\u001B[38;5;241m.\u001B[39mon_end(\n\u001B[1;32m    227\u001B[0m             payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mNODES: nodes},\n\u001B[1;32m    228\u001B[0m         )\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-66f8e693-e286-42f4-98bb-4aa22059fe50/lib/python3.10/site-packages/llama_index/indices/vector_store/retrievers/retriever.py:92\u001B[0m, in \u001B[0;36mVectorIndexRetriever._retrieve\u001B[0;34m(self, query_bundle)\u001B[0m\n\u001B[1;32m     86\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m query_bundle\u001B[38;5;241m.\u001B[39membedding \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(query_bundle\u001B[38;5;241m.\u001B[39membedding_strs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     87\u001B[0m         query_bundle\u001B[38;5;241m.\u001B[39membedding \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     88\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_service_context\u001B[38;5;241m.\u001B[39membed_model\u001B[38;5;241m.\u001B[39mget_agg_embedding_from_queries(\n\u001B[1;32m     89\u001B[0m                 query_bundle\u001B[38;5;241m.\u001B[39membedding_strs\n\u001B[1;32m     90\u001B[0m             )\n\u001B[1;32m     91\u001B[0m         )\n\u001B[0;32m---> 92\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_nodes_with_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_bundle\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-66f8e693-e286-42f4-98bb-4aa22059fe50/lib/python3.10/site-packages/llama_index/indices/vector_store/retrievers/retriever.py:168\u001B[0m, in \u001B[0;36mVectorIndexRetriever._get_nodes_with_embeddings\u001B[0;34m(self, query_bundle_with_embeddings)\u001B[0m\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_nodes_with_embeddings\u001B[39m(\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;28mself\u001B[39m, query_bundle_with_embeddings: QueryBundle\n\u001B[1;32m    166\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[NodeWithScore]:\n\u001B[1;32m    167\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_vector_store_query(query_bundle_with_embeddings)\n\u001B[0;32m--> 168\u001B[0m     query_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_vector_store\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_node_list_from_query_result(query_result)\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-66f8e693-e286-42f4-98bb-4aa22059fe50/lib/python3.10/site-packages/llama_index/vector_stores/simple.py:274\u001B[0m, in \u001B[0;36mSimpleVectorStore.query\u001B[0;34m(self, query, **kwargs)\u001B[0m\n\u001B[1;32m    266\u001B[0m     top_similarities, top_ids \u001B[38;5;241m=\u001B[39m get_top_k_mmr_embeddings(\n\u001B[1;32m    267\u001B[0m         query_embedding,\n\u001B[1;32m    268\u001B[0m         embeddings,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    271\u001B[0m         mmr_threshold\u001B[38;5;241m=\u001B[39mmmr_threshold,\n\u001B[1;32m    272\u001B[0m     )\n\u001B[1;32m    273\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m query\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m VectorStoreQueryMode\u001B[38;5;241m.\u001B[39mDEFAULT:\n\u001B[0;32m--> 274\u001B[0m     top_similarities, top_ids \u001B[38;5;241m=\u001B[39m \u001B[43mget_top_k_embeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery_embedding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    276\u001B[0m \u001B[43m        \u001B[49m\u001B[43membeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    277\u001B[0m \u001B[43m        \u001B[49m\u001B[43msimilarity_top_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimilarity_top_k\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    278\u001B[0m \u001B[43m        \u001B[49m\u001B[43membedding_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnode_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    279\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    280\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    281\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid query mode: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquery\u001B[38;5;241m.\u001B[39mmode\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-66f8e693-e286-42f4-98bb-4aa22059fe50/lib/python3.10/site-packages/llama_index/indices/query/embedding_utils.py:31\u001B[0m, in \u001B[0;36mget_top_k_embeddings\u001B[0;34m(query_embedding, embeddings, similarity_fn, similarity_top_k, embedding_ids, similarity_cutoff)\u001B[0m\n\u001B[1;32m     29\u001B[0m similarity_heap: List[Tuple[\u001B[38;5;28mfloat\u001B[39m, Any]] \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, emb \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(embeddings_np):\n\u001B[0;32m---> 31\u001B[0m     similarity \u001B[38;5;241m=\u001B[39m \u001B[43msimilarity_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_embedding_np\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43memb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m similarity_cutoff \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m similarity \u001B[38;5;241m>\u001B[39m similarity_cutoff:\n\u001B[1;32m     33\u001B[0m         heapq\u001B[38;5;241m.\u001B[39mheappush(similarity_heap, (similarity, embedding_ids[i]))\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-66f8e693-e286-42f4-98bb-4aa22059fe50/lib/python3.10/site-packages/llama_index/core/embeddings/base.py:48\u001B[0m, in \u001B[0;36msimilarity\u001B[0;34m(embedding1, embedding2, mode)\u001B[0m\n\u001B[1;32m     46\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mdot(embedding1, embedding2)\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     product \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43membedding1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding2\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     norm \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(embedding1) \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(embedding2)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m product \u001B[38;5;241m/\u001B[39m norm\n\nFile \u001B[0;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mdot\u001B[0;34m(*args, **kwargs)\u001B[0m\n\n\u001B[0;31mValueError\u001B[0m: shapes (1536,) and (384,) not aligned: 1536 (dim 0) != 384 (dim 0)",
       "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: shapes (1536,) and (384,) not aligned: 1536 (dim 0) != 384 (dim 0)",
       "errorTraceType": "baseError",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine = vectorstoreindex.as_query_engine(retriever_mode=\"embedding\",\n",
    "                                                response_mode=\"compact\",\n",
    "                                                verbose=True)\n",
    "response = query_engine.query(\"Will GenAI create new jobs?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70cafdcf-5f47-4abd-a84c-879b452fde33",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Creating an Simple Interactive Chatbot for our Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d647efc0-5b89-4b38-9ebd-7f8efad51833",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chat_engine = index.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
    "chat_engine.reset()\n",
    "chat_engine.chat_repl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35818547-da92-4040-b80c-8fa5ff5a5e21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "https://sharmadave.medium.com/llama-index-unleashes-the-power-of-chatgpt-over-your-own-data-b67cc2e4e277  \n",
    "https://blog.streamlit.io/build-a-chatbot-with-custom-data-sources-powered-by-llamaindex/  \n",
    "https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/chatbots/building_a_chatbot.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ae22c93-1fb9-411a-b23e-f2e61ac269ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Creating an Customized Chatbot for our Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02355d72-98cb-4ac1-86f8-7a9ba12f795c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_custom_chatEngine(index):\n",
    "   \n",
    "    template = (\n",
    "    \"Following Informations : \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Please answer the question always from the first person perspective and always start your answer with Renato: {query_str}\\n\"\n",
    ")\n",
    "    qa_template = Prompt(template)\n",
    "    query_engine = index.as_query_engine(text_qa_template=qa_template)\n",
    "    chat_engine = CondenseQuestionChatEngine.from_defaults(query_engine=query_engine, verbose=False)\n",
    "    return chat_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "493a16a1-3379-4ac1-8352-009b3fc0f22f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Creating an [interactive Chatbot](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/chatbots/building_a_chatbot.html) with Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec07400c-cfbd-4d7a-bec4-84b2ef395180",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "years = [2022, 2021, 2020, 2019]\n",
    "loader = UnstructuredReader()\n",
    "doc_set = {}\n",
    "all_docs = []\n",
    "for year in years:\n",
    "    year_docs = loader.load_data(file=Path(f\"../../Data/html/UBER_{year}.html\"), split_documents=False)\n",
    "    for d in year_docs:\n",
    "        d.metadata = {\"year\": year}\n",
    "    doc_set[year] = year_docs\n",
    "    all_docs.extend(year_docs)\n",
    "\n",
    "index_set = {}\n",
    "service_context = ServiceContext.from_defaults(chunk_size=512)\n",
    "for year in years:\n",
    "    storage_context = StorageContext.from_defaults()\n",
    "    cur_index = VectorStoreIndex.from_documents(doc_set[year],\n",
    "                                                service_context=service_context,\n",
    "                                                storage_context=storage_context,\n",
    "                                                )\n",
    "    index_set[year] = cur_index\n",
    "    storage_context.persist(persist_dir=f\"./storage/{year}\")\n",
    "\n",
    "index_set = {}\n",
    "for year in years:\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=f\"./storage/{year}\")\n",
    "    cur_index = load_index_from_storage(storage_context, \n",
    "                                        service_context=service_context,\n",
    "                                        )\n",
    "    index_set[year] = cur_index\n",
    "\n",
    "\n",
    "individual_query_engine_tools = [QueryEngineTool(query_engine=index_set[year].as_query_engine(),\n",
    "                                                 metadata=ToolMetadata(name=f\"vector_index_{year}\", description=f\"useful for when you want to answer queries about the {year} SEC 10-K for Uber\",),\n",
    "                                                 ) for year in years]    \n",
    "\n",
    "query_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=individual_query_engine_tools,\n",
    "                                                    service_context=service_context,)\n",
    "\n",
    "query_engine_tool = QueryEngineTool(query_engine=query_engine,\n",
    "                                    metadata=ToolMetadata(name=\"sub_question_query_engine\",\n",
    "                                                          description=\"useful for when you want to answer queries that require analyzing multiple SEC 10-K documents for Uber\",\n",
    "                                                          ),\n",
    "                                    )\n",
    "\n",
    "tools = individual_query_engine_tools + [query_engine_tool]\n",
    "agent = OpenAIAgent.from_tools(tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "add9058a-023e-400f-bf79-29e64c9e5d99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response = agent.chat(\"hi, i am bob\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd4e28c8-0c9c-4e11-9bac-f246a3e5d933",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response = agent.chat(\"What were some of the biggest risk factors in 2020 for Uber?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d50b08bb-8154-43de-94c0-ebe663d406ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cross_query_str = \"Compare/contrast the risk factors described in the Uber 10-K across years. Give answer in bullet points.\"\n",
    "response = agent.chat(cross_query_str)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad832e4c-9fc8-4e5a-b301-f012c259bd6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "agent = OpenAIAgent.from_tools(tools)  # verbose=False by default\n",
    "\n",
    "while True:\n",
    "    text_input = input(\"User: \")\n",
    "    if text_input == \"exit\":\n",
    "        break\n",
    "    response = agent.chat(text_input)\n",
    "    print(f\"Agent: {response}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3-Chatbot_LLamaIndex",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
