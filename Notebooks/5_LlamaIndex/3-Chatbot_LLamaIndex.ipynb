{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e664a6a8-404e-4a51-b017-d52a0159f34d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### LlamaIndex: Customizing Chatbots  \n",
    "Sources: [1](https://lmy.medium.com/comparing-langchain-and-llamaindex-with-4-tasks-2970140edf33), [2](https://docs.llamaindex.ai/en/stable/), [3](https://github.com/run-llama/llama_index), [4](https://nanonets.com/blog/llamaindex/), [5](https://sharmadave.medium.com/llama-index-unleashes-the-power-of-chatgpt-over-your-own-data-b67cc2e4e277), [6](https://blog.streamlit.io/build-a-chatbot-with-custom-data-sources-powered-by-llamaindex/), [7](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/chatbots/building_a_chatbot.html)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3870825c-294f-4209-b7fd-0b575e426efe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Installing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ff6e701-2f86-4569-a844-87e281df3140",
     "showTitle": false,
     "title": "ro"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install -q openai==0.27.0\n",
    "#!pip install -qU llama-index[local_models]  # Installs tools useful for private LLMs, local inference, and HuggingFace models\n",
    "#!pip install -q llama-index[postgres]       # Is useful if you are working with Postgres, PGVector or Supabase\n",
    "#!pip install -q llama-index[query_tools]    # Gives you tools for hybrid search, structured outputs, and node post-processing\n",
    "!pip install -q llama-index==0.9.47                 # Just the core components  ## Follow: https://github.com/run-llama/llama_index/issues/10636\n",
    "##!pip install -q llama-hub \n",
    "#!pip install -qU chromadb\n",
    "!pip install -qU pypdf\n",
    "!pip install -qU docx2txt\n",
    "!pip install -qU sentence-transformers\n",
    "!pip install -q unstructured\n",
    "!pip install -q aa-llm-utils\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "656e5f42-85bb-4cd0-b856-49150d512ee1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7f5e7dc-ad27-4669-9d7f-f4e3fa9b893d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import glob\n",
    "import re\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import nest_asyncio\n",
    "#nest_asyncio.apply()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "#import tiktoken\n",
    "#from funcy import lcat, lmap, linvoke\n",
    "#from IPython.display import Markdown, display\n",
    "import openai\n",
    "#import chromadb\n",
    "\n",
    "## LlamaIndex LLMs\n",
    "#from openai import OpenAI\n",
    "#from openai import AzureOpenAI\n",
    "from llama_index.llms import AzureOpenAI\n",
    "from llama_index.llms import ChatMessage\n",
    "\n",
    "from llama_index.llms import MessageRole\n",
    "#from llama_index.llms import Ollama\n",
    "#from llama_index.llms import PaLM\n",
    "\n",
    "## LlamaIndex Embeddings\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.embeddings import AzureOpenAIEmbedding\n",
    "from llama_index.embeddings import resolve_embed_model\n",
    "\n",
    "## Llamaindex readers \n",
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index import Document\n",
    "#from llama_hub.file.unstructured.base import UnstructuredReader\n",
    "\n",
    "## LlamaIndex Index Types\n",
    "#from llama_index import GPTListIndex             \n",
    "from llama_index import VectorStoreIndex\n",
    "#from llama_index import GPTVectorStoreIndex  \n",
    "#from llama_index import GPTTreeIndex\n",
    "#from llama_index import GPTKeywordTableIndex\n",
    "#from llama_index import GPTSimpleKeywordTableIndex\n",
    "#from llama_index import GPTDocumentSummaryIndex\n",
    "#from llama_index import GPTKnowledgeGraphIndex\n",
    "#from llama_index.indices.struct_store import GPTPandasIndex\n",
    "#from llama_index.vector_stores import ChromaVectorStore\n",
    "\n",
    "## LlamaIndex Context Managers\n",
    "from llama_index import ServiceContext\n",
    "from llama_index import StorageContext\n",
    "from llama_index import load_index_from_storage\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.response_synthesizers import ResponseMode\n",
    "from llama_index.schema import Node\n",
    "#from llama_index import LLMPredictor\n",
    "\n",
    "## LlamaIndex Templates\n",
    "from llama_index.prompts import PromptTemplate\n",
    "from llama_index.prompts import ChatPromptTemplate\n",
    "\n",
    "## LlamaIndex Tools\n",
    "from llama_index.tools import QueryEngineTool\n",
    "from llama_index.tools import ToolMetadata\n",
    "from llama_index.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.chat_engine import SimpleChatEngine\n",
    "\n",
    "## LlamaIndex Agents\n",
    "from llama_index.agent import OpenAIAgent\n",
    "\n",
    "## LlamaIndex Callbacks\n",
    "from llama_index.callbacks import CallbackManager\n",
    "from llama_index.callbacks import LlamaDebugHandler\n",
    "\n",
    "from aa_llm_utils.utils import ensure_certificates\n",
    "ensure_certificates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cf9b236-7b3b-4c86-a12f-28ffa334d575",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Defining Model and Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f2ba122-e919-4768-840a-03db5ccc0e8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Defining LLM Model\n",
    "## A full guide to using and configuring LLMs available here: https://docs.llamaindex.ai/en/stable/module_guides/models/llms.html\n",
    "## Check also: https://docs.llamaindex.ai/en/stable/module_guides/models/llms/local.html\n",
    "llm_option = \"OpenAI\"\n",
    "if llm_option == \"OpenAI\":\n",
    "    openai.api_type = \"azure\"\n",
    "    azure_endpoint = \"https://rg-rbi-aa-aitest-dsacademy.openai.azure.com/\"\n",
    "    #azure_endpoint = \"https://chatgpt-summarization.openai.azure.com/\"\n",
    "    openai.api_version = \"2023-07-01-preview\"\n",
    "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "    deployment_name = \"model-gpt-35-turbo\"\n",
    "    openai_model_name = \"gpt-35-turbo\"\n",
    "    llm = AzureOpenAI(api_key=openai.api_key,\n",
    "                      azure_endpoint=azure_endpoint,\n",
    "                      model=openai_model_name,\n",
    "                      engine=deployment_name,\n",
    "                      api_version=openai.api_version,\n",
    "                      )\n",
    "elif llm_option == \"Local\":  \n",
    "    print(\"Make sure you have installed Local Models - !pip install llama-index[local_models]\")\n",
    "    llm = Ollama(model=\"mistral\", request_timeout=30.0)\n",
    "else:\n",
    "    raise ValueError(\"Invalid LLM Model\")\n",
    "\n",
    "## Defining Embedding Model\n",
    "## A full guide to using and configuring embedding models is available here. https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html\n",
    "emb_option = \"OpenAI\"\n",
    "if emb_option == \"OpenAI\":\n",
    "    embed_model_name = \"text-embedding-ada-002\"\n",
    "    embed_model_deployment_name = \"model-text-embedding-ada-002\"\n",
    "    embed_model = AzureOpenAIEmbedding(model=embed_model_name,\n",
    "                                       deployment_name=embed_model_deployment_name,\n",
    "                                       api_key=openai.api_key,\n",
    "                                       azure_endpoint=azure_endpoint)\n",
    "elif emb_option == \"Local\":\n",
    "    embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\")   ## bge-m3 embedding model\n",
    "else:\n",
    "    raise ValueError(\"Invalid Embedding Model\")\n",
    "\n",
    "## Logging Optionals\n",
    "#logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "#logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "PERSIST_DIR = \"/Workspace/ds-academy-research/LLamaIndex/VectorStoreIndex/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c74520b-5c61-43fe-a212-c96128d0e5d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm,\n",
    "                                               #prompt_helper= <>,\n",
    "                                               embed_model=embed_model,\n",
    "                                               #node_parser= <>,\n",
    "                                               #chunk_size=1000,                                        #Parse Documents into smaller chunks\n",
    "                                               callback_manager=callback_manager,                       #Visualize execution\n",
    "                                               #system_prompt=(Optional[str]),                          #System-wide prompt to be prepended to all input prompts, used to guide system “decision making”\n",
    "                                               #query_wrapper_prompt=(Optional[BasePromptTemplate]),    #A format to wrap passed-in input queries.\n",
    "                                               )\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a7e70de-4c59-4b75-98e5-6c19cb7d0b79",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### [Storage Context](https://docs.llamaindex.ai/en/stable/api_reference/storage.html)  \n",
    "LlamaIndex offers core abstractions around storage of Nodes, indices, and vectors. A key abstraction is the StorageContext - this contains the underlying BaseDocumentStore (for nodes), BaseIndexStore (for indices), and VectorStore (for vectors).\n",
    "StorageContext defines the storage backend for where the documents, embeddings, and indexes are stored.   \n",
    "```\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"<path/to/index>\")\n",
    "```\n",
    "You can learn more about [storage](https://docs.llamaindex.ai/en/stable/module_guides/storing/storing.html) and how to [customize](https://docs.llamaindex.ai/en/stable/module_guides/storing/customization.html) it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "542f7f6a-3d03-46e7-9cc5-8a276b4cdc0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Reading [Vector Store Index](https://docs.llamaindex.ai/en/stable/api_reference/query/retrievers/vector_store.html)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfb7d30d-dd4e-419d-a332-368e14a0fb1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vectorstoreindex = load_index_from_storage(storage_context=StorageContext.from_defaults(persist_dir=PERSIST_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "446fdf10-cbff-4719-9502-30af18b8e2c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Querying Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0284771-615d-4ff7-a0a0-963aeb044546",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_engine = vectorstoreindex.as_query_engine(retriever_mode=\"embedding\",\n",
    "                                                response_mode=\"compact\",\n",
    "                                                verbose=True)\n",
    "response = query_engine.query(\"Will GenAI create new jobs?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70cafdcf-5f47-4abd-a84c-879b452fde33",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Creating an Simple Interactive Chatbot for our Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d647efc0-5b89-4b38-9ebd-7f8efad51833",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chat_engine = vectorstoreindex.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
    "chat_engine.reset()\n",
    "chat_engine.chat_repl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ae22c93-1fb9-411a-b23e-f2e61ac269ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Creating an Customized Prompt Chatbot  (Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02355d72-98cb-4ac1-86f8-7a9ba12f795c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "template = (\n",
    "    \"Following Informations : \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Please answer the question even if the context does not provide a clear answer\" \n",
    "    \"Always start your answer with Renato: {query_str}\\n\"\n",
    "    )\n",
    "qa_template = PromptTemplate(template)\n",
    "chat_engine = vectorstoreindex.as_chat_engine(chat_mode=\"condense_question\", \n",
    "                                              verbose=True, \n",
    "                                              text_qa_template=qa_template)\n",
    "chat_engine.chat_repl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "493a16a1-3379-4ac1-8352-009b3fc0f22f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Creating an [interactive Chatbot](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/chatbots/building_a_chatbot.html) with Agents\n",
    "\n",
    "(only works with certain OpenAi models/versions that implement agents)  \n",
    "https://github.com/openai/openai-python/issues/517  \n",
    "https://github.com/run-llama/llama_index/issues/9618 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "827b20bd-c0c0-437e-855b-56e47aba2d9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DOCS_DIR = \"../../Data/pdf/\"\n",
    "doclist = os.listdir(DOCS_DIR)\n",
    "doclist = [d for d in doclist if d.startswith(\"NASDAQ\")]\n",
    "doclist.sort()\n",
    "for d in doclist:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2e27bc6-fe96-4da2-ac9d-ea841fde127c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "AGENT_DIR = \"/Workspace/ds-academy-research/LLamaIndex/AgentsIndex/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9db2a5d6-93ed-4763-94c6-792f18182524",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(llm=llm,\n",
    "                                               embed_model=embed_model,\n",
    "                                               callback_manager=callback_manager,\n",
    "                                               chunk_size=512,\n",
    "                                               )\n",
    "\n",
    "index_set = {}\n",
    "years = []\n",
    "reader = SimpleDirectoryReader(input_files= [DOCS_DIR + d for d in doclist], recursive=True)\n",
    "for docs in reader.iter_data():\n",
    "    storage_context = StorageContext.from_defaults()\n",
    "    for doc in docs:\n",
    "        year = re.findall('\\d+', doc.metadata[\"file_name\"])[0]\n",
    "        doc.metadata[\"year\"] = year\n",
    "    years.append(year)\n",
    "    index = VectorStoreIndex.from_documents(docs,\n",
    "                                            service_context=service_context,\n",
    "                                            storage_context=storage_context,\n",
    "                                            )\n",
    "    storage_context.persist(persist_dir=AGENT_DIR+f\"{year}\")\n",
    "    index_set[year] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a633815-91a3-4720-92bd-e0bafc735f50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "index_set = {}\n",
    "years =  [\"2019\", \"2020\",\"2021\"]\n",
    "for year in years:\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=AGENT_DIR+f\"{year}\")\n",
    "    index = load_index_from_storage(storage_context, service_context=service_context)\n",
    "    index_set[year] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a980d87b-0477-40b3-abae-d9ab1406cca6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "273cb518-9fa2-46c6-acac-7d3f40a0a3f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Logging Optionals\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec07400c-cfbd-4d7a-bec4-84b2ef395180",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "individual_query_engine_tools = [QueryEngineTool(query_engine=index_set[year].as_query_engine(),\n",
    "                                                 metadata=ToolMetadata(name=f\"vector_index_{year}\", description=f\"useful for when you want to answer queries about the {year} for AWS\",),\n",
    "                                                 ) for year in years]    \n",
    "\n",
    "query_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=individual_query_engine_tools,\n",
    "                                                    service_context=ServiceContext.from_defaults(llm=llm,\n",
    "                                                                                                 embed_model=embed_model,\n",
    "                                                                                                 callback_manager=callback_manager,),\n",
    "                                                    use_async=True,\n",
    "                                                    )\n",
    "\n",
    "query_engine_tool = QueryEngineTool(query_engine=query_engine,\n",
    "                                    metadata=ToolMetadata(name=\"sub_question_query_engine\",\n",
    "                                                          description=\"useful for when you want to answer queries that require analyzing multiple documents documents for AWS\",\n",
    "                                                          ),\n",
    "                                    )\n",
    "\n",
    "tools = individual_query_engine_tools + [query_engine_tool]\n",
    "agent = OpenAIAgent.from_tools(tools=tools, \n",
    "                               llm=llm,\n",
    "                               service_context=ServiceContext.from_defaults(llm=llm,\n",
    "                                                                            embed_model=embed_model,\n",
    "                                                                            callback_manager=callback_manager,\n",
    "                                                ), \n",
    "                               verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b715f98-1504-4738-975f-f00629490226",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#import inspect\n",
    "#lines = inspect.getsource(OpenAIAgent)\n",
    "#print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be573ddd-352d-47ca-85f6-e9c356fb1b88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "individual_query_engine_tools[0]('What is AWS?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "add9058a-023e-400f-bf79-29e64c9e5d99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response = agent.chat(\"Hi, my name is Renato\", )\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd4e28c8-0c9c-4e11-9bac-f246a3e5d933",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response = agent.chat(\"What were some of the biggest risk factors in 2020 for AWS?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d50b08bb-8154-43de-94c0-ebe663d406ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cross_query_str = \"Compare/contrast the risk factors described in the AWS Executive Reports across years. Give answer in bullet points.\"\n",
    "response = agent.chat(cross_query_str)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "566432e0-07b0-4747-84a8-d972e4f1fd30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "agent.chat_repl()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3-Chatbot_LLamaIndex",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
