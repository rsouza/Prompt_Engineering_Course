{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34e0b4aa-addf-4e66-8fc3-0d89e8e1a6c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Connect to ChatGPT API\n",
    "In this lesson, you'll manage to connect to the OpenAi and AzureOpenAI APIS for ChatGPT.\n",
    "\n",
    "## Setup\n",
    "#### Load the API key and relevant Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95b9f9e0-6725-486e-8317-78344ec9c25a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -qU openai\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c1997ff-4712-4903-ba14-19bdb0cf71a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dd7a5a2-a7f4-4d3d-bee9-a71c9a3d8114",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://rg-rbi-aa-aitest-dsacademy.openai.azure.com/\"\n",
    "#openai.api_base = \"https://chatgpt-summarization.openai.azure.com/\"\n",
    "openai.api_version = \"2023-07-01-preview\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "openai_deploy_name = \"model-gpt-35-turbo\"\n",
    "openai_model_name = \"gpt-35-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e13c7a0e-b469-43ec-82ec-d40ac3b9b3f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### OpenAI or AzureOpenAI?  \n",
    "\n",
    "When you access the model via the API in Azure OpenAI you will need to refer to the deployment name rather than the underlying model name in API calls. This is one of the key differences between OpenAI and Azure OpenAI. OpenAI only requires the model name, Azure OpenAI always requires deployment name, even when using the model parameter. In our docs we often have examples where deployment names are represented as identical to model names to help indicate which model works with a particular API endpoint. Ultimately your deployment names can follow whatever naming convention is best for your use case.\n",
    "\n",
    "#### OpenAI:  \n",
    "```\n",
    "client = OpenAI(api_key=openai.api_key)\n",
    "\n",
    "completion = client.completions.create(model=\"<model_name>\", prompt=\"<prompt>\")  \n",
    "chat_completion = client.chat.completions.create(model=\"<model_name>\",messages=\"<messages>\")  \n",
    "embedding = client.embeddings.create(model=\"<model_name>\", input=\"<input>\")  \n",
    "```\n",
    "\n",
    "#### AzureOpenAI:\n",
    "```\n",
    "client = AzureOpenAI(api_key=openai.api_key, api_version=openai.api_version, azure_endpoint=openai.api_base)  \n",
    "\n",
    "completion = client.completions.create(model=\"<model_deployment_name>\", prompt=\"<prompt>\")  \n",
    "chat_completion = client.chat.completions.create(model=\"<model_deployment_name>\", messages=\"<messages>\")  \n",
    "embedding = client.embeddings.create(model=\"<model_deployment_name>\", input=\"<input>\")  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfff749b-9677-4a1b-82c9-dff71ee85ba1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client = AzureOpenAI(api_key=openai.api_key,\n",
    "                     api_version=openai.api_version,\n",
    "                     azure_endpoint=openai.api_base,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cbcdeda-2f8c-4eac-b870-9d948f7dc71f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Simple Completions  \n",
    "\n",
    "OpenAI refers to text generation simply as completions as in text completion. It is interesting to note the naming convention, which derives from how these language models generate text via the use of word probability, one word at a time as it completes the initial starting words to form complete sentences. Completions models are being discontinued (text-davinci-003, text-davinci-002, davinci, curie, babbage, ada)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96e85668-3de5-4fa8-9fc6-b84ced382eea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "text = f\"\"\"\n",
    "You should express what you want a model to do by \\ \n",
    "providing instructions that are as clear and \\ \n",
    "specific as you can possibly make them. \\ \n",
    "This will guide the model towards the desired output, \\ \n",
    "and reduce the chances of receiving irrelevant \\ \n",
    "or incorrect responses. Don't confuse writing a \\ \n",
    "clear prompt with writing a short prompt. \\ \n",
    "In many cases, longer prompts provide more clarity \\ \n",
    "and context for the model, which can lead to \\ \n",
    "more detailed and relevant outputs.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple backticks \\ \n",
    "into a single sentence.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "completion = client.completions.create(model=openai_deploy_name,\n",
    "                                       prompt=prompt,\n",
    "                                       temperature=0,\n",
    "                                       max_tokens=800,\n",
    "                                       #max_tokens=OPENAI_MAX_TOKENS[openai_model_name],\n",
    "                                       top_p=1,\n",
    "                                       frequency_penalty=0,\n",
    "                                       presence_penalty=0,\n",
    "                                       stop=None,\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3708fec3-5174-4b15-b640-03a3dc52b81c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#print(completion.model_dump_json(indent=2))\n",
    "print(completion.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "160e58c2-6b13-451c-aa4d-6c80df632342",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can see that the response has generated much more than a simple answer, but continued the probability-based text generation.  \n",
    "Let's suppose we were using one of the (soon to be deprecated) fine tuned completion models (**model-text-davinci-003**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49bb7a2c-c313-448e-b4ac-b8b08907262a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "completion2 = client.completions.create(model=\"model-text-davinci-003\",\n",
    "                                        prompt=prompt,\n",
    "                                        temperature=0,\n",
    "                                        max_tokens=800,\n",
    "                                        #max_tokens=OPENAI_MAX_TOKENS[openai_model_name],\n",
    "                                        top_p=1,\n",
    "                                        frequency_penalty=0,\n",
    "                                        presence_penalty=0,\n",
    "                                        stop=None,\n",
    "                                        )\n",
    "\n",
    "print(completion2.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf19e633-2f6c-431f-9a48-9ca8a34691c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can see that the response is very different from just predicting next sentences.  \n",
    "But as these models are being discontinued, we should focus on Chat Completions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b17ce033-b09f-403f-b74d-eb446ecfe881",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Chat Completions  \n",
    "\n",
    "An alternative to completions are chat completions that are GPT models that have been optimized for conversational text (gpt-4, gpt-3.5-turbo). We are probably most familiar with this GPT type as the underlying GPT 3.5 and their flagship GPT 4 are powering the vastly popular ChatGPT. An added benefit of chat completions is that they are less prone to prompt injection attacks as user-provided content is separate from instruction prompt.  \n",
    "\n",
    "Within the OpenAI API, messages often adopt [specific roles](https://www.tinydesk.ai/post/unlocking-the-potential-understanding-the-roles-in-chatgpt-s-api-system-user-assistant-and) to guide the model’s responses. Commonly used roles include “system,” “user,” and “assistant.”  \n",
    "+ The “system” provides high-level instructions\n",
    "+ The “user” presents queries or prompts  \n",
    "+ The “assistant” is the model’s response.  \n",
    "\n",
    "By differentiating these roles, we can set the context and direct the conversation efficiently.  \n",
    "There is also the role \"function\", that we will present later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eec303e-128c-47aa-bf6d-25269425ed47",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Example #1    \n",
    "(reusing the prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aef53f0-cb1d-45be-8611-2b72f0899d9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    model=openai_deploy_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{prompt}\"},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "138005b1-ba18-4644-943a-458640dee80e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc7a9d11-4244-4d37-8fd7-0d308490f55d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Example #2  \n",
    "(Adding some history of conversations to the request)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2ae6179-ad10-40e1-b851-522d30a815cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    model=openai_deploy_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},\n",
    "        {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e3c7809-d1d1-4369-8cf3-b4c3f6f683e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#print(chat_completion.model_dump_json(indent=2))\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "628fc194-66e3-4a58-9285-8d898cd6fb83",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Wraping in a function:  \n",
    "\n",
    "We could encapsulate all the commands in one function, as seen below. This could be customized to any specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2779b1a-12d3-4b1c-b138-029a310901ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ask(prompt):\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            model=openai_deploy_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{prompt}\"},\n",
    "            ],\n",
    "            )\n",
    "        print(chat_completion.choices[0].message.content)\n",
    "\n",
    "    except openai.error.APIError as e:\n",
    "        print(f\"OpenAI API returned an API Error: {e}\")\n",
    "    except openai.error.AuthenticationError as e:\n",
    "        print(f\"OpenAI API returned an Authentication Error: {e}\")\n",
    "    except openai.error.APIConnectionError as e:\n",
    "        print(f\"Failed to connect to OpenAI API: {e}\")\n",
    "    except openai.error.InvalidRequestError as e:\n",
    "        print(f\"Invalid Request Error: {e}\")\n",
    "    except openai.error.RateLimitError as e:\n",
    "        print(f\"OpenAI API request exceeded rate limit: {e}\")\n",
    "    except openai.error.ServiceUnavailableError as e:\n",
    "        print(f\"Service Unavailable: {e}\")\n",
    "    except openai.error.Timeout as e:\n",
    "        print(f\"Request timed out: {e}\")\n",
    "    except:\n",
    "        print(\"An exception has occured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7268bfb9-e4a9-4d76-9d27-12634c4fa3db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ask(prompt=prompt)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1-ConnectingAPI",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
