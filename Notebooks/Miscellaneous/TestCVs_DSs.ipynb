{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71489e69-5931-4029-a3be-b2244a132d98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -qU openai\n",
    "!pip install -qU langchain \n",
    "!pip install -qU langchain-openai\n",
    "!pip install -qU InstructorEmbedding\n",
    "!pip install -qU transformers\n",
    "!pip install -qU chromadb\n",
    "!pip install -qU faiss-cpu\n",
    "!pip install -qU tiktoken\n",
    "!pip install -q pydantic==1.10.9  #https://stackoverflow.com/questions/76934579/pydanticusererror-if-you-use-root-validator-with-pre-false-the-default-you\n",
    "!pip install -q urllib3==1.26.18\n",
    "!pip install -q requests==2.28.1\n",
    "!pip install -qU SQLAlchemy\n",
    "#!pip install -qU docarray\n",
    "#!pip install -qU python-docx\n",
    "!pip install -qU pypdf\n",
    "!pip install -qU docx2txt\n",
    "#!pip install -qU unstructured[pdf]\n",
    "!pip install -qU lark\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20976513-04b6-4f3e-9a34-3e3664e5c28d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "import glob\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "#from funcy import lcat, lmap, linvoke\n",
    "\n",
    "## Langchain LLM Objects\n",
    "import openai\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_openai import AzureOpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "## Langchain Prompt Templates\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "## Langchain Chains \n",
    "#from langchain.chains import ConversationChain\n",
    "#from langchain.chains import LLMChain\n",
    "#from langchain.chains import ConversationChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "#from langchain.chains.mapreduce import MapReduceChain\n",
    "#from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "#from langchain.chains import SimpleSequentialChain\n",
    "#from langchain.chains import SequentialChain\n",
    "#from langchain.chains.router import MultiPromptChain\n",
    "#from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "\n",
    "## Langchain Output Parsers \n",
    "#from langchain.output_parsers import ResponseSchema\n",
    "#from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "## Langchain Memory \n",
    "#from langchain.memory import ConversationBufferMemory\n",
    "#from langchain.memory import ConversationBufferWindowMemory\n",
    "#from langchain.memory import ConversationTokenBufferMemory\n",
    "#from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "## Langchain Text Splitters\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#from langchain.text_splitter import TokenTextSplitter\n",
    "#from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "## Langchain Document Object and Loaders\n",
    "#from docx import Document\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.schema import Document as LangchainDocument\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "## Langchain Vector Databases\n",
    "#from langchain.vectorstores import DocArrayInMemorySearch\n",
    "#from langchain.vectorstores.base import VectorStore\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "## Langchain  Embedding Models\n",
    "#from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "\n",
    "## Langchain retrievers\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever ## Error\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers import SVMRetriever\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = \"/Workspace/ds-academy-research/Models/\"\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "azure_endpoint = \"https://rg-rbi-aa-aitest-dsacademy.openai.azure.com/\"\n",
    "#azure_endpoint = \"https://chatgpt-summarization.openai.azure.com/\"\n",
    "\n",
    "openai.api_version = \"2023-07-01-preview\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "deployment_name = \"model-gpt-35-turbo\"\n",
    "openai_model_name = \"gpt-35-turbo\"\n",
    "\n",
    "client = AzureOpenAI(api_key=openai.api_key,\n",
    "                     api_version=openai.api_version,\n",
    "                     azure_endpoint=azure_endpoint,\n",
    "                     )\n",
    "\n",
    "chat = AzureChatOpenAI(azure_endpoint=azure_endpoint,\n",
    "                       openai_api_version=openai.api_version,\n",
    "                       deployment_name=deployment_name,\n",
    "                       openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "                       openai_api_type=openai.api_type,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e066aa5-be31-4057-b98b-a5ab60469214",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Examining files in the examples folder  \n",
    "It may be necessary to change the default folder for your documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fe6ebab-b944-45a4-b74d-898c3c17b6aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fullpath = \"/Workspace/ds-academy-research/Docs/Test_CVs/\"\n",
    "docs = os.listdir(fullpath)\n",
    "docs = [d for d in docs if d.endswith(\".pdf\")]\n",
    "docs.sort()\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bb62b22-ab36-4d18-b20f-fdb3696f7c62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "for filename in os.listdir(fullpath):\n",
    "    if filename.endswith('.pdf'):\n",
    "        print(f\"Ingesting document {filename}\")\n",
    "        pdf_path = fullpath + filename\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents.extend(loader.load())\n",
    "print(f\"We have {len(documents)} pages from all the pdf files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a19c1662-b7c4-4e1a-9baa-2e2eadd6fe75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, \n",
    "                                               chunk_overlap=200,\n",
    "                                               separators=[\"\\n\\n\", \"\\n\", \"\\. \", \" \", \"\"],\n",
    "                                               length_function=len\n",
    "                                               )\n",
    "chunked_documents = text_splitter.split_documents(documents)\n",
    "print(len(chunked_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b7d52b6-27d4-49ab-a23a-e4698bccaae3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#openai_embeddings = OpenAIEmbeddings(deployment=\"model-text-embedding-ada-002\", chunk_size = 1)\n",
    "\n",
    "instruct_embeddings = HuggingFaceInstructEmbeddings(query_instruction=\"Represent the query for retrieval: \", model_name=\"hkunlp/instructor-xl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa82f488-a364-4d2f-af2b-3f096a716d9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "files = glob.glob('/Workspace/ds-academy-research/VectorDB_CVs/*')\n",
    "\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "faissdb = FAISS.from_documents(chunked_documents, \n",
    "                                embedding=instruct_embeddings,\n",
    "                               )\n",
    "#print(f\"There are {vectordb.ntotal} documents in the index\")\n",
    "faissdb.save_local('/Workspace/ds-academy-research/VectorDB_CVs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5748a33c-a5b3-452f-9604-a123a4b61b6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "docsearch = FAISS.load_local(\"/Workspace/ds-academy-research/VectorDB_CVs/\", instruct_embeddings)\n",
    "print(len(docsearch.index_to_docstore_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c0bfc43-39fb-4e62-9ed7-aa90193ea1f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Search using a score function and a maximum number of documents in return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f605abb0-cfc4-418e-b765-3957d28bc9ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"Python\"\n",
    "result = docsearch.similarity_search_with_score(query, k=2)\n",
    "for r in result:\n",
    "    print(r)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55a58498-6930-45d5-8572-ce2501ed5a5a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Addressing Diversity: Maximum marginal relevance\n",
    "\n",
    "`Maximum marginal relevance` strives to achieve both relevance to the query *and diversity* among the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eed6987d-b91f-4dc0-bae9-8058ba9e417f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"Python\"\n",
    "result = docsearch.max_marginal_relevance_search(query, k=2)\n",
    "for r in result:\n",
    "    print(r)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "153ab76a-c3a7-4d1e-8e0f-8c36e3f0af89",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Addressing Specificity: working with metadata\n",
    "\n",
    "To address this, many vectorstores support operations on `metadata`.\n",
    "\n",
    "`metadata` provides context for each embedded chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17f60291-c424-48b7-a891-1b657be5d752",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"Whom is this curriculum about?\"\n",
    "for cv in docs:\n",
    "    result = docsearch.similarity_search(query, k=1, filter={\"source\":f\"/Workspace/ds-academy-research/Docs/Test_CVs/{cv}\"})\n",
    "    for r in result:\n",
    "        print(r)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f245edf-79c7-49e8-9127-3bd1c0de5038",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### [Stuffing Chain](https://medium.com/@minh.hoque/what-are-llm-chains-671b84103ba9)\n",
    "The Stuffing chain serves as a solution for scenarios where the context length of the LLM is inadequate to handle extensive documents or a substantial amount of information. In such cases, a large document can be divided into smaller segments, and semantic search techniques can be employed to retrieve relevant documents based on the query. These retrieved documents are then “stuffed” into the LLM context, allowing for the generation of a response.\n",
    "\n",
    "Benefits:\n",
    "\n",
    "+ Consolidation of multiple documents: The Stuffing chain allows the aggregation of several relevant documents, overcoming the context length limitation of LLMs for large documents.\n",
    "Comprehensive information processing: By leveraging multiple documents, the chain can generate more comprehensive and relevant answers.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "+ Increased complexity: Retrieving relevant document requires good semantic search and vector database.\n",
    "+ Potential loss of contextual coherency: Since we are retrieving N documents, the LLM might not have all relevant context to generate a cohesive answer.\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "+ Document Retrieval Question Answering: Utilizing the Stuffing chain, document chunks retrieved of the larger document can be effectively leveraged to provide accurate answers to your questions. For example, suppose you have a lengthy legal document and need to find specific answers to legal questions. By using the Stuffing chain, you can break down the document into smaller chunks, retrieve relevant chunks based on the question, and utilize the information within those chunks to generate accurate answers.\n",
    "+ Complex Question Answering: When answering complex questions that require information from diverse sources, the Stuffing chain can provide more comprehensive and accurate responses. For instance, imagine you have a research project that requires answering complex scientific queries. The Stuffing chain allows you to divide relevant scientific papers into smaller chunks, retrieve the necessary information from these chunks, and synthesize it to provide a thorough and precise response to the complex question at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c740a091-6ce5-4aa1-9d69-f5d99b3ac52b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=chat,\n",
    "                                       retriever=docsearch.as_retriever(),\n",
    "                                       #retriever=docsearch.as_retriever(search_kwargs={'k': 7}),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type=\"stuff\",\n",
    "                                       )\n",
    "\n",
    "query = \"What is an extreme outlier?\"\n",
    "result = qa_chain(query)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82aff5ec-7c08-4fe3-b458-c276a90df5db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### [Map-Reduce Chain](https://medium.com/@minh.hoque/what-are-llm-chains-671b84103ba9)\n",
    "The Map-Reduce chain enables the iteration over a list of documents, generating individual outputs for each document, which can later be combined to produce a final result. This chain is useful for tasks that involve processing documents in parallel and then aggregating the outputs. \n",
    "\n",
    "Benefits:\n",
    "\n",
    "+ Parallel processing: The Map-Reduce chain allows for parallel execution of the language model on individual documents, improving efficiency and reducing processing time.\n",
    "+ Scalability: The chain can handle large collections of documents by distributing the processing load across multiple iterations.\n",
    "+ Enhanced information extraction: By generating individual outputs for each document, the chain can extract specific information that contributes to a more comprehensive final result.  \n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "+ Complexity in output aggregation: Combining the outputs of multiple iterations requires careful handling to ensure coherency and meaningful synthesis.\n",
    "+ Potential redundancy: In some cases, the individual outputs of the Map-Reduce chain may contain redundant information, necessitating further post-processing steps.\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "+ Multiple document summarization: The Map-Reduce chain can be used to generate summaries for many documents and then to combine the singular summaries to create a final comprehensive summary for the whole group of documents. For example, imagine you have a collection of research papers on a particular topic. By employing the Map-Reduce chain, you can generate summaries for each research paper, and finally merge the individual summaries to produce a comprehensive summary that captures the key information from the entire collection of papers. This approach enables efficient and accurate summarization of large volumes of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af69b812-07b4-4352-bccf-5a799b925a6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=chat,\n",
    "                                       retriever=docsearch.as_retriever(),\n",
    "                                       #retriever=docsearch.as_retriever(search_kwargs={'k': 7}),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type=\"map_reduce\",\n",
    "                                       )\n",
    "\n",
    "query = \"What is an extreme outlier?\"\n",
    "result = qa_chain(query)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e75b453-8d32-4a36-a87d-c8a765569f57",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### [Refine Chain](https://medium.com/@minh.hoque/what-are-llm-chains-671b84103ba9)  \n",
    "The Refine chain focuses on iterative refinement of the output by feeding the output of one iteration into the next, aiming to enhance the accuracy and quality of the final result.\n",
    "\n",
    "Benefits:\n",
    "\n",
    "+ Continuous improvement: The Refine chain allows for progressive refinement of the output by iteratively updating and enhancing the information.\n",
    "Enhanced accuracy: By refining the output in each iteration, the chain can improve the accuracy and relevance of the final result.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "+ Increased computational resources: The iterative nature of the Refine chain may require additional computational resources compared to non-iterative approaches.\n",
    "+ Longer processing time: Each iteration adds to the overall processing time, which may be a consideration when real-time or near-real-time responses are required.\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "+ Long-form text generation: The Refine chain proves exceptionally valuable in the creation of extensive text compositions, such as essays, articles, or stories, where the iterative refinement process greatly enhances coherence and readability. For instance, envision interacting with a substantial research paper and progressively employing the LLM to craft an abstract, refining it with each iteration to achieve an optimal outcome.\n",
    "+ Answer synthesis: The Refine chain demonstrates its prowess in synthesizing answers derived from multiple sources or generating comprehensive responses. Through iterative refinement, the chain progressively improves the accuracy and comprehensiveness of the final answer. This capability is especially advantageous when each retrieved document contributes crucial context to the answer generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97731ce8-edac-49d4-9a47-ab6f5650c35e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=chat,\n",
    "                                       retriever=docsearch.as_retriever(),\n",
    "                                       #retriever=docsearch.as_retriever(search_kwargs={'k': 7}),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type=\"refine\",\n",
    "                                       )\n",
    "\n",
    "query = \"What is an extreme outlier?\"\n",
    "result = qa_chain(query)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68c697e0-ce52-4c66-bbac-0b0fd3df3fc6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### [Map Rerank](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.map_rerank.MapRerankDocumentsChain.html#)  \n",
    "\n",
    "Combining documents by mapping a chain over them, then reranking results.\n",
    "This algorithm calls an LLMChain on each input document. The LLMChain is expected to have an OutputParser that parses the result into both an answer (answer_key) and a score (rank_key). The answer with the highest score is then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c2d128b-a560-4e2a-9fd6-365aea634359",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=chat,\n",
    "                                       retriever=docsearch.as_retriever(),\n",
    "                                       #retriever=docsearch.as_retriever(search_kwargs={'k': 7}),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type=\"map_rerank\",\n",
    "                                       )\n",
    "\n",
    "query = \"What is an extreme outlier?\"\n",
    "result = qa_chain(query)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e499363-81ce-4333-8d6b-dca3b8292252",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Adding Chat History\n",
    "Now, if we want to take things one step further, we can also make it so that our chatbot will remember any previous questions.\n",
    "\n",
    "Implementation-wise, all that happens is that on each interaction with the chatbot, all of our previous conversation history, including the questions and answers, needs to be passed into the prompt. That is because the LLM does not have a way to store information about our previous requests, so we must pass in all the information on every call to the LLM.\n",
    "\n",
    "Fortunately, LangChain also has a set of classes that let us do this out of the box. This is called the ConversationalRetrievalChain, which allows us to pass in an extra parameter called chat_history , which contains a list of our previous conversations with the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12291ac4-ee85-404a-80da-d9c483f6e080",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qa_chain = ConversationalRetrievalChain.from_llm(llm=chat,\n",
    "                                                 retriever=docsearch.as_retriever(),\n",
    "                                                 return_source_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb4eff5d-2111-4d0d-bbc1-670212d8c126",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The chain run command accepts the chat_history as a parameter. We must manually build up this list based on our conversation with the LLM.  \n",
    "The chain does not do this out of the box, so for each question and answer, we will build up a list called chat_history , which we will pass back into the chain run command each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b786400c-1df3-4496-a930-0961e645c7bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "while True:\n",
    "    # this prints to the terminal, and waits to accept an input from the user\n",
    "    query = input('Prompt: ')\n",
    "    # give us a way to exit the script\n",
    "    if query == \"exit\" or query == \"quit\" or query == \"q\":\n",
    "        print('Exiting')\n",
    "        break\n",
    "    # we pass in the query to the LLM, and print out the response. As well as\n",
    "    # our query, the context of semantically relevant information from our\n",
    "    # vector store will be passed in, as well as list of our chat history\n",
    "    result = qa_chain({'question': query, 'chat_history': chat_history})\n",
    "    print('Answer: ' + result['answer'])\n",
    "    # we build up the chat_history list, based on our question and response\n",
    "    # from the LLM, and the script then returns to the start of the loop\n",
    "    # and is again ready to accept user input.\n",
    "    chat_history.append((query, result['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cef3e6d9-6f73-462b-858c-0f0646f9ed61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chat_history"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 295794915741311,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "TestCVs_DSs",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
