{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62bf611b-a1ba-4408-ab05-4d1eacaa317d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#Ask Questions to your Document\n",
    "\n",
    "In this example we are demonstrating how to ask questions to a PDF file.\n",
    "The PDF file is assumed to have been uploaded to the DBFS via a SFTP connection and stored in your user workspace in some folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cde8151-9a50-4b40-9b1a-cc10b42266ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# 1. Step: Set-up a cluster\n",
    "<p>\n",
    "- Go to the menu item Compute<p>\n",
    "- Create a new cluster with the 'Create Compute' button<p>\n",
    "- Choose a use case you access to (cf. Policy)<p>\n",
    "- Choose Multi node<p>\n",
    "- Choose Access mode 'No Isolation shared'<p>\n",
    "- Pick the latest databricks runtime<p>\n",
    "- Choose as worker type: g5.4xlarge with: Min workers:2, Max workers: 8<p>\n",
    "- Choose Driver type: g5.4xlarge<p>\n",
    "- Enable autoscaling<p>\n",
    "- Pick a Terminate time, e.g. 240 min.<p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33c9ba68-c155-4738-8f52-826cb6762076",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2. Step: Install all libraries\n",
    "<p>\n",
    "<em>Comment: Execution takes about 1,5 minute</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2650501f-5177-4be6-8877-9900f68b23a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# 2. Loading all relevant libraries\n",
    "# -----------------------------------------------\n",
    "%pip install funcy\n",
    "%pip install huggingface_hub\n",
    "%pip install InstructorEmbedding\n",
    "%pip install langchain\n",
    "%pip install chromadb\n",
    "%pip install openpyxl\n",
    "%pip install python-docx\n",
    "%pip install sentence-transformers\n",
    "%pip install tiktoken\n",
    "%pip install torch\n",
    "%pip install pypdf\n",
    "%pip install xformers\n",
    "%pip install langchainhub\n",
    "%pip install llama-cpp-python\n",
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74a154d4-9b64-4931-8d00-2cdf9f72da61",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Step 3: Set-up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcac10f8-6547-4461-b1a5-555254f2c04e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# all the function definitions\n",
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.embeddings.huggingface import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from functools import partial\n",
    "from funcy import lmap\n",
    "from typing import Tuple, Callable\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b946bc2-e23c-4fa5-afe7-abad7fcf5bf9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Providing the access token to Azure OpenAI\n",
    "# This only works if you have access to the respective use cases\n",
    "# --------------------------------------------------------------\n",
    "os.environ[\"OPENAI_API_KEY\"] = dbutils.secrets.get(scope=\"llm-usecases\", key=\"AZURE_TOKEN\")\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2022-12-01\"\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2022-12-01\"\n",
    "openai.api_base = \"https://rg-rbi-aa-aitest-semantic-vectordb.openai.azure.com/\"\n",
    "\n",
    "openAI_text_llm = AzureOpenAI(deployment_name=\"model-text-davinci-003\", temperature=0)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Load OpenAI Chat model\n",
    "# -----------------------------------------------------------------\n",
    "version = \"2023-07-01-preview\"\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = dbutils.secrets.get(scope=\"llm-usecases\", key=\"AZURE_TOKEN\")\n",
    "os.environ[\"OPENAI_API_VERSION\"] = version\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://rg-rbi-aa-aitest-semantic-vectordb.openai.azure.com/\"\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = version\n",
    "openai.api_base = \"https://rg-rbi-aa-aitest-semantic-vectordb.openai.azure.com/\"\n",
    "\n",
    "openAI_chat_llm = AzureChatOpenAI(deployment_name=\"model-gpt-35-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf17f281-af4a-4109-8e23-3bf05ce850a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------\n",
    "# First Register with your company email to Huggingface (free of charge) https://huggingface.co/\n",
    "# Second you get an access token via the section: user profile/edit profile/Access Token\n",
    "# Third register for Llama2 (https://ai.meta.com/resources/models-and-libraries/llama-downloads/). It is free of charge but does require registration. Make sure that you use the same email address as you used for Huggingface\n",
    "\n",
    "from getpass import getpass\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=getpass(\"Huggingface Token:\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acd31cfe-1fb5-4d87-b2b6-bb0cc3aa22c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Step 4 - Load required components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df7ecf67-a299-4a16-9602-d708edaefc15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Initialized and load llama-2-7b-chat-hf\n",
    "# Note: You might also use llama-2-13b-chat-hf, but experiment first with the smaller model\n",
    "# Note: Not recommended! 70b variant does work on a 4xA10G GPU but only in 8bit, which is rather slow.\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "#model = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "\n",
    "# Start with the Autotokenizer, but you might also try other tokenizers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "# Start with a rather simple pipeline, and then become more sophisticated\n",
    "llama_chat = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    temperature=0.05,\n",
    "    max_new_tokens=400,\n",
    "    #trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df6a0295-684e-4ac7-8863-d7fd7a441c3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------\n",
    "# Load InstructXL embeddings used for OpenAI GPT models\n",
    "# -----------------------------------------------------\n",
    "instruct_embeddings = HuggingFaceInstructEmbeddings(\n",
    "    query_instruction=\"Represent the query for retrieval: \", \n",
    "    model_name=\"hkunlp/instructor-xl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac311333-0029-4385-aca9-aed471b5ebed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Chatting with your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56322b2a-4fbd-44b8-8446-9aaca89ea5fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------\n",
    "# Llama2 Set-up\n",
    "# -----------\n",
    "question_template=\"Please answer the following question as diligently as possible: {question}\"\n",
    "\n",
    "def llama_chat_completion(question: str, pipeline: transformers.Pipeline, **pipeline_kwargs: dict[str, Any]) -> list[str]:\n",
    "    query = question_template.format(question=question)\n",
    "    sequences = pipeline(query, **pipeline_kwargs)\n",
    "    return [s[\"generated_text\"] for s in sequences]\n",
    "\n",
    "\n",
    "llama_chat_completion = partial(llama_chat_completion, \n",
    "    pipeline=llama_chat,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.05,\n",
    "    top_k=5,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01137eac-420b-4895-adec-fe1829d387a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Put here your question\n",
    "question =\"You are a market reasearch anaylsts. Plrase project the consumer price index to the years 2024 and 2025?\"\n",
    "\n",
    "# Calculate the answer with Llama2\n",
    "answer = llama_chat_completion(question)[0]\n",
    "print(f\"Question: {question}. \\n Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90558961-3ce9-4fe4-8794-e0345bd177e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Chat with GPT 3.5\n",
    "# -------------------------\n",
    "SystemPrompt = \"Please answer the following question as diligently as possible.\"\n",
    "UserPrompt = \"You are a market reasearch anaylsts. Plrase give me the numbers for projecting the consumer price index to the years 2024 and 2025?\"\n",
    "\n",
    "response = openAI_chat_llm([\n",
    "    SystemMessage(content=SystemPrompt),\n",
    "    HumanMessage(content=UserPrompt)\n",
    "])\n",
    "\n",
    "answer = response.content\n",
    "\n",
    "print(f\"Question: {UserPrompt}. \\n Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "931be9b8-8422-4710-bc90-63cd23ead3cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Ask your Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0333f9ea-dc2c-44f5-bd7a-afb67d911c70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# GPT 3.5 / Load your PDF to the indicated path and ask questions to it\n",
    "# ------------------------------------------------------------\n",
    "#selected_file_path = \"/Workspace/Users/david.eschwe@rbinternational.com/PDFs/Transformer.pdf\"\n",
    "selected_file_path = \"/Workspace/Users/david.eschwe@rbinternational.com/PDFs/RBI Group Risk Manual.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(selected_file_path)\n",
    "\n",
    "document = loader.load()\n",
    "documents_content = '\\n'.join(page.page_content for page in document)\n",
    "\n",
    "len(documents_content)\n",
    "\n",
    "nb_characters = 400\n",
    "\n",
    "print(f\"First {nb_characters} Characters of the Paper: \\n{documents_content[:nb_characters]} ...\")\n",
    "print(f\"Lenght of Document: {len(documents_content)}\")\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "doc_chunks = text_splitter.split_text(documents_content)\n",
    "\n",
    "print(f\"# Chunks in Document: {len(doc_chunks)}\")\n",
    "\n",
    "vector_db = Chroma.from_texts(doc_chunks, instruct_embeddings)\n",
    "chain = load_qa_chain(openAI_text_llm, chain_type=\"stuff\")\n",
    "\n",
    "def ask(question:str):\n",
    "    retriever = vector_db.as_retriever()\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    answer = chain.run(input_documents=docs, question=question).strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f7162c2-0a7c-4fb6-9ade-1ce25c5efff3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "question = \"What is a transformer\"\n",
    "answer = ask(question)\n",
    "\n",
    "print(f\"GPT3.5 // Question: {question}\\n\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2d75b40-4d75-44a7-b516-fb7e9f99b058",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Llama2 / Load your PDF to the indicated path and ask questions to it\n",
    "# ------------------------------------------------------------\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=llama_chat)\n",
    "retriever = vector_db.as_retriever()\n",
    "\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "def query(question, verbose=False):\n",
    "    retriever = vector_db.as_retriever()\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    return chain.run(input_documents=docs, question=question).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81f6ac37-9377-4de5-90d3-3a0f1e62d2bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "question = \"What is an LRG?\"\n",
    "\n",
    "answer = query(question)\n",
    "\n",
    "print(f\"Llama2 // Question: {question}\\n\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb91d3e0-98bf-4b44-a567-b2d66e2a451d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "AskYourPDF",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
