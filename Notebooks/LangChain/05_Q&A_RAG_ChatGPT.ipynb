{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -1145767553588021,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de6a690f-4fa9-4835-ad60-fd05644c60d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## LangChain: Q&A over Documents\n",
    "\n",
    "Sources: [Here](https://learn.deeplearning.ai/langchain/lesson/5/question-and-answer),\n",
    "[here](https://betterprogramming.pub/building-a-multi-document-reader-and-chatbot-with-langchain-and-chatgpt-d1864d47e339) and \n",
    "[here](https://python.langchain.com/docs/integrations/vectorstores/faiss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -1145767553588021,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a292d0e8-9bed-4f8d-9af0-edac213f873f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1 - Installing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71489e69-5931-4029-a3be-b2244a132d98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!pip install -qU docarray\n",
    "#!pip install -qU python-docx\n",
    "!pip install -qU pypdf\n",
    "!pip install -qU docx2txt\n",
    "!pip install -qU transformers\n",
    "!pip install -qU InstructorEmbedding\n",
    "!pip install -q pydantic==1.10.9  #https://stackoverflow.com/questions/76934579/pydanticusererror-if-you-use-root-validator-with-pre-false-the-default-you\n",
    "!pip install -qU unstructured[pdf]\n",
    "#!pip install -qU chromadb\n",
    "!pip install -qU faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fad5fde-e022-40ad-a506-d3c2d87ce44f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -1145767553588021,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8aeef1e-1e22-4b3d-b2bf-6bafdd5f6fdd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2 - Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20976513-04b6-4f3e-9a34-3e3664e5c28d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "#from docx import Document\n",
    "import tiktoken\n",
    "#from funcy import lcat, lmap, linvoke\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## Langchain LLM Objects\n",
    "import openai\n",
    "#from langchain.llms import OpenAI\n",
    "from langchain.llms import AzureOpenAI\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "## Langchain Prompt Templates\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "## Langchain Chains \n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "## Langchain Memory \n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "## Langchain Text Splitters\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#from langchain.text_splitter import TokenTextSplitter\n",
    "#from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "## Langchain Document Object and Loaders\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.schema import Document as LangchainDocument\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "## Langchain Vector Databases\n",
    "#from langchain.vectorstores import DocArrayInMemorySearch\n",
    "#from langchain.vectorstores.base import VectorStore\n",
    "#from langchain.vectorstores import Chroma\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "## Langchain  Embedding Models\n",
    "#from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "\n",
    "## Langchain retrievers\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers import SVMRetriever\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "\n",
    "#os.environ['TRANSFORMERS_CACHE'] = \"/Workspace/ds-academy-embedded-wave-4/Models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -1145767553588021,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43cf7b28-b48a-437f-a562-d0ab4001585a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3 - Loading the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47931d96-249a-4422-b4ba-a0dc2eedaa82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://rg-rbi-aa-aitest-dsacademy.openai.azure.com/\"\n",
    "#openai.api_base = \"https://chatgpt-summarization.openai.azure.com/\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "openai_model_name = \"gpt-35-turbo\"\n",
    "openai_deploy_name = \"model-gpt-35-turbo\"\n",
    "openai.api_version = \"2023-07-01-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26138747-b5d4-46bc-9baa-abec84d0714e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chat = AzureChatOpenAI(openai_api_base=openai.api_base,\n",
    "                      openai_api_version=openai.api_version,\n",
    "                      deployment_name=openai_deploy_name,\n",
    "                      openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "                      openai_api_type=openai.api_type,\n",
    "                      temperature=0.2,\n",
    "                      #max_tokens=4000,\n",
    "                      )\n",
    "\n",
    "\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -1145767553588021,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e066aa5-be31-4057-b98b-a5ab60469214",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4 - Examining files in the examples folder  \n",
    "It may be necessary to change the default folder for your documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fe6ebab-b944-45a4-b74d-898c3c17b6aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fullpath = \"/Workspace/ds-academy-embedded-wave-4/ExampleDocs/\"\n",
    "docs = os.listdir(fullpath)\n",
    "docs = [d for d in docs if d.endswith(\".pdf\")]\n",
    "docs.sort()\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -1145767553588021,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fe28b21-fd6e-4c89-ba9e-6496daaeb04f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5 - LangChain: Creating a Document Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -1145767553588021,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66926874-16b2-43a8-b630-c3e4b8712062",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The simplest Q&A chain implementation we can use is the load_qa_chain.  \n",
    "It loads a chain that allows you to pass in all of the documents you would like to query against using your LLM. \n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*rF3UlC7vWiVFGlXFNZ1XHw.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -1145767553588021,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d343fd0-4377-4d93-854a-252d04eb80dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 6 - Querying a single PDF document\n",
    "\n",
    "Now we will instantiate the PDF Loader, load one small document and create a list of Langchain documents object\n",
    "\n",
    "Info about the page splitting [here](https://datascience.stackexchange.com/questions/123076/splitting-documents-with-langchain-when-a-sentence-straddles-the-a-page-break)  \n",
    "You can also define your own document splitter using `pdf_loader.load_and_split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaa56086-59ae-4837-aef2-417e71d6e8a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f'Loading Document: {fullpath+docs[0]}')\n",
    "pdf_loader = PyPDFLoader(fullpath+docs[0])\n",
    "documents = pdf_loader.load()\n",
    "print(f\"We have {len(documents)} pages in the pdf file\")\n",
    "\n",
    "print(type(documents))\n",
    "print(type(documents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a196032-0871-4d2c-a479-b8bbf075aeec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chain = load_qa_chain(llm=chat, verbose=False)\n",
    "query = 'What is the document about?'\n",
    "response = chain.run(input_documents=documents, question=query)\n",
    "print(response) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -1145767553588021,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25a81887-3c62-4568-a21b-9aac3cc95018",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### This method is all good when we only have a short amount of information to send in the [context size of our model](https://platform.openai.com/docs/models/overview).  \n",
    "However, most LLMs will have a limit on the amount of information that can be sent in a single request. So we will not be able to send all the information in our documents within a single request.  \n",
    "To overcome this, we need a smart way to send only the information we think will be relevant to our question/prompt.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -1145767553588021,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e6b2680-3238-4f4c-9bad-d6da2030b8e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 7 - Interacting with documents using Embeddings\n",
    "\n",
    "We can use embeddings and vector stores to send only relevant information to our prompt.  \n",
    "The steps we will need to follow are:\n",
    "\n",
    "+ Split all the documents into small chunks of text\n",
    "+ Pass each chunk of text into an embedding transformer to turn it into an embedding\n",
    "+ Store the embeddings and related pieces of text in a vector store, instead of a list of Langchain document objects\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:828/format:webp/1*FWwgOvUE660a04zoQplS7A.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -1145767553588021,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d120d082-42ab-432c-ab0c-a3cdcacc0b3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 7.1 - Creating a list of Document Objects for all PDF files in the examples folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bb62b22-ab36-4d18-b20f-fdb3696f7c62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "for filename in os.listdir(fullpath):\n",
    "    if filename.endswith('.pdf'):\n",
    "        print(f\"Ingesting document {filename}\")\n",
    "        pdf_path = fullpath + filename\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents.extend(loader.load())\n",
    "print(f\"We have {len(documents)} pages from all the pdf files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -1145767553588021,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58eedd71-860e-4e68-b0d0-20831da09f3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 7.2 - Splitting each document into small chunks\n",
    "\n",
    "When we load documents, the splitting is done by pages. We can change that using Splitters, to avoid the limitations on the LLM contexts  \n",
    "\n",
    "Langchain offer different Text Splitters\n",
    "+ RecursiveCharacterTextSplitter: Divides the text into fragments based on characters, starting with the first character. If the fragments turn out to be too large, it moves on to the next character. It offers flexibility by allowing you to define the division characters and fragment size.\n",
    "+ CharacterTextSplitter: Similar to the RecursiveCharacterTextSplitter, but with the ability to define a custom separator for more specific division. By default, it tries to split on characters like “\\n\\n”, “\\n”, “ “, and “”.\n",
    "+ RecursiveTextSplitter: Unlike the previous ones, the RecursiveTextSplitter divides text into fragments based on words or tokens instead of characters. This provides a more semantic view and is ideal for content analysis rather than structure.\n",
    "+ TokenTextSplitter: Uses the OpenAI language model to split text into fragments based on tokens, allowing for precise and contextualized segmentation, ideal for advanced natural language processing applications.\n",
    "+ And some more specific ones\n",
    "\n",
    "\n",
    "We will split the data into chunks of 1,000 characters, with an overlap of 200 characters between the chunks, which helps to give better results and contain the context of the information between chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a19c1662-b7c4-4e1a-9baa-2e2eadd6fe75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, \n",
    "                                               chunk_overlap=200,\n",
    "                                               separators=[\"\\n\\n\", \"\\n\", \"\\. \", \" \", \"\"],\n",
    "                                               length_function=len\n",
    "                                               )\n",
    "chunked_documents = text_splitter.split_documents(documents)\n",
    "print(len(chunked_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -1145767553588021,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0a1df7c-6006-4a3a-af55-27e306c08085",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 7.3 - Choosing a Model to create Embeddings of each chunk:  \n",
    "\n",
    "We could create embeddings with many different transformers. \n",
    "We could have used using **OpenAIEmbeddings**, but then we would have to pay for each token sent to the API. In our case, we will create our vectorDB using **InstructEmbeddings** transformer from **[Hugging Face](https://huggingface.co/hkunlp/instructor-xl)** to provide embeddings from our text chunks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b7d52b6-27d4-49ab-a23a-e4698bccaae3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#openai_embeddings = OpenAIEmbeddings(deployment=\"model-text-embedding-ada-002\", chunk_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ee21e33-a4d6-4c30-8f00-3c241cd9bc41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Alternative\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "\n",
    "modelpath = \"/Workspace/ds-academy-embedded-wave-4/Models/model.bin\"\n",
    "try:\n",
    "    instruct_embeddings = INSTRUCTOR(modelpath)\n",
    "    print(\"Successfully loaded Model locally\")\n",
    "except:\n",
    "    print(\"Loading from the Web\")\n",
    "    instruct_embeddings = INSTRUCTOR('hkunlp/instructor-xl')\n",
    "    #instruct_embeddings.save(modelpath)\n",
    "    #print(f\"Saved model to {modelpath}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9e4e3f8-2624-4443-9ef5-657685cc6701",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "instruct_embeddings = HuggingFaceInstructEmbeddings(query_instruction=\"Represent the query for retrieval: \", model_name=\"hkunlp/instructor-xl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -1145767553588021,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57aab747-e822-4174-81d5-d7deb974f224",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 8 - Creating a Vector Database\n",
    "\n",
    "![Vector Databases](https://miro.medium.com/v2/resize:fit:828/format:webp/1*vIkxM-u3zrkHMZuIRURc0A.png)\n",
    "\n",
    "There are [many Vector Databases](https://thenewstack.io/top-5-vector-database-solutions-for-your-ai-project/)  products, both paid and open source, that could be used. \n",
    "\n",
    "We set all the db information to be stored inside the `/Workspace/ds-academy-embedded-wave-4/VectorDB`, so it doesn't clutter up our source files.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -1145767553588021,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90e96b65-c773-45e8-b330-11dc0cc254d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### We have first tried [ChromaDB](https://www.trychroma.com/), but some incompatibilities with the current versions of Python motivated us to try [FAISS](https://faiss.ai/) (from Meta)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c24cf1f-a247-415c-992b-24d35808f833",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### First attempt with ChromaDb (commented)\n",
    "\n",
    "#vectordb = Chroma.from_documents(chunked_documents,\n",
    "#                                 #embedding=openai_embeddings,\n",
    "#                                 embedding_function=instruct_embeddings,\n",
    "#                                 persist_directory='/Workspace/ds-academy-embedded-wave-4/VectorDB'\n",
    "#)\n",
    "#vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbdcc978-46d2-4ee8-8ea5-41dc526b1a6d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 8.1 - Setting the FAISS Vector DB\n",
    "Deleting previous databases from the folder we have create to store the files (only if creating new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dead1bf-47f2-4aaa-add1-871d965ec131",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "files = glob.glob('/Workspace/ds-academy-embedded-wave-4/VectorDB/*')\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfa51b1a-1edb-43f6-aa5c-7949a3dff8bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Loading all PDF documents into the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa82f488-a364-4d2f-af2b-3f096a716d9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vectordb = FAISS.from_documents(chunked_documents, \n",
    "                                embedding=instruct_embeddings,\n",
    "                               )\n",
    "#print(f\"There are {vectordb.ntotal} documents in the index\")\n",
    "vectordb.save_local('/Workspace/ds-academy-embedded-wave-4/VectorDB/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bcefcb4-e17c-44b2-9df8-7cc5528c610b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 9 - Retrieval Tasks  \n",
    "\n",
    "Once we have loaded our content as embeddings into the vector store, we are back to a similar situation as to when we only had one PDF to interact with. As in, we are now ready to pass information into the LLM prompt.  \n",
    "However, instead of passing in all the documents as a source for our context to the chain, as we did initially, we will pass in our vector store as a source/retriever, and the chain will retrieve only the relevant text based on our question and send that information only inside the LLM prompt.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:828/format:webp/1*leoW-Pn0ohWalrUBbzdidA.png)\n",
    "\n",
    "First we will only use the RetrievalQA chain, which will use our vector store as a source for the context information.\n",
    "\n",
    "Again, the chain will wrap our prompt with some text, instructing it to only use the information provided for answering the questions.  \n",
    "So the prompt we end up sending to the LLM something that looks like this:\n",
    "\n",
    "    Use the following pieces of context to answer the question at the end.\n",
    "    If you don't know the answer, just say that you don't know, don't try to\n",
    "    make up an answer.\n",
    "\n",
    "    {context} // i.e the chunks of text retrieved deemed to be most semantically\n",
    "              // relevant to our question\n",
    "\n",
    "    Question: {query} // i.e our actualy query\n",
    "    Helpful Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00654f2b-4aad-4b4f-b067-535d1935c369",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 9.1 - Loading the recently created Vector Database object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5748a33c-a5b3-452f-9604-a123a4b61b6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "docsearch = FAISS.load_local(\"/Workspace/ds-academy-embedded-wave-4/VectorDB/\", instruct_embeddings)\n",
    "print(len(docsearch.index_to_docstore_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58e47ab3-9a0d-4148-81fe-0f483a1a5902",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 9.2 - Retrieving chunks of Documents using Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e170d81-79a9-44fd-ac9e-13b9d7e421a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"Natural Language Processing\"\n",
    "result = docsearch.similarity_search(query)\n",
    "print(result[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c0bfc43-39fb-4e62-9ed7-aa90193ea1f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 9.3 - Search using a score function and a maximum number of documents in return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f605abb0-cfc4-418e-b765-3957d28bc9ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"Natural Language Processing\"\n",
    "result = docsearch.similarity_search_with_score(query, k=2)\n",
    "for r in result:\n",
    "    print(r)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55a58498-6930-45d5-8572-ce2501ed5a5a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 9.4 - Addressing Diversity: Maximum marginal relevance\n",
    "\n",
    "`Maximum marginal relevance` strives to achieve both relevance to the query *and diversity* among the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eed6987d-b91f-4dc0-bae9-8058ba9e417f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"Natural Language Processing\"\n",
    "result = docsearch.max_marginal_relevance_search(query, k=2)\n",
    "for r in result:\n",
    "    print(r)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "153ab76a-c3a7-4d1e-8e0f-8c36e3f0af89",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 9.5 - Addressing Specificity: working with metadata\n",
    "\n",
    "To address this, many vectorstores support operations on `metadata`.\n",
    "\n",
    "`metadata` provides context for each embedded chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17f60291-c424-48b7-a891-1b657be5d752",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"Natural Language Processing\"\n",
    "result = vectordb.similarity_search(query,\n",
    "                                    k=3,\n",
    "                                    filter={\"source\":\"/Workspace/ds-academy-embedded-wave-4/ExampleDocs/37pagesPDF.pdf\"}\n",
    "                                    )\n",
    "for r in result:\n",
    "    print(r)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6af0df61-a9fa-4d51-81c3-660499e2e23f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 9.6 - Addressing Specificity: working with metadata using self-query retriever\n",
    "\n",
    "But we have an interesting challenge: we often want to infer the metadata from the query itself.\n",
    "\n",
    "To address this, we can use `SelfQueryRetriever`, which uses an LLM to extract:\n",
    " \n",
    "1. The `query` string to use for vector search\n",
    "2. A metadata filter to pass in as well\n",
    "\n",
    "Most vector databases support metadata filters, so this doesn't require any new databases or indexes.\n",
    "\n",
    "##### Disclaimer: Not implemented for FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fffb8c2-753e-4f29-9c16-bdd6dc7c24ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"The answer should come from `/Workspace/ds-academy-embedded-wave-4/ExampleDocs//13pagesPDF.pdf` or `/Workspace/ds-academy-embedded-wave-4/ExampleDocs/22pagesPDF.pdf`\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"the page extracted\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "document_content_description = \"Natural Language Processing\"\n",
    "retriever = SelfQueryRetriever.from_llm(chat, \n",
    "                                        docsearch,\n",
    "                                        document_content_description,\n",
    "                                        metadata_field_info,\n",
    "                                        verbose=True\n",
    "                                        )\n",
    "\n",
    "question = \"What did they say about NLP?\"\n",
    "docs = retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82a739b8-28b1-4229-a338-aadda9fd1bf9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 10 - Q&A Retrieval from Chain  \n",
    "\n",
    "![Methods](https://miro.medium.com/v2/resize:fit:720/format:webp/1*0vTWjqREMHkdman0WoLqzQ.png)\n",
    "\n",
    "It is much more efficient to query the document with the Q&A chain.  \n",
    "Now we create a Retrieval chain using the Vector Database object:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52dac948-a7d2-41c4-9376-fe8ca7032cd6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 10.1 - LangChain has four types of Q&A methods.  \n",
    "It is imperative to understand how these methods work in order to create and implement customized and complex question-answer applications.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*ATLDF3UAPoMy3UOvzS2g5g.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f245edf-79c7-49e8-9127-3bd1c0de5038",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 10.1.1 - Using the Default \"Stuff\" Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c740a091-6ce5-4aa1-9d69-f5d99b3ac52b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=chat,\n",
    "                                       retriever=docsearch.as_retriever(),\n",
    "                                       #retriever=docsearch.as_retriever(search_kwargs={'k': 7}),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type=\"stuff\",\n",
    "                                       )\n",
    "\n",
    "query = \"What is an extreme outlier?\"\n",
    "result = qa_chain(query)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82aff5ec-7c08-4fe3-b458-c276a90df5db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####  10.1.2 - Using the \"Map reduce\" Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af69b812-07b4-4352-bccf-5a799b925a6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=chat,\n",
    "                                       retriever=docsearch.as_retriever(),\n",
    "                                       #retriever=docsearch.as_retriever(search_kwargs={'k': 7}),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type=\"map_reduce\",\n",
    "                                       )\n",
    "\n",
    "query = \"What is an extreme outlier?\"\n",
    "result = qa_chain(query)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e75b453-8d32-4a36-a87d-c8a765569f57",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####  10.1.3 - Using the \"Refine\" Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97731ce8-edac-49d4-9a47-ab6f5650c35e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=chat,\n",
    "                                       retriever=docsearch.as_retriever(),\n",
    "                                       #retriever=docsearch.as_retriever(search_kwargs={'k': 7}),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type=\"refine\",\n",
    "                                       )\n",
    "\n",
    "query = \"What is an extreme outlier?\"\n",
    "result = qa_chain(query)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68c697e0-ce52-4c66-bbac-0b0fd3df3fc6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####  10.1.4 - Using the \"Map rerank\" Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c2d128b-a560-4e2a-9fd6-365aea634359",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=chat,\n",
    "                                       retriever=docsearch.as_retriever(),\n",
    "                                       #retriever=docsearch.as_retriever(search_kwargs={'k': 7}),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type=\"map_rerank\",\n",
    "                                       )\n",
    "\n",
    "query = \"What is an extreme outlier?\"\n",
    "result = qa_chain(query)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e499363-81ce-4333-8d6b-dca3b8292252",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####10.2 - Adding Chat History\n",
    "Now, if we want to take things one step further, we can also make it so that our chatbot will remember any previous questions.\n",
    "\n",
    "Implementation-wise, all that happens is that on each interaction with the chatbot, all of our previous conversation history, including the questions and answers, needs to be passed into the prompt. That is because the LLM does not have a way to store information about our previous requests, so we must pass in all the information on every call to the LLM.\n",
    "\n",
    "Fortunately, LangChain also has a set of classes that let us do this out of the box. This is called the ConversationalRetrievalChain, which allows us to pass in an extra parameter called chat_history , which contains a list of our previous conversations with the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12291ac4-ee85-404a-80da-d9c483f6e080",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qa_chain = ConversationalRetrievalChain.from_llm(llm=chat,\n",
    "                                                 retriever=docsearch.as_retriever(),\n",
    "                                                 return_source_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb4eff5d-2111-4d0d-bbc1-670212d8c126",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The chain run command accepts the chat_history as a parameter. We must manually build up this list based on our conversation with the LLM.  \n",
    "The chain does not do this out of the box, so for each question and answer, we will build up a list called chat_history , which we will pass back into the chain run command each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b786400c-1df3-4496-a930-0961e645c7bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "while True:\n",
    "    # this prints to the terminal, and waits to accept an input from the user\n",
    "    query = input('Prompt: ')\n",
    "    # give us a way to exit the script\n",
    "    if query == \"exit\" or query == \"quit\" or query == \"q\":\n",
    "        print('Exiting')\n",
    "        break\n",
    "    # we pass in the query to the LLM, and print out the response. As well as\n",
    "    # our query, the context of semantically relevant information from our\n",
    "    # vector store will be passed in, as well as list of our chat history\n",
    "    result = qa_chain({'question': query, 'chat_history': chat_history})\n",
    "    print('Answer: ' + result['answer'])\n",
    "    # we build up the chat_history list, based on our question and response\n",
    "    # from the LLM, and the script then returns to the start of the loop\n",
    "    # and is again ready to accept user input.\n",
    "    chat_history.append((query, result['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cef3e6d9-6f73-462b-858c-0f0646f9ed61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f94d84d-e360-4819-a7d7-0516e2e74b11",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 11 - Interacting With Multiple Document types  \n",
    "If you remember, the Documents were only PDF files. To increase our base of documents to interact with, we can just add more Documents types to this list.\n",
    "\n",
    "Now we can simply iterate over all of the files in that folder, and convert the information in them into Documents. From then onwards, the process is the same as before. We just pass our list of documents to the text splitter, which passes the chunked information to the embeddings transformer and vector store.\n",
    "\n",
    "So, in our case, we want to be able to handle pdfs, Microsoft Word documents, and text files. We will iterate over the docs folder, handle files based on their extensions, use the appropriate loaders for them, and add them to the documents' list, which we then pass on to the text splitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad17b38d-3237-471c-9299-0f2f91957405",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### First let's create Langchain Document objects for all different files in our storage folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da01b773-2836-4807-8fc9-6ae8b46e917e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fullpath = \"/Workspace/ds-academy-embedded-wave-4/ExampleDocs/\"\n",
    "documents = []\n",
    "for filename in os.listdir(fullpath):\n",
    "    print(f\"Ingesting document {filename}\")\n",
    "    if filename.endswith('.pdf'):\n",
    "        pdf_path = fullpath + filename\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents.extend(loader.load())\n",
    "    elif filename.endswith('.docx') or filename.endswith('.doc'):\n",
    "        doc_path = fullpath + filename\n",
    "        loader = Docx2txtLoader(doc_path)\n",
    "        documents.extend(loader.load())\n",
    "    elif filename.endswith('.txt'):\n",
    "        text_path = fullpath + filename\n",
    "        loader = TextLoader(text_path)\n",
    "        documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61bf04e6-34f3-4800-a90d-1a2acaf4bbef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 11.1 - Checking How many objects were created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a0fd73c-5ea4-4d1f-8cb7-8a979fb36bf2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(len(documents))\n",
    "for d in documents[0:10]:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a36799e-6cbb-4e89-b809-0988f0fddbc5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####11.2 - Splitting the texts (as done before): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dac3651-aa45-4857-a5a2-6aee9d0616f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"])\n",
    "\n",
    "new_chunked_documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b93610b-8da0-475d-bda6-3867672e4058",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(len(new_chunked_documents))\n",
    "for d in new_chunked_documents[0:5]:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17a0f38a-b96a-4b6e-af68-3a3efa41f976",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### The vector database does not distinguish which documents were indexed before, so we have to take care when ingesting to avoid duplicates  \n",
    "##### 11.2.1 - We could either delete the old VectorDB..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41c1a649-e703-481c-b386-2512f5768d2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#files = glob.glob('/Workspace/ds-academy-embedded-wave-4/VectorDB/*')\n",
    "#for f in files:\n",
    "#    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92da2d92-5016-44bf-a694-d52e736fa600",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 11.2.2 - Or check for duplicates, and only ingest new documents  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3e845d2-4893-4c43-afab-d77ebbb9aabd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(len(chunked_documents))\n",
    "print(len(new_chunked_documents))\n",
    "\n",
    "print(chunked_documents[0].metadata)\n",
    "print(new_chunked_documents[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54586b51-ef0c-4903-a9f4-9a2f8e5ab01c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delta = [d for d in new_chunked_documents if d.metadata[\"source\"] not in [h.metadata[\"source\"] for h in chunked_documents]]\n",
    "print(len(delta))\n",
    "for d in delta[0:5]:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4d36d84-ebe0-4789-b3a3-0145012f4228",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 11.2.3 - Now we are going to add only the new documents to the previously created Vector Database index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5907f7f5-d65e-4107-a026-596700e81413",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"We have {len(vectordb.docstore._dict)} documents in the collection\")\n",
    "vectordb.add_documents(delta,\n",
    "                       embedding=instruct_embeddings,\n",
    "                       )\n",
    "print(f\"We have {len(vectordb.docstore._dict)} documents in the collection\")\n",
    "vectordb.save_local('/Workspace/ds-academy-embedded-wave-4/VectorDB/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24fa4d49-6b8c-4ecd-8662-01b9397a859c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 12 - Chat with our documents from multiple types via LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22e3c811-3bb5-49ec-932b-fb9d1462c778",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf_qa = ConversationalRetrievalChain.from_llm(chat,\n",
    "                                               retriever=vectordb.as_retriever(),\n",
    "                                               return_source_documents=True,\n",
    "                                               verbose=False\n",
    "                                               )\n",
    "\n",
    "chat_history = []\n",
    "print(f\"---------------------------------------------------------------------------------\")\n",
    "print('Welcome to the DocBot. You are now ready to start interacting with your documents')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "while True:\n",
    "    query = input(f\"Prompt: \")\n",
    "    if query == \"exit\" or query == \"quit\" or query == \"q\" or query == \"f\":\n",
    "        print('Exiting')\n",
    "        break\n",
    "    if query == '':\n",
    "        continue\n",
    "    result = pdf_qa({\"question\": query, \"chat_history\": chat_history})\n",
    "    print(f\"Answer: \" + result[\"answer\"])\n",
    "    chat_history.append((query, result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "813d58f7-da24-4912-94a4-20a2f1dd2e23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4d267e7-05b6-444c-8f38-ff70c3b5437b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 13 - (Bonus) Operations among Vector Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df50c732-6cde-4c07-893c-06dac1f5df9b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### You can merge many FAISS vector indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb024e61-2f52-45da-bf71-a26c4a4e2752",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "db1 = FAISS.from_texts([\"Oranges are orange or yellow when ripe\"], embedding=instruct_embeddings,)\n",
    "db2 = FAISS.from_texts([\"Grapes can be red, purple or green\"], embedding=instruct_embeddings,)\n",
    "db3 = FAISS.from_texts([\"Watermelons are green outside, and red inside\"], embedding=instruct_embeddings,)\n",
    "db4 = FAISS.from_texts([\"Lemons are green or yellow\"], embedding=instruct_embeddings,)\n",
    "db5 = FAISS.from_texts([\"Oranges are orange or yellow when ripe\"], embedding=instruct_embeddings,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d9c31cd-66d3-488f-bb0b-987488f108e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(db1.docstore._dict)\n",
    "print(db2.docstore._dict)\n",
    "print(db3.docstore._dict)\n",
    "print(db4.docstore._dict)\n",
    "print(db5.docstore._dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dd52e9e-eae0-4668-b0de-911372a944a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "db1.merge_from(db2)\n",
    "db1.merge_from(db3)\n",
    "db1.merge_from(db4)\n",
    "db1.merge_from(db5)\n",
    "db1.docstore._dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "407ffbc0-2ace-40ee-a449-2498c987c9f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_with_scores = db1.similarity_search_with_score(\"red and green\",)\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"Content: {doc.page_content}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc0f4110-09ff-4eea-9416-d17bea041400",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Another useful thing is to add documents with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eb87d8a-d597-4717-a336-c4a4b4d5e8df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "list_of_documents = [\n",
    "    LangchainDocument(page_content=\"Orange is orange\", metadata=dict(topic=\"Fruit\")),\n",
    "    LangchainDocument(page_content=\"Lemon is green\",  metadata=dict(topic=\"Fruit\")),\n",
    "    LangchainDocument(page_content=\"Watermelon is green\",  metadata=dict(topic=\"Fruit\")),\n",
    "    LangchainDocument(page_content=\"Grapes are red or green\",  metadata=dict(topic=\"Fruit\")),\n",
    "    LangchainDocument(page_content=\"The sun is orange\",  metadata=dict(topic=\"Astronomy\")),\n",
    "    LangchainDocument(page_content=\"Mars is red\",  metadata=dict(topic=\"Astronomy\")),\n",
    "    LangchainDocument(page_content=\"The Earth is blue\",  metadata=dict(topic=\"Astronomy\")),\n",
    "    LangchainDocument(page_content=\"Our planet is Earth\",  metadata=dict(topic=\"Astronomy\")),\n",
    "]\n",
    "db = FAISS.from_documents(list_of_documents, embedding=instruct_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90b823cc-ff94-43ca-b16a-2921527d1705",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "First we make the query without filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0ceadeb-6594-410f-9513-21c8f8c48e16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_with_scores = db.similarity_search_with_score(\"orange\")\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4772c2a1-80b3-428e-8df4-7768d3c124bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we make the same query call but we filter for only topic = \"Fruit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26449cde-0cdd-4bbc-bed3-ebfc1274185c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_with_scores = db.similarity_search_with_score(\"orange\")\n",
    "for doc, score in results_with_scores:\n",
    "    if doc.metadata['topic'] == \"Fruit\":\n",
    "        print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 295794915741311,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "05_Q&A_RAG_ChatGPT",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
