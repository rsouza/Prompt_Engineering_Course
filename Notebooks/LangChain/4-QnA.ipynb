{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de6a690f-4fa9-4835-ad60-fd05644c60d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# LangChain: Q&A over Documents\n",
    "\n",
    "An example might be a tool that would allow you to query a product catalog for items of interest.\n",
    "\n",
    "Sources: [Here](https://learn.deeplearning.ai/langchain/lesson/5/question-and-answer),\n",
    "[here](https://betterprogramming.pub/building-a-multi-document-reader-and-chatbot-with-langchain-and-chatgpt-d1864d47e339) and \n",
    "[here](https://python.langchain.com/docs/integrations/vectorstores/faiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71489e69-5931-4029-a3be-b2244a132d98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!pip install -q docarray\n",
    "#!pip install python-docx\n",
    "!pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d97d829b-1421-49eb-a4c2-447284b87d8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q pydantic==1.10.9  #https://stackoverflow.com/questions/76934579/pydanticusererror-if-you-use-root-validator-with-pre-false-the-default-you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5b6da6c-761b-49f2-a14b-bcea4274dde6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "!pip install -q InstructorEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2378eb14-4a75-40b1-bd4d-cc169e10f9a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "990e3aca-8d8b-4fb0-8e20-f5b3076f8aae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q unstructured[pdf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e455817-0f81-4937-8a81-1129021f0a45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!pip install -qU numpy\n",
    "#!pip install numpy==1.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5bd0ac7-c0ad-4640-97e9-2df5a2ec304c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!pip install -q chromadb\n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fad5fde-e022-40ad-a506-d3c2d87ce44f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20976513-04b6-4f3e-9a34-3e3664e5c28d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import openai\n",
    "#from langchain.llms import OpenAI\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.schema import Document as LangchainDocument\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "#from langchain.vectorstores import DocArrayInMemorySearch\n",
    "#from langchain.vectorstores.base import VectorStore\n",
    "#from langchain.vectorstores import Chroma\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "#from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "#from docx import Document\n",
    "import tiktoken\n",
    "#from funcy import lcat, lmap, linvoke\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43cf7b28-b48a-437f-a562-d0ab4001585a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Loading the ´gpt-35-turbo´ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47931d96-249a-4422-b4ba-a0dc2eedaa82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://rg-rbi-aa-aitest-dsacademy.openai.azure.com/\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "openai_model_name = \"gpt-35-turbo\"\n",
    "openai_deploy_name = \"model-gpt-35-turbo\"\n",
    "openai.api_version = \"2023-07-01-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26138747-b5d4-46bc-9baa-abec84d0714e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(openai_api_base=openai.api_base,\n",
    "                      openai_api_version=openai.api_version,\n",
    "                      deployment_name=openai_deploy_name,\n",
    "                      openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "                      openai_api_type=openai.api_type,\n",
    "                      temperature=0.9,\n",
    "                      #max_tokens=4000,\n",
    "                      )\n",
    "\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e066aa5-be31-4057-b98b-a5ab60469214",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Loading files in the examples folder\n",
    "\n",
    "(In this first part, we are just minding PDF docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fe6ebab-b944-45a4-b74d-898c3c17b6aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fullpath = \"/Workspace/ds-academy-embedded-wave-4/ExampleDocs\"\n",
    "docs = os.listdir(fullpath)\n",
    "docs = [d for d in docs if d.endswith(\".pdf\")]\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d343fd0-4377-4d93-854a-252d04eb80dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we will instantiate the PDF Loader, load one small document and create a list of Langchain documents object\n",
    "\n",
    "Info about the page splitting [here](https://datascience.stackexchange.com/questions/123076/splitting-documents-with-langchain-when-a-sentence-straddles-the-a-page-break)  \n",
    "You can also define your own document splitter using `pdf_loader.load_and_split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaa56086-59ae-4837-aef2-417e71d6e8a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader(fullpath+\"/\"+docs[4])\n",
    "documents = pdf_loader.load()\n",
    "print(f\"We have {len(documents)} pages in the pdf file\")\n",
    "print(type(documents))\n",
    "print(type(documents[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66926874-16b2-43a8-b630-c3e4b8712062",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The simplest Q&A chain implementation we can use is the load_qa_chain.  \n",
    "It loads a chain that allows you to pass in all of the documents you would like to query against using your LLM. \n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*rF3UlC7vWiVFGlXFNZ1XHw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a196032-0871-4d2c-a479-b8bbf075aeec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chain = load_qa_chain(llm=llm, verbose=False)\n",
    "query = 'What is the document about?'\n",
    "response = chain.run(input_documents=documents, question=query)\n",
    "print(response) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25a81887-3c62-4568-a21b-9aac3cc95018",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This method is all good when we only have a short amount of information to send in the [context size of our model](https://platform.openai.com/docs/models/overview).  \n",
    "However, most LLMs will have a limit on the amount of information that can be sent in a single request. So we will not be able to send all the information in our documents within a single request.  \n",
    "To overcome this, we need a smart way to send only the information we think will be relevant to our question/prompt.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e6b2680-3238-4f4c-9bad-d6da2030b8e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Interacting With a Single PDF Using Embeddings\n",
    "\n",
    "We can use embeddings and vector stores to send only relevant information to our prompt.  \n",
    "The steps we will need to follow are:\n",
    "\n",
    "+ Split all the documents into small chunks of text\n",
    "+ Pass each chunk of text into an embedding transformer to turn it into an embedding\n",
    "+ Store the embeddings and related pieces of text in a vector store, instead of a list of Langchain document objects\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:828/format:webp/1*FWwgOvUE660a04zoQplS7A.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bbfa6e7-4831-4641-9835-edf27f0937ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "First testing with a single and bigger PDF file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b03b2327-7807-484f-b61d-de4f6947bb32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader(fullpath+\"/\"+docs[0])\n",
    "documents = pdf_loader.load()\n",
    "print(f\"We have {len(documents)} pages in the pdf file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bbfb662-a3f7-4924-b947-2223641b3cb3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will split the data into chunks of 1,000 characters, with an overlap of 200 characters between the chunks, which helps to give better results and contain the context of the information between chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a19c1662-b7c4-4e1a-9baa-2e2eadd6fe75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0a1df7c-6006-4a3a-af55-27e306c08085",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We could create embeddings with many different transformers. We could have used using **OpenAIEmbeddings**, but then we would have to pay for each token sent to the API. In our case, we will create our vectorDB using **InstructEmbeddings** transformer from **[Hugging Face](https://huggingface.co/hkunlp/instructor-xl)** to provide embeddings from our text chunks.  \n",
    "We set all the db information to be stored inside the `/Workspace/ds-academy-embedded-wave-4/VectorDB`, so it doesn't clutter up our source files.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e030bd0b-96d5-407d-81ed-0dbece05c9c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#openai_embeddings = OpenAIEmbeddings(deployment=\"model-text-embedding-ada-002\", chunk_size = 1)\n",
    "\n",
    "instruct_embeddings = HuggingFaceInstructEmbeddings(query_instruction=\"Represent the query for retrieval: \", \n",
    "                                                    model_name=\"hkunlp/instructor-xl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57aab747-e822-4174-81d5-d7deb974f224",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Setting up a Vector Database\n",
    "\n",
    "![Vector Databases](https://miro.medium.com/v2/resize:fit:828/format:webp/1*vIkxM-u3zrkHMZuIRURc0A.png)\n",
    "\n",
    "There are [many Vector Databases](https://thenewstack.io/top-5-vector-database-solutions-for-your-ai-project/)  products, both paid and open source, that could be used. \n",
    "We have first tried [ChromaDB](https://www.trychroma.com/), but some incompatibilities with the current versions of Python motivated us to try [FAISS](https://faiss.ai/) (from Meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90e96b65-c773-45e8-b330-11dc0cc254d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "First attempt with ChromaDb (commented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c24cf1f-a247-415c-992b-24d35808f833",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#vectordb = Chroma.from_documents(documents,\n",
    "#                                 #embedding=openai_embeddings,\n",
    "#                                 embedding=instruct_embeddings,\n",
    "#                                 persist_directory='/Workspace/ds-academy-embedded-wave-4/VectorDB'\n",
    "#)\n",
    "#vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbdcc978-46d2-4ee8-8ea5-41dc526b1a6d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Deleting previous databases from the folder we have create to store the files (only if creating new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dead1bf-47f2-4aaa-add1-871d965ec131",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "files = glob.glob('/Workspace/ds-academy-embedded-wave-4/VectorDB/*')\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfa51b1a-1edb-43f6-aa5c-7949a3dff8bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Loading all PDF documents into the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa82f488-a364-4d2f-af2b-3f096a716d9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vectordb = FAISS.from_documents(documents, \n",
    "                                embedding=instruct_embeddings,\n",
    "                               )\n",
    "print(f\"There are {vectordb.ntotal} documents in the index\")\n",
    "vectordb.save_local('/Workspace/ds-academy-embedded-wave-4/VectorDB/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bcefcb4-e17c-44b2-9df8-7cc5528c610b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Once we have loaded our content as embeddings into the vector store, we are back to a similar situation as to when we only had one PDF to interact with. As in, we are now ready to pass information into the LLM prompt.  \n",
    "However, instead of passing in all the documents as a source for our context to the chain, as we did initially, we will pass in our vector store as a source/retriever, and the chain will retrieve only the relevant text based on our question and send that information only inside the LLM prompt.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:828/format:webp/1*leoW-Pn0ohWalrUBbzdidA.png)\n",
    "\n",
    "First we will only use the RetrievalQA chain, which will use our vector store as a source for the context information.\n",
    "\n",
    "Again, the chain will wrap our prompt with some text, instructing it to only use the information provided for answering the questions.  \n",
    "So the prompt we end up sending to the LLM something that looks like this:\n",
    "\n",
    "    Use the following pieces of context to answer the question at the end.\n",
    "    If you don't know the answer, just say that you don't know, don't try to\n",
    "    make up an answer.\n",
    "\n",
    "    {context} // i.e the chunks of text retrieved deemed to be most semantically\n",
    "              // relevant to our question\n",
    "\n",
    "    Question: {query} // i.e our actualy query\n",
    "    Helpful Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00654f2b-4aad-4b4f-b067-535d1935c369",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Loading the recently created Vector Database object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5748a33c-a5b3-452f-9604-a123a4b61b6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "docsearch = FAISS.load_local(\"/Workspace/ds-academy-embedded-wave-4/VectorDB/\", instruct_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58e47ab3-9a0d-4148-81fe-0f483a1a5902",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we could query our documents directly using the native similarity search from the vector DB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e170d81-79a9-44fd-ac9e-13b9d7e421a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"What are the documents about?\"\n",
    "result = docsearch.similarity_search(query)\n",
    "print(result[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c0bfc43-39fb-4e62-9ed7-aa90193ea1f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can also search using a score function and a maximum number of documents in return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f605abb0-cfc4-418e-b765-3957d28bc9ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"What are the documents about?\"\n",
    "result = docsearch.similarity_search_with_score(query, k=2)\n",
    "for r in result:\n",
    "    print(r)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d1b9117-d57c-4d68-b0ad-de61ac3e307f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "But it is much better to query the document with the Q&Q chain.  \n",
    "Now we create a Retrieval chain using the Vector Database object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c740a091-6ce5-4aa1-9d69-f5d99b3ac52b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                       retriever=docsearch.as_retriever(),\n",
    "                                       #retriever=docsearch.as_retriever(search_kwargs={'k': 7}),\n",
    "                                       return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeef53d9-67a5-4f25-a641-608ece7175d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"What are the documents about?\"\n",
    "result = qa_chain(query)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e499363-81ce-4333-8d6b-dca3b8292252",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Adding Chat History\n",
    "Now, if we want to take things one step further, we can also make it so that our chatbot will remember any previous questions.\n",
    "\n",
    "Implementation-wise, all that happens is that on each interaction with the chatbot, all of our previous conversation history, including the questions and answers, needs to be passed into the prompt. That is because the LLM does not have a way to store information about our previous requests, so we must pass in all the information on every call to the LLM.\n",
    "\n",
    "Fortunately, LangChain also has a set of classes that let us do this out of the box. This is called the ConversationalRetrievalChain, which allows us to pass in an extra parameter called chat_history , which contains a list of our previous conversations with the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12291ac4-ee85-404a-80da-d9c483f6e080",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qa_chain = ConversationalRetrievalChain.from_llm(llm=llm,\n",
    "                                                 retriever=docsearch.as_retriever(),\n",
    "                                                 return_source_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb4eff5d-2111-4d0d-bbc1-670212d8c126",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The chain run command accepts the chat_history as a parameter. We must manually build up this list based on our conversation with the LLM.  \n",
    "The chain does not do this out of the box, so for each question and answer, we will build up a list called chat_history , which we will pass back into the chain run command each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b786400c-1df3-4496-a930-0961e645c7bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "while True:\n",
    "    # this prints to the terminal, and waits to accept an input from the user\n",
    "    query = input('Prompt: ')\n",
    "    # give us a way to exit the script\n",
    "    if query == \"exit\" or query == \"quit\" or query == \"q\":\n",
    "        print('Exiting')\n",
    "        break\n",
    "    # we pass in the query to the LLM, and print out the response. As well as\n",
    "    # our query, the context of semantically relevant information from our\n",
    "    # vector store will be passed in, as well as list of our chat history\n",
    "    result = qa_chain({'question': query, 'chat_history': chat_history})\n",
    "    print('Answer: ' + result['answer'])\n",
    "    # we build up the chat_history list, based on our question and response\n",
    "    # from the LLM, and the script then returns to the start of the loop\n",
    "    # and is again ready to accept user input.\n",
    "    chat_history.append((query, result['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cef3e6d9-6f73-462b-858c-0f0646f9ed61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f94d84d-e360-4819-a7d7-0516e2e74b11",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Interacting With Multiple Document types  \n",
    "If you remember, the Documents created from our PDF Document Loader is just a list of parts of one Documents. So to increase our base of documents to interact with, we can just add more Documents to this list.\n",
    "\n",
    "Now we can simply iterate over all of the files in that folder, and convert the information in them into Documents. From then onwards, the process is the same as before. We just pass our list of documents to the text splitter, which passes the chunked information to the embeddings transformer and vector store.\n",
    "\n",
    "So, in our case, we want to be able to handle pdfs, Microsoft Word documents, and text files. We will iterate over the docs folder, handle files based on their extensions, use the appropriate loaders for them, and add them to the documentslist, which we then pass on to the text splitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe304660-f646-471a-935b-c9a222a0b427",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "First we are going to delete the old VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41c1a649-e703-481c-b386-2512f5768d2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#files = glob.glob('/Workspace/ds-academy-embedded-wave-4/VectorDB/*')\n",
    "#for f in files:\n",
    "#    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad17b38d-3237-471c-9299-0f2f91957405",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's now create Langchain Document objects for all different files in our storage folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da01b773-2836-4807-8fc9-6ae8b46e917e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fullpath = \"/Workspace/ds-academy-embedded-wave-4/ExampleDocs/\"\n",
    "documents = []\n",
    "for filename in os.listdir(fullpath):\n",
    "    print(f\"Ingesting document {filename}\")\n",
    "    if filename.endswith('.pdf'):\n",
    "        pdf_path = fullpath + filename\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents.extend(loader.load())\n",
    "    elif filename.endswith('.docx') or filename.endswith('.doc'):\n",
    "        doc_path = fullpath + filename\n",
    "        loader = Docx2txtLoader(doc_path)\n",
    "        documents.extend(loader.load())\n",
    "    elif filename.endswith('.txt'):\n",
    "        text_path = fullpath + filename\n",
    "        loader = TextLoader(text_path)\n",
    "        documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61bf04e6-34f3-4800-a90d-1a2acaf4bbef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Checking How many objects were created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a0fd73c-5ea4-4d1f-8cb7-8a979fb36bf2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(len(documents))\n",
    "for d in documents[0:5]:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a36799e-6cbb-4e89-b809-0988f0fddbc5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we are going to split the texts as we have done before: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dac3651-aa45-4857-a5a2-6aee9d0616f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"])\n",
    "\n",
    "chunked_documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b93610b-8da0-475d-bda6-3867672e4058",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(len(chunked_documents))\n",
    "for d in chunked_documents[0:5]:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4d36d84-ebe0-4789-b3a3-0145012f4228",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we are going to add documents to the previously created Vector Database index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5907f7f5-d65e-4107-a026-596700e81413",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"We have {len(vectordb.docstore._dict)} documents in the collection\")\n",
    "vectordb.add_documents(chunked_documents,\n",
    "                       embedding=instruct_embeddings,\n",
    "                       )\n",
    "print(f\"We have {len(vectordb.docstore._dict)} documents in the collection\")\n",
    "vectordb.save_local('/Workspace/ds-academy-embedded-wave-4/VectorDB/faiss_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17a0f38a-b96a-4b6e-af68-3a3efa41f976",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The vector database does not distinguish which documents were indexed before, so we have to take care when ingesting to avoid duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24fa4d49-6b8c-4ecd-8662-01b9397a859c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we can chat with our documents from multiple types via LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22e3c811-3bb5-49ec-932b-fb9d1462c778",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf_qa = ConversationalRetrievalChain.from_llm(llm,\n",
    "                                               retriever=vectordb.as_retriever(),\n",
    "                                               return_source_documents=True,\n",
    "                                               verbose=False\n",
    "                                               )\n",
    "\n",
    "chat_history = []\n",
    "print(f\"---------------------------------------------------------------------------------\")\n",
    "print('Welcome to the DocBot. You are now ready to start interacting with your documents')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "while True:\n",
    "    query = input(f\"Prompt: \")\n",
    "    if query == \"exit\" or query == \"quit\" or query == \"q\" or query == \"f\":\n",
    "        print('Exiting')\n",
    "        break\n",
    "    if query == '':\n",
    "        continue\n",
    "    result = pdf_qa({\"question\": query, \"chat_history\": chat_history})\n",
    "    print(f\"Answer: \" + result[\"answer\"])\n",
    "    chat_history.append((query, result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "813d58f7-da24-4912-94a4-20a2f1dd2e23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4d267e7-05b6-444c-8f38-ff70c3b5437b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Bonus: operations among Vector Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df50c732-6cde-4c07-893c-06dac1f5df9b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### You can merge many FAISS vector indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb024e61-2f52-45da-bf71-a26c4a4e2752",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "db1 = FAISS.from_texts([\"Oranges are orange or yellow when ripe\"], embedding=instruct_embeddings,)\n",
    "db2 = FAISS.from_texts([\"Grapes can be red, purple or green\"], embedding=instruct_embeddings,)\n",
    "db3 = FAISS.from_texts([\"Watermelons are green outside, and red inside\"], embedding=instruct_embeddings,)\n",
    "db4 = FAISS.from_texts([\"Lemons are green or yellow\"], embedding=instruct_embeddings,)\n",
    "db5 = FAISS.from_texts([\"Oranges are orange or yellow when ripe\"], embedding=instruct_embeddings,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d9c31cd-66d3-488f-bb0b-987488f108e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(db1.docstore._dict)\n",
    "print(db2.docstore._dict)\n",
    "print(db3.docstore._dict)\n",
    "print(db4.docstore._dict)\n",
    "print(db5.docstore._dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dd52e9e-eae0-4668-b0de-911372a944a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "db1.merge_from(db2)\n",
    "db1.merge_from(db3)\n",
    "db1.merge_from(db4)\n",
    "db1.merge_from(db5)\n",
    "db1.docstore._dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "407ffbc0-2ace-40ee-a449-2498c987c9f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_with_scores = db1.similarity_search_with_score(\"red and green\",)\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"Content: {doc.page_content}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc0f4110-09ff-4eea-9416-d17bea041400",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Another useful thing is to add documentd with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eb87d8a-d597-4717-a336-c4a4b4d5e8df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "list_of_documents = [\n",
    "    LangchainDocument(page_content=\"Orange is orange\", metadata=dict(topic=\"Fruit\")),\n",
    "    LangchainDocument(page_content=\"Lemon is green\",  metadata=dict(topic=\"Fruit\")),\n",
    "    LangchainDocument(page_content=\"Watermelon is green\",  metadata=dict(topic=\"Fruit\")),\n",
    "    LangchainDocument(page_content=\"Grapes are red or green\",  metadata=dict(topic=\"Fruit\")),\n",
    "    LangchainDocument(page_content=\"The sun is orange\",  metadata=dict(topic=\"Astronomy\")),\n",
    "    LangchainDocument(page_content=\"Mars is red\",  metadata=dict(topic=\"Astronomy\")),\n",
    "    LangchainDocument(page_content=\"The Earth is blue\",  metadata=dict(topic=\"Astronomy\")),\n",
    "    LangchainDocument(page_content=\"Our planet is Earth\",  metadata=dict(topic=\"Astronomy\")),\n",
    "]\n",
    "db = FAISS.from_documents(list_of_documents, embedding=instruct_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90b823cc-ff94-43ca-b16a-2921527d1705",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "First we make the query without filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0ceadeb-6594-410f-9513-21c8f8c48e16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_with_scores = db.similarity_search_with_score(\"orange\")\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4772c2a1-80b3-428e-8df4-7768d3c124bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we make the same query call but we filter for only topic = \"Fruit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26449cde-0cdd-4bbc-bed3-ebfc1274185c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_with_scores = db.similarity_search_with_score(\"orange\")\n",
    "for doc, score in results_with_scores:\n",
    "    if doc.metadata['topic'] == \"Fruit\":\n",
    "        print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 295794915741311,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "4-QnA",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
