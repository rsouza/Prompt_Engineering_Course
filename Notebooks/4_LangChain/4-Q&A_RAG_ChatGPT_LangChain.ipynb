{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de6a690f-4fa9-4835-ad60-fd05644c60d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## LangChain: Q&A over Documents\n",
    "\n",
    "Sources: [Here](https://learn.deeplearning.ai/langchain/lesson/5/question-and-answer),\n",
    "[here](https://betterprogramming.pub/building-a-multi-document-reader-and-chatbot-with-langchain-and-chatgpt-d1864d47e339) and \n",
    "[here](https://python.langchain.com/docs/integrations/vectorstores/faiss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a292d0e8-9bed-4f8d-9af0-edac213f873f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71489e69-5931-4029-a3be-b2244a132d98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -qU openai\n",
    "!pip install -qU langchain \n",
    "!pip install -qU langchain-openai\n",
    "!pip install -qU InstructorEmbedding\n",
    "!pip install -qU transformers\n",
    "!pip install -qU chromadb\n",
    "!pip install -qU faiss-cpu\n",
    "!pip install -qU tiktoken\n",
    "!pip install -q pydantic==1.10.9  #https://stackoverflow.com/questions/76934579/pydanticusererror-if-you-use-root-validator-with-pre-false-the-default-you\n",
    "!pip install -q urllib3==1.26.18\n",
    "!pip install -q requests==2.28.1\n",
    "!pip install -qU SQLAlchemy\n",
    "#!pip install -qU docarray\n",
    "#!pip install -qU python-docx\n",
    "!pip install -qU pypdf\n",
    "!pip install -qU docx2txt\n",
    "#!pip install -qU unstructured[pdf]\n",
    "!pip install -qU lark\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20976513-04b6-4f3e-9a34-3e3664e5c28d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "import glob\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "#from funcy import lcat, lmap, linvoke\n",
    "\n",
    "## Langchain LLM Objects\n",
    "import openai\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_openai import AzureOpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "## Langchain Prompt Templates\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "## Langchain Chains \n",
    "#from langchain.chains import ConversationChain\n",
    "#from langchain.chains import LLMChain\n",
    "#from langchain.chains import ConversationChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "#from langchain.chains.mapreduce import MapReduceChain\n",
    "#from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "#from langchain.chains import SimpleSequentialChain\n",
    "#from langchain.chains import SequentialChain\n",
    "#from langchain.chains.router import MultiPromptChain\n",
    "#from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "\n",
    "## Langchain Output Parsers \n",
    "#from langchain.output_parsers import ResponseSchema\n",
    "#from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "## Langchain Memory \n",
    "#from langchain.memory import ConversationBufferMemory\n",
    "#from langchain.memory import ConversationBufferWindowMemory\n",
    "#from langchain.memory import ConversationTokenBufferMemory\n",
    "#from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "## Langchain Text Splitters\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#from langchain.text_splitter import TokenTextSplitter\n",
    "#from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "## Langchain Document Object and Loaders\n",
    "#from docx import Document\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.schema import Document as LangchainDocument\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "## Langchain Vector Databases\n",
    "#from langchain.vectorstores import DocArrayInMemorySearch\n",
    "#from langchain.vectorstores.base import VectorStore\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "## Langchain  Embedding Models\n",
    "#from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "\n",
    "## Langchain retrievers\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever ## Error\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers import SVMRetriever\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = \"/Workspace/ds-academy-research/Models/\"\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "azure_endpoint = \"https://rg-rbi-aa-aitest-dsacademy.openai.azure.com/\"\n",
    "#azure_endpoint = \"https://chatgpt-summarization.openai.azure.com/\"\n",
    "\n",
    "openai.api_version = \"2023-07-01-preview\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "deployment_name = \"model-gpt-35-turbo\"\n",
    "openai_model_name = \"gpt-35-turbo\"\n",
    "\n",
    "client = AzureOpenAI(api_key=openai.api_key,\n",
    "                     api_version=openai.api_version,\n",
    "                     azure_endpoint=azure_endpoint,\n",
    "                     )\n",
    "\n",
    "chat = AzureChatOpenAI(azure_endpoint=azure_endpoint,\n",
    "                       openai_api_version=openai.api_version,\n",
    "                       deployment_name=deployment_name,\n",
    "                       openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "                       openai_api_type=openai.api_type,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e066aa5-be31-4057-b98b-a5ab60469214",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Examining files in the examples folder  \n",
    "It may be necessary to change the default folder for your documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fe6ebab-b944-45a4-b74d-898c3c17b6aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fullpath = \"/Workspace/ds-academy-research/Docs/Generic/\"\n",
    "docs = os.listdir(fullpath)\n",
    "docs = [d for d in docs if d.endswith(\".pdf\")]\n",
    "docs.sort()\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fe28b21-fd6e-4c89-ba9e-6496daaeb04f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### LangChain: Creating a Document Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66926874-16b2-43a8-b630-c3e4b8712062",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The simplest Q&A chain implementation we can use is the load_qa_chain.  \n",
    "It loads a chain that allows you to pass in all of the documents you would like to query against using your LLM. \n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*rF3UlC7vWiVFGlXFNZ1XHw.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d343fd0-4377-4d93-854a-252d04eb80dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Querying a single PDF document\n",
    "\n",
    "Now we will instantiate the PDF Loader, load one small document and create a list of Langchain documents object\n",
    "\n",
    "Info about the page splitting [here](https://datascience.stackexchange.com/questions/123076/splitting-documents-with-langchain-when-a-sentence-straddles-the-a-page-break)  \n",
    "You can also define your own document splitter using `pdf_loader.load_and_split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaa56086-59ae-4837-aef2-417e71d6e8a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "docid = 0\n",
    "print(f'Loading Document: {fullpath+docs[docid]}')\n",
    "pdf_loader = PyPDFLoader(fullpath+docs[docid])\n",
    "documents = pdf_loader.load()\n",
    "print(f\"We have {len(documents)} pages in the pdf file\")\n",
    "\n",
    "print(type(documents))\n",
    "print(type(documents[docid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a196032-0871-4d2c-a479-b8bbf075aeec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chain = load_qa_chain(llm=chat, verbose=False)\n",
    "query = 'What is the document about?'\n",
    "response = chain.run(input_documents=documents, question=query)\n",
    "print(response) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25a81887-3c62-4568-a21b-9aac3cc95018",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### This method is all good when we only have a short amount of information to send in the [context size of our model](https://platform.openai.com/docs/models/overview).  \n",
    "However, most LLMs will have a limit on the amount of information that can be sent in a single request. So we will not be able to send all the information in our documents within a single request.  \n",
    "To overcome this, we need a smart way to send only the information we think will be relevant to our question/prompt.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e6b2680-3238-4f4c-9bad-d6da2030b8e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Interacting with documents using Embeddings\n",
    "\n",
    "We can use embeddings and vector stores to send only relevant information to our prompt.  \n",
    "The steps we will need to follow are:\n",
    "\n",
    "+ Split all the documents into small chunks of text\n",
    "+ Pass each chunk of text into an embedding transformer to turn it into an embedding\n",
    "+ Store the embeddings and related pieces of text in a vector store, instead of a list of Langchain document objects\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:828/format:webp/1*FWwgOvUE660a04zoQplS7A.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d120d082-42ab-432c-ab0c-a3cdcacc0b3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Creating a list of Document Objects for all PDF files in the examples folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bb62b22-ab36-4d18-b20f-fdb3696f7c62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "for filename in os.listdir(fullpath):\n",
    "    if filename.endswith('.pdf'):\n",
    "        print(f\"Ingesting document {filename}\")\n",
    "        pdf_path = fullpath + filename\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents.extend(loader.load())\n",
    "print(f\"We have {len(documents)} pages from all the pdf files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58eedd71-860e-4e68-b0d0-20831da09f3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Splitting each document into small chunks\n",
    "\n",
    "When we load documents, the splitting is done by pages. We can change that using Splitters, to avoid the limitations on the LLM contexts  \n",
    "\n",
    "Langchain offer different Text Splitters\n",
    "+ RecursiveCharacterTextSplitter: Divides the text into fragments based on characters, starting with the first character. If the fragments turn out to be too large, it moves on to the next character. It offers flexibility by allowing you to define the division characters and fragment size.\n",
    "+ CharacterTextSplitter: Similar to the RecursiveCharacterTextSplitter, but with the ability to define a custom separator for more specific division. By default, it tries to split on characters like “\\n\\n”, “\\n”, “ “, and “”.\n",
    "+ RecursiveTextSplitter: Unlike the previous ones, the RecursiveTextSplitter divides text into fragments based on words or tokens instead of characters. This provides a more semantic view and is ideal for content analysis rather than structure.\n",
    "+ TokenTextSplitter: Uses the OpenAI language model to split text into fragments based on tokens, allowing for precise and contextualized segmentation, ideal for advanced natural language processing applications.\n",
    "+ And some more specific ones\n",
    "\n",
    "\n",
    "We will split the data into chunks of 1,000 characters, with an overlap of 200 characters between the chunks, which helps to give better results and contain the context of the information between chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a19c1662-b7c4-4e1a-9baa-2e2eadd6fe75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, \n",
    "                                               chunk_overlap=200,\n",
    "                                               separators=[\"\\n\\n\", \"\\n\", \"\\. \", \" \", \"\"],\n",
    "                                               length_function=len\n",
    "                                               )\n",
    "chunked_documents = text_splitter.split_documents(documents)\n",
    "print(len(chunked_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0a1df7c-6006-4a3a-af55-27e306c08085",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Choosing a Model to create Embeddings of each chunk:  \n",
    "\n",
    "We could create embeddings with many different transformers. \n",
    "We could have used using **OpenAIEmbeddings**, but then we would have to pay for each token sent to the API.  \n",
    "In our case, we will create our vectorDB using **InstructEmbeddings** transformer from **[Hugging Face](https://huggingface.co/hkunlp/instructor-xl)** to provide embeddings from our text chunks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b7d52b6-27d4-49ab-a23a-e4698bccaae3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#openai_embeddings = OpenAIEmbeddings(deployment=\"model-text-embedding-ada-002\", chunk_size = 1)\n",
    "\n",
    "instruct_embeddings = HuggingFaceInstructEmbeddings(query_instruction=\"Represent the query for retrieval: \", model_name=\"hkunlp/instructor-xl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57aab747-e822-4174-81d5-d7deb974f224",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Creating a Vector Database\n",
    "\n",
    "![Vector Databases](https://miro.medium.com/v2/resize:fit:828/format:webp/1*vIkxM-u3zrkHMZuIRURc0A.png)\n",
    "\n",
    "There are [many Vector Databases](https://thenewstack.io/top-5-vector-database-solutions-for-your-ai-project/)  products, both paid and open source, that could be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90e96b65-c773-45e8-b330-11dc0cc254d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Setting Chroma Vector DB  \n",
    "\n",
    "Deleting previous databases from the folder we have create to store the files and loading all PDF documents into the Vector Database   \n",
    "##### We have first tried [ChromaDB](https://www.trychroma.com/), but some incompatibilities with the current versions of Python motivated us to try [FAISS](https://faiss.ai/) (from Meta)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c24cf1f-a247-415c-992b-24d35808f833",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Commented Code - ChromaDB\n",
    "```\n",
    "files = glob.glob('/Workspace/ds-academy-research/VectorDB_Chroma/*')\n",
    "\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "chromadb = Chroma.from_documents(chunked_documents,\n",
    "                                 #embedding=openai_embeddings,\n",
    "                                 embedding_function=instruct_embeddings,\n",
    "                                 persist_directory='/Workspace/ds-academy-research/VectorDB_Chroma/'\n",
    ")\n",
    "chromadb.persist()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbdcc978-46d2-4ee8-8ea5-41dc526b1a6d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Setting FAISS Vector DB\n",
    "Deleting previous databases from the folder we have create to store the files and loading all PDF documents into the Vector Database  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa82f488-a364-4d2f-af2b-3f096a716d9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "files = glob.glob('/Workspace/ds-academy-research/VectorDB_FAISS/*')\n",
    "\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "faissdb = FAISS.from_documents(chunked_documents, \n",
    "                                embedding=instruct_embeddings,\n",
    "                               )\n",
    "#print(f\"There are {vectordb.ntotal} documents in the index\")\n",
    "faissdb.save_local('/Workspace/ds-academy-research/VectorDB_FAISS/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bcefcb4-e17c-44b2-9df8-7cc5528c610b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Retrieval Tasks  \n",
    "\n",
    "Once we have loaded our content as embeddings into the vector store, we are back to a similar situation as to when we only had one PDF to interact with. As in, we are now ready to pass information into the LLM prompt.  \n",
    "However, instead of passing in all the documents as a source for our context to the chain, as we did initially, we will pass in our vector store as a source/retriever, and the chain will retrieve only the relevant text based on our question and send that information only inside the LLM prompt.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:828/format:webp/1*leoW-Pn0ohWalrUBbzdidA.png)\n",
    "\n",
    "First we will only use the RetrievalQA chain, which will use our vector store as a source for the context information.\n",
    "\n",
    "Again, the chain will wrap our prompt with some text, instructing it to only use the information provided for answering the questions.  \n",
    "So the prompt we end up sending to the LLM something that looks like this:\n",
    "\n",
    "    Use the following pieces of context to answer the question at the end.\n",
    "    If you don't know the answer, just say that you don't know, don't try to\n",
    "    make up an answer.\n",
    "\n",
    "    {context} // i.e the chunks of text retrieved deemed to be most semantically\n",
    "              // relevant to our question\n",
    "\n",
    "    Question: {query} // i.e our actualy query\n",
    "    Helpful Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00654f2b-4aad-4b4f-b067-535d1935c369",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Loading the recently created Vector Database object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5748a33c-a5b3-452f-9604-a123a4b61b6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "docsearch = FAISS.load_local(\"/Workspace/ds-academy-research/VectorDB_FAISS/\", instruct_embeddings)\n",
    "print(len(docsearch.index_to_docstore_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58e47ab3-9a0d-4148-81fe-0f483a1a5902",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Retrieving chunks of Documents using Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e170d81-79a9-44fd-ac9e-13b9d7e421a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"Natural Language Processing\"\n",
    "result = docsearch.similarity_search(query)\n",
    "#print(result[0].page_content)\n",
    "for r in result:\n",
    "    print(r)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c0bfc43-39fb-4e62-9ed7-aa90193ea1f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Search using a score function and a maximum number of documents in return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f605abb0-cfc4-418e-b765-3957d28bc9ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"Natural Language Processing\"\n",
    "result = docsearch.similarity_search_with_score(query, k=2)\n",
    "for r in result:\n",
    "    print(r)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55a58498-6930-45d5-8572-ce2501ed5a5a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Addressing Diversity: Maximum marginal relevance\n",
    "\n",
    "`Maximum marginal relevance` strives to achieve both relevance to the query *and diversity* among the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eed6987d-b91f-4dc0-bae9-8058ba9e417f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"Natural Language Processing\"\n",
    "result = docsearch.max_marginal_relevance_search(query, k=2)\n",
    "for r in result:\n",
    "    print(r)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "153ab76a-c3a7-4d1e-8e0f-8c36e3f0af89",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Addressing Specificity: working with metadata\n",
    "\n",
    "To address this, many vectorstores support operations on `metadata`.\n",
    "\n",
    "`metadata` provides context for each embedded chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17f60291-c424-48b7-a891-1b657be5d752",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"Natural Language Processing\"\n",
    "result = docsearch.similarity_search(query,\n",
    "                                     k=3,\n",
    "                                     filter={\"source\":\"/Workspace/ds-academy-research/Docs/37pagesPDF.pdf\"}\n",
    "                                     )\n",
    "for r in result:\n",
    "    print(r)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6af0df61-a9fa-4d51-81c3-660499e2e23f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Addressing Specificity: working with metadata using self-query retriever\n",
    "\n",
    "But we have an interesting challenge: we often want to infer the metadata from the query itself.\n",
    "\n",
    "To address this, we can use `SelfQueryRetriever`, which uses an LLM to extract:\n",
    " \n",
    "1. The `query` string to use for vector search\n",
    "2. A metadata filter to pass in as well\n",
    "\n",
    "Most vector databases support metadata filters, so this doesn't require any new databases or indexes.\n",
    "\n",
    "##### Disclaimer: Not implemented for FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fffb8c2-753e-4f29-9c16-bdd6dc7c24ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"The answer should come from `/Workspace/ds-academy-research/Docs/13pagesPDF.pdf` or `/Workspace/ds-academy-research/Docs/22pagesPDF.pdf`\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"the page extracted\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "document_content_description = \"Natural Language Processing\"\n",
    "retriever = SelfQueryRetriever.from_llm(chat, \n",
    "                                        docsearch,\n",
    "                                        document_content_description,\n",
    "                                        metadata_field_info,\n",
    "                                        verbose=True\n",
    "                                        )\n",
    "\n",
    "question = \"What did they say about NLP?\"\n",
    "docs = retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82a739b8-28b1-4229-a338-aadda9fd1bf9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Q&A Retrieval from Chain  \n",
    "\n",
    "![Methods](https://miro.medium.com/v2/resize:fit:720/format:webp/1*0vTWjqREMHkdman0WoLqzQ.png)\n",
    "\n",
    "It is much more efficient to query the document with the Q&A chain.  \n",
    "Now we create a Retrieval chain using the Vector Database object:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52dac948-a7d2-41c4-9376-fe8ca7032cd6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### LangChain has four types of Q&A methods.  \n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*ATLDF3UAPoMy3UOvzS2g5g.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f245edf-79c7-49e8-9127-3bd1c0de5038",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### [Stuffing Chain](https://medium.com/@minh.hoque/what-are-llm-chains-671b84103ba9)\n",
    "The Stuffing chain serves as a solution for scenarios where the context length of the LLM is inadequate to handle extensive documents or a substantial amount of information. In such cases, a large document can be divided into smaller segments, and semantic search techniques can be employed to retrieve relevant documents based on the query. These retrieved documents are then “stuffed” into the LLM context, allowing for the generation of a response.\n",
    "\n",
    "Benefits:\n",
    "\n",
    "+ Consolidation of multiple documents: The Stuffing chain allows the aggregation of several relevant documents, overcoming the context length limitation of LLMs for large documents.\n",
    "Comprehensive information processing: By leveraging multiple documents, the chain can generate more comprehensive and relevant answers.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "+ Increased complexity: Retrieving relevant document requires good semantic search and vector database.\n",
    "+ Potential loss of contextual coherency: Since we are retrieving N documents, the LLM might not have all relevant context to generate a cohesive answer.\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "+ Document Retrieval Question Answering: Utilizing the Stuffing chain, document chunks retrieved of the larger document can be effectively leveraged to provide accurate answers to your questions. For example, suppose you have a lengthy legal document and need to find specific answers to legal questions. By using the Stuffing chain, you can break down the document into smaller chunks, retrieve relevant chunks based on the question, and utilize the information within those chunks to generate accurate answers.\n",
    "+ Complex Question Answering: When answering complex questions that require information from diverse sources, the Stuffing chain can provide more comprehensive and accurate responses. For instance, imagine you have a research project that requires answering complex scientific queries. The Stuffing chain allows you to divide relevant scientific papers into smaller chunks, retrieve the necessary information from these chunks, and synthesize it to provide a thorough and precise response to the complex question at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c740a091-6ce5-4aa1-9d69-f5d99b3ac52b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=chat,\n",
    "                                       retriever=docsearch.as_retriever(),\n",
    "                                       #retriever=docsearch.as_retriever(search_kwargs={'k': 7}),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type=\"stuff\",\n",
    "                                       )\n",
    "\n",
    "query = \"What is an extreme outlier?\"\n",
    "result = qa_chain(query)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82aff5ec-7c08-4fe3-b458-c276a90df5db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### [Map-Reduce Chain](https://medium.com/@minh.hoque/what-are-llm-chains-671b84103ba9)\n",
    "The Map-Reduce chain enables the iteration over a list of documents, generating individual outputs for each document, which can later be combined to produce a final result. This chain is useful for tasks that involve processing documents in parallel and then aggregating the outputs. \n",
    "\n",
    "Benefits:\n",
    "\n",
    "+ Parallel processing: The Map-Reduce chain allows for parallel execution of the language model on individual documents, improving efficiency and reducing processing time.\n",
    "+ Scalability: The chain can handle large collections of documents by distributing the processing load across multiple iterations.\n",
    "+ Enhanced information extraction: By generating individual outputs for each document, the chain can extract specific information that contributes to a more comprehensive final result.  \n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "+ Complexity in output aggregation: Combining the outputs of multiple iterations requires careful handling to ensure coherency and meaningful synthesis.\n",
    "+ Potential redundancy: In some cases, the individual outputs of the Map-Reduce chain may contain redundant information, necessitating further post-processing steps.\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "+ Multiple document summarization: The Map-Reduce chain can be used to generate summaries for many documents and then to combine the singular summaries to create a final comprehensive summary for the whole group of documents. For example, imagine you have a collection of research papers on a particular topic. By employing the Map-Reduce chain, you can generate summaries for each research paper, and finally merge the individual summaries to produce a comprehensive summary that captures the key information from the entire collection of papers. This approach enables efficient and accurate summarization of large volumes of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af69b812-07b4-4352-bccf-5a799b925a6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=chat,\n",
    "                                       retriever=docsearch.as_retriever(),\n",
    "                                       #retriever=docsearch.as_retriever(search_kwargs={'k': 7}),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type=\"map_reduce\",\n",
    "                                       )\n",
    "\n",
    "query = \"What is an extreme outlier?\"\n",
    "result = qa_chain(query)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e75b453-8d32-4a36-a87d-c8a765569f57",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### [Refine Chain](https://medium.com/@minh.hoque/what-are-llm-chains-671b84103ba9)  \n",
    "The Refine chain focuses on iterative refinement of the output by feeding the output of one iteration into the next, aiming to enhance the accuracy and quality of the final result.\n",
    "\n",
    "Benefits:\n",
    "\n",
    "+ Continuous improvement: The Refine chain allows for progressive refinement of the output by iteratively updating and enhancing the information.\n",
    "Enhanced accuracy: By refining the output in each iteration, the chain can improve the accuracy and relevance of the final result.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "+ Increased computational resources: The iterative nature of the Refine chain may require additional computational resources compared to non-iterative approaches.\n",
    "+ Longer processing time: Each iteration adds to the overall processing time, which may be a consideration when real-time or near-real-time responses are required.\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "+ Long-form text generation: The Refine chain proves exceptionally valuable in the creation of extensive text compositions, such as essays, articles, or stories, where the iterative refinement process greatly enhances coherence and readability. For instance, envision interacting with a substantial research paper and progressively employing the LLM to craft an abstract, refining it with each iteration to achieve an optimal outcome.\n",
    "+ Answer synthesis: The Refine chain demonstrates its prowess in synthesizing answers derived from multiple sources or generating comprehensive responses. Through iterative refinement, the chain progressively improves the accuracy and comprehensiveness of the final answer. This capability is especially advantageous when each retrieved document contributes crucial context to the answer generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97731ce8-edac-49d4-9a47-ab6f5650c35e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=chat,\n",
    "                                       retriever=docsearch.as_retriever(),\n",
    "                                       #retriever=docsearch.as_retriever(search_kwargs={'k': 7}),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type=\"refine\",\n",
    "                                       )\n",
    "\n",
    "query = \"What is an extreme outlier?\"\n",
    "result = qa_chain(query)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68c697e0-ce52-4c66-bbac-0b0fd3df3fc6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### [Map Rerank](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.map_rerank.MapRerankDocumentsChain.html#)  \n",
    "\n",
    "Combining documents by mapping a chain over them, then reranking results.\n",
    "This algorithm calls an LLMChain on each input document. The LLMChain is expected to have an OutputParser that parses the result into both an answer (answer_key) and a score (rank_key). The answer with the highest score is then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c2d128b-a560-4e2a-9fd6-365aea634359",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=chat,\n",
    "                                       retriever=docsearch.as_retriever(),\n",
    "                                       #retriever=docsearch.as_retriever(search_kwargs={'k': 7}),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type=\"map_rerank\",\n",
    "                                       )\n",
    "\n",
    "query = \"What is an extreme outlier?\"\n",
    "result = qa_chain(query)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e499363-81ce-4333-8d6b-dca3b8292252",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Adding Chat History\n",
    "Now, if we want to take things one step further, we can also make it so that our chatbot will remember any previous questions.\n",
    "\n",
    "Implementation-wise, all that happens is that on each interaction with the chatbot, all of our previous conversation history, including the questions and answers, needs to be passed into the prompt. That is because the LLM does not have a way to store information about our previous requests, so we must pass in all the information on every call to the LLM.\n",
    "\n",
    "Fortunately, LangChain also has a set of classes that let us do this out of the box. This is called the ConversationalRetrievalChain, which allows us to pass in an extra parameter called chat_history , which contains a list of our previous conversations with the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12291ac4-ee85-404a-80da-d9c483f6e080",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qa_chain = ConversationalRetrievalChain.from_llm(llm=chat,\n",
    "                                                 retriever=docsearch.as_retriever(),\n",
    "                                                 return_source_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb4eff5d-2111-4d0d-bbc1-670212d8c126",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The chain run command accepts the chat_history as a parameter. We must manually build up this list based on our conversation with the LLM.  \n",
    "The chain does not do this out of the box, so for each question and answer, we will build up a list called chat_history , which we will pass back into the chain run command each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b786400c-1df3-4496-a930-0961e645c7bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "while True:\n",
    "    # this prints to the terminal, and waits to accept an input from the user\n",
    "    query = input('Prompt: ')\n",
    "    # give us a way to exit the script\n",
    "    if query == \"exit\" or query == \"quit\" or query == \"q\":\n",
    "        print('Exiting')\n",
    "        break\n",
    "    # we pass in the query to the LLM, and print out the response. As well as\n",
    "    # our query, the context of semantically relevant information from our\n",
    "    # vector store will be passed in, as well as list of our chat history\n",
    "    result = qa_chain({'question': query, 'chat_history': chat_history})\n",
    "    print('Answer: ' + result['answer'])\n",
    "    # we build up the chat_history list, based on our question and response\n",
    "    # from the LLM, and the script then returns to the start of the loop\n",
    "    # and is again ready to accept user input.\n",
    "    chat_history.append((query, result['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cef3e6d9-6f73-462b-858c-0f0646f9ed61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f94d84d-e360-4819-a7d7-0516e2e74b11",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Interacting With Multiple Document types  \n",
    "If you remember, the Documents were only PDF files. To increase our base of documents to interact with, we can just add more Documents types to this list.\n",
    "\n",
    "Now we can simply iterate over all of the files in that folder, and convert the information in them into Documents. From then onwards, the process is the same as before. We just pass our list of documents to the text splitter, which passes the chunked information to the embeddings transformer and vector store.\n",
    "\n",
    "So, in our case, we want to be able to handle pdfs, Microsoft Word documents, and text files. We will iterate over the docs folder, handle files based on their extensions, use the appropriate loaders for them, and add them to the documents' list, which we then pass on to the text splitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad17b38d-3237-471c-9299-0f2f91957405",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### First let's create Langchain Document objects for all different files in our storage folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da01b773-2836-4807-8fc9-6ae8b46e917e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fullpath = \"/Workspace/ds-academy-research/Docs/Generic/\"\n",
    "documents = []\n",
    "for filename in os.listdir(fullpath):\n",
    "    print(f\"Ingesting document {filename}\")\n",
    "    if filename.endswith('.pdf'):\n",
    "        pdf_path = fullpath + filename\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents.extend(loader.load())\n",
    "    elif filename.endswith('.docx') or filename.endswith('.doc'):\n",
    "        doc_path = fullpath + filename\n",
    "        loader = Docx2txtLoader(doc_path)\n",
    "        documents.extend(loader.load())\n",
    "    elif filename.endswith('.txt'):\n",
    "        text_path = fullpath + filename\n",
    "        loader = TextLoader(text_path)\n",
    "        documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61bf04e6-34f3-4800-a90d-1a2acaf4bbef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Checking How many objects were created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a0fd73c-5ea4-4d1f-8cb7-8a979fb36bf2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(len(documents))\n",
    "for d in documents[0:10]:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a36799e-6cbb-4e89-b809-0988f0fddbc5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Splitting the texts (as done before): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dac3651-aa45-4857-a5a2-6aee9d0616f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"])\n",
    "\n",
    "new_chunked_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "print(len(new_chunked_documents))\n",
    "for d in new_chunked_documents[0:5]:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17a0f38a-b96a-4b6e-af68-3a3efa41f976",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### The vector database does not distinguish which documents were indexed before, so we have to take care when ingesting to avoid duplicates  \n",
    "##### We could either delete the old VectorDB..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41c1a649-e703-481c-b386-2512f5768d2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#files = glob.glob('/Workspace/ds-academy-research/VectorDB/FAISS/*')\n",
    "#for f in files:\n",
    "#    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92da2d92-5016-44bf-a694-d52e736fa600",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Or check for duplicates, and only ingest new documents  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3e845d2-4893-4c43-afab-d77ebbb9aabd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(len(chunked_documents))\n",
    "print(len(new_chunked_documents))\n",
    "\n",
    "print(chunked_documents[0].metadata)\n",
    "print(new_chunked_documents[0].metadata)\n",
    "\n",
    "delta = [d for d in new_chunked_documents if d.metadata[\"source\"] not in [h.metadata[\"source\"] for h in chunked_documents]]\n",
    "print(len(delta))\n",
    "for d in delta[0:5]:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4d36d84-ebe0-4789-b3a3-0145012f4228",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Now we are going to add only the new documents to the previously created Vector Database index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5907f7f5-d65e-4107-a026-596700e81413",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"We have {len(faissdb.docstore._dict)} documents in the collection\")\n",
    "faissdb.add_documents(delta, embedding=instruct_embeddings,)\n",
    "\n",
    "print(f\"We have {len(faissdb.docstore._dict)} documents in the collection\")\n",
    "faissdb.save_local('/Workspace/ds-academy-research/VectorDB_FAISS/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24fa4d49-6b8c-4ecd-8662-01b9397a859c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Chat with our documents from multiple types via LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d601c698-05d4-4cec-864e-254f56ed1f27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "docsearch = FAISS.load_local(\"/Workspace/ds-academy-research/VectorDB_FAISS/\", instruct_embeddings)\n",
    "print(len(docsearch.index_to_docstore_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22e3c811-3bb5-49ec-932b-fb9d1462c778",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf_qa = ConversationalRetrievalChain.from_llm(chat,\n",
    "                                               retriever=docsearch.as_retriever(),\n",
    "                                               return_source_documents=True,\n",
    "                                               verbose=False\n",
    "                                               )\n",
    "\n",
    "chat_history = []\n",
    "print(f\"---------------------------------------------------------------------------------\")\n",
    "print('Welcome to the DocBot. You are now ready to start interacting with your documents')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "while True:\n",
    "    query = input(f\"Prompt: \")\n",
    "    if query == \"exit\" or query == \"quit\" or query == \"q\" or query == \"f\":\n",
    "        print('Exiting')\n",
    "        break\n",
    "    if query == '':\n",
    "        continue\n",
    "    result = pdf_qa({\"question\": query, \"chat_history\": chat_history})\n",
    "    print(f\"Answer: \" + result[\"answer\"])\n",
    "    chat_history.append((query, result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "813d58f7-da24-4912-94a4-20a2f1dd2e23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4d267e7-05b6-444c-8f38-ff70c3b5437b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (Bonus) Operations among Vector Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df50c732-6cde-4c07-893c-06dac1f5df9b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### You can merge many FAISS vector indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb024e61-2f52-45da-bf71-a26c4a4e2752",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "db1 = FAISS.from_texts([\"Oranges are orange or yellow when ripe\"], embedding=instruct_embeddings,)\n",
    "db2 = FAISS.from_texts([\"Grapes can be red, purple or green\"], embedding=instruct_embeddings,)\n",
    "db3 = FAISS.from_texts([\"Watermelons are green outside, and red inside\"], embedding=instruct_embeddings,)\n",
    "db4 = FAISS.from_texts([\"Lemons are green or yellow\"], embedding=instruct_embeddings,)\n",
    "db5 = FAISS.from_texts([\"Oranges are orange or yellow when ripe\"], embedding=instruct_embeddings,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d9c31cd-66d3-488f-bb0b-987488f108e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(db1.docstore._dict)\n",
    "print(db2.docstore._dict)\n",
    "print(db3.docstore._dict)\n",
    "print(db4.docstore._dict)\n",
    "print(db5.docstore._dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dd52e9e-eae0-4668-b0de-911372a944a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "db1.merge_from(db2)\n",
    "db1.merge_from(db3)\n",
    "db1.merge_from(db4)\n",
    "db1.merge_from(db5)\n",
    "db1.docstore._dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "407ffbc0-2ace-40ee-a449-2498c987c9f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_with_scores = db1.similarity_search_with_score(\"red and green\",)\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"Content: {doc.page_content}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc0f4110-09ff-4eea-9416-d17bea041400",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Another useful thing is to add documents with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eb87d8a-d597-4717-a336-c4a4b4d5e8df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "list_of_documents = [\n",
    "    LangchainDocument(page_content=\"Orange is orange\", metadata=dict(topic=\"Fruit\")),\n",
    "    LangchainDocument(page_content=\"Lemon is green\",  metadata=dict(topic=\"Fruit\")),\n",
    "    LangchainDocument(page_content=\"Watermelon is green\",  metadata=dict(topic=\"Fruit\")),\n",
    "    LangchainDocument(page_content=\"Grapes are red or green\",  metadata=dict(topic=\"Fruit\")),\n",
    "    LangchainDocument(page_content=\"The sun is orange\",  metadata=dict(topic=\"Astronomy\")),\n",
    "    LangchainDocument(page_content=\"Mars is red\",  metadata=dict(topic=\"Astronomy\")),\n",
    "    LangchainDocument(page_content=\"The Earth is blue\",  metadata=dict(topic=\"Astronomy\")),\n",
    "    LangchainDocument(page_content=\"Our planet is Earth\",  metadata=dict(topic=\"Astronomy\")),\n",
    "]\n",
    "db = FAISS.from_documents(list_of_documents, embedding=instruct_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90b823cc-ff94-43ca-b16a-2921527d1705",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "First we make the query without filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0ceadeb-6594-410f-9513-21c8f8c48e16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_with_scores = db.similarity_search_with_score(\"orange\")\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4772c2a1-80b3-428e-8df4-7768d3c124bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we make the same query call but we filter for only topic = \"Fruit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26449cde-0cdd-4bbc-bed3-ebfc1274185c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_with_scores = db.similarity_search_with_score(\"orange\")\n",
    "for doc, score in results_with_scores:\n",
    "    if doc.metadata['topic'] == \"Fruit\":\n",
    "        print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 295794915741311,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "4-Q&A_RAG_ChatGPT_LangChain",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
