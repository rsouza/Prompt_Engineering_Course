{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78c4c6f9-afa3-4284-9d23-3993096b3cfd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -qU openai\n",
    "!pip install -qU langchain\n",
    "!pip install -qU langchain-openai\n",
    "#!pip install -qU langchain-experimental\n",
    "!pip install -q langchain-experimental==0.0.49\n",
    "!pip install -qU langchain-community\n",
    "!pip install -qU langchainhub \n",
    "!pip install -q numexpr\n",
    "#!pip install -q tavily-python==0.3.0 \n",
    "#!pip install -qU tiktoken\n",
    "!pip install wikipedia==1.4.0\n",
    "!pip install -q duckduckgo-search\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68a58098-cc0b-4bef-9db4-1c9747eb08ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# LangChain: Agents  \n",
    "Sources: [1](https://brightinventions.pl/blog/introducing-langchain-agents-tutorial-with-example/), [2](https://levelup.gitconnected.com/using-langchain-csv-agent-for-performing-analytical-tasks-79d073fcbde7), [3](https://python.langchain.com/docs/integrations/retrievers/tavily)  \n",
    "\n",
    "#### The core idea of agents is to use an LLM to choose a sequence of actions to take. \n",
    "+ In chains, a sequence of actions is hardcoded (in code).  \n",
    "+ In agents, a language model is used as a reasoning engine to determine which actions to take and in which order.\n",
    "\n",
    "![Agents vs Chains](https://brightinventions.pl/static/98578848ff94ebbf2fd58883fbe3e780/d9199/llm_agents_loop.png)  \n",
    "\n",
    "Agents are crucial for handling tasks ranging from simple automated responses to complex, context-aware interactions.\n",
    "For example, you may have an agent integrated with Google Search, Wikipedia and OpenAI LLM. With given agent tools, they can search for results in Google, then use retrieved context in Wikipedia tool to find detailed information and expand the context. Bear in mind that you have to put clearly defined instructions to make sure that the agent will invoke tools in a proper order.\n",
    "\n",
    "![Architecture](https://brightinventions.pl/static/5acdb7bfc6e66980738c29a5e9c96d76/d9199/llm_agents.png)\n",
    "\n",
    "\n",
    "Examples of Agents:\n",
    "1. Web search tool: You can easily add different types of web search as an available action to your agent. It might be Google Search, Tavily Search, DuckDuckGo and many others.\n",
    "2. Embedding search in vector database: You can create a tool from retriever and describe it as you need, so the agent will use this tool to get some kind of data doing e.g. similarity check and embedding model.\n",
    "3. Doing some actions: Your agent can be multipurpose one. For example, it might be searching for some kind of information on the internet, doing the reasoning step, and after all invoking action to create a Jira issue.\n",
    "4. API integration tool: There are many API integration done for the LangChain framework, all you need to do is take the API key, install the package and attach the tool to the agent.\n",
    "5. Custom tool: You can write your own tool, refer to the documentation to see how to do it. It might be integration with your internal API, your document system, and many others!\n",
    "\n",
    "LangChain Agent types: LangChain categorizes agents based on several dimensions:  \n",
    "+ model type;\n",
    "+ support for chat history;\n",
    "+ multi-input tools;\n",
    "+ parallel function calling;\n",
    "+ required model parameters.\n",
    "\n",
    "It is important to choose the option that fits to your use case:  \n",
    "1. OpenAI functions: There are certain models fine-tuned where input is a bit different than usual. There are special functions that can be called and the role of this agent is to determine when it should be invoked. This agent is designed to work with this kind of OpenAI model. It supports chat history.  \n",
    "2. OpenAI tools: This agent is designed to work with OpenAI tools, so its role is to interact and determine whether to use e.g. image generation tool or another built-in one. The main difference between OpenAI function is the fact that the function is trying to find the best fitting algorithm/part of an algorithm to do better reasoning, while OpenAI tool is about built-in tools like image generation, and executing code. It supports chat history.  \n",
    "3. XML Agent: There are models, where reasoning/writing XML is on a very advanced level (a good example is Anthropic Claude's model). If you're operating on XML files, that might be the right one to be considered. It supports chat history.  \n",
    "4. JSON Chat Agent: Several LLMs available in the market are particularly handy when it comes to reading JSON. JSON is also a very common standard of some e.g. entity representation. If you're building some kind of integration that operates on JSON files and the model is supporting it, you can try to use this agent. It supports chat history.  \n",
    "5. Structured chat: Intended for multi-input tools. It supports chat history.  \n",
    "6. ReAct agent: Made for simple models (LLM - not conversational). It supports chat history.  \n",
    "7. Self-ask with search: This kind of agent supports only one tool as an input. The main goal is to divide your query into smaller ones, use tools to get the answer, and then combine it into a full answer to your question. This kind of agent doesnâ€™t support chat history.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5c651ef-da1d-403e-a5bf-1ad6e19a8a96",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Example Notebooks:\n",
    "+ https://github.com/langchain-ai/langchain/blob/master/cookbook/two_agent_debate_tools.ipynb  \n",
    "+ https://github.com/langchain-ai/langchain/blob/master/cookbook/databricks_sql_db.ipynb  \n",
    "+ https://github.com/langchain-ai/langchain/blob/master/cookbook/baby_agi.ipynb  \n",
    "+ https://github.com/langchain-ai/langchain/blob/master/cookbook/baby_agi_with_agent.ipynb  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23929b62-444f-4107-ba70-01f4134457eb",
     "showTitle": false,
     "title": ""
    },
    "height": 115,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "#from funcy import lcat, lmap, linvoke\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## Langchain LLM Objects\n",
    "import openai\n",
    "import langchain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_openai import AzureOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "## Langchain Chains \n",
    "#from langchain.chains import ConversationChain\n",
    "#from langchain.chains import LLMChain\n",
    "#from langchain.chains import ConversationChain\n",
    "#from langchain.chains import ConversationalRetrievalChain\n",
    "#from langchain.chains import RetrievalQA\n",
    "#from langchain.chains.mapreduce import MapReduceChain\n",
    "#from langchain.chains.summarize import load_summarize_chain\n",
    "#from langchain.chains.question_answering import load_qa_chain\n",
    "#from langchain.chains import SimpleSequentialChain\n",
    "#from langchain.chains import SequentialChain\n",
    "#from langchain.chains.router import MultiPromptChain\n",
    "#from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "#from langchain.chains import PALChain\n",
    "#from libs.experimental.langchain.experimental.pal_chain.base import PALChain\n",
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "## Langchain Schemas \n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "## Langchain Agents  \n",
    "from langchain import hub\n",
    "from langchain.agents import tool\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.python import PythonREPL\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_csv_agent\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.agents.agent_toolkits import create_spark_dataframe_agent\n",
    "\n",
    "#from langchain.tools.tavily_search import TavilySearchResults\n",
    "#from langchain.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "azure_endpoint = \"https://rg-rbi-aa-aitest-dsacademy.openai.azure.com/\"\n",
    "#azure_endpoint = \"https://chatgpt-summarization.openai.azure.com/\"\n",
    "\n",
    "openai.api_version = \"2023-07-01-preview\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "deployment_name = \"model-gpt-35-turbo\"\n",
    "openai_model_name = \"gpt-35-turbo\"\n",
    "\n",
    "client = AzureOpenAI(api_key=openai.api_key,\n",
    "                     api_version=openai.api_version,\n",
    "                     azure_endpoint=azure_endpoint,\n",
    "                     )\n",
    "\n",
    "chat = AzureChatOpenAI(azure_endpoint=azure_endpoint,\n",
    "                       openai_api_version=openai.api_version,\n",
    "                       deployment_name=deployment_name,\n",
    "                       openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "                       openai_api_type=openai.api_type,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a565341-fbc4-4579-9b30-a4181958eea6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Simple Agents  \n",
    "\n",
    "##### Most of the time, the LLM gives a satisfactory answer to what we need:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77a4faa3-ee77-41d1-a9c6-cb8e6d2051ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"What is the sixt element in the fibonacci sequence\"\n",
    "response = chat(messages=[HumanMessage(content=query)])\n",
    "print(response.content)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c936cf15-0f17-4da1-badd-0a3dbdc39750",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### But in this second case, only the Python agent is able to figure out how to provide the missing information (the year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e37a2e4-13f5-4e39-a203-99c512a00d5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"We are in 2024. In which day of week the Austrian National Holiday will be next year?\"\n",
    "response = chat(messages=[HumanMessage(content=query)])\n",
    "print(response.content)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13464a76-2bac-49ee-bd08-67e535e6c54a",
     "showTitle": false,
     "title": ""
    },
    "height": 98
   },
   "outputs": [],
   "source": [
    "agent = create_python_agent(chat, tool=PythonREPLTool(), verbose=True, handle_parsing_errors=True)\n",
    "#langchain.debug=True\n",
    "agent.run(query) \n",
    "#langchain.debug=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6bd7c08-f34f-4860-b05b-67f2466d9ba5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### We can use a Python Agent for different tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72d9f2d4-a7a5-4bcf-8af1-6a3eecb2eccd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Task #1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f67f745-cc44-4c65-84f6-a00372f5f04d",
     "showTitle": false,
     "title": ""
    },
    "height": 149
   },
   "outputs": [],
   "source": [
    "customer_list = [[\"Renato\", \"Fiacchini\"], \n",
    "                 [\"Charles\", \"Aznavour\"],\n",
    "                 [\"Bernhard\", \"Thomas\"],\n",
    "                 [\"Joel\", \"Billy\"], \n",
    "                 [\"David\",\"Goliah\"], \n",
    "                 [\"Brani\",\"Acer\"],\n",
    "                 [\"Georg\",\"Curious\"]\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bf7ef68-a082-4be7-a7fd-ee6205a6004e",
     "showTitle": false,
     "title": ""
    },
    "height": 64
   },
   "outputs": [],
   "source": [
    "#langchain.debug=True\n",
    "agent.run(f\"\"\"Sort these customers by last name and then first name and print the output: {customer_list}\"\"\") \n",
    "#langchain.debug=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59e18a27-61e8-4a34-bddb-cb87dd0b2075",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Task #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "353e9d55-d753-441a-a4c5-2d50637ac211",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "agent.run(f\"\"\"What are the char codes for each name on the list in hexadecimal: {customer_list}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81cbb53e-3ebd-44c0-96e6-25dc61dc275b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### We have to be aware that the models are incorporating themselves math functionalities.  \n",
    "See the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da4bd3f2-21c4-40e3-865e-39f4a24bdbbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def collatz(n):\n",
    "    print(n)\n",
    "    while n != 1:\n",
    "        if n%2 == 0:\n",
    "            n /= 2\n",
    "        else:\n",
    "            n = (3*n) + 1\n",
    "        print(int(n), end=\" \")\n",
    "\n",
    "collatz(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6be3098-8ac6-45ec-9a10-d9767ea5e404",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query1 = \"Print the Collatz sequence for the number 45\"\n",
    "query2 = \"What is the biggest prime number below 1 billion?\"\n",
    "response = chat(messages=[HumanMessage(content=query1)])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76341771-00b0-41b9-bc42-5bb8fda76876",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "langchain.debug=True\n",
    "print(agent.run(query1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21f9d533-c816-4ec9-aaca-8460e67986c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Loading specific Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c1c841d-5015-42f5-a6c5-8d1a46a7e552",
     "showTitle": false,
     "title": ""
    },
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = load_tools([\"llm-math\",\"wikipedia\"], llm=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dbe8513-b2f7-4714-8335-b5eabe96d328",
     "showTitle": false,
     "title": ""
    },
    "height": 115,
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent= initialize_agent(tools,\n",
    "                        chat,\n",
    "                        agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "                        handle_parsing_errors=True,\n",
    "                        verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fea18f58-d16a-4cc0-b1ed-091dccd70fe0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Mind the message ponting out to the new architecture:  \n",
    "LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0752c800-6715-467b-85e8-9b70e41cda3c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1 - Wikipedia (blocked by Firewall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a01e422-4e79-4c37-8ce2-090a30e96e35",
     "showTitle": false,
     "title": ""
    },
    "height": 98
   },
   "outputs": [],
   "source": [
    "#langchain.debug=True\n",
    "question = \"Tom M. Mitchell is an American computer scientist \\\n",
    "and the Founders University Professor at Carnegie Mellon University (CMU)\\\n",
    "what book did he write?\"\n",
    "result = agent(question)\n",
    "#langchain.debug=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17ce8e74-4757-4857-a70c-645a256df445",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2 - LLM-Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc7bdcd2-5515-44ea-8822-0840305492d4",
     "showTitle": false,
     "title": ""
    },
    "height": 30
   },
   "outputs": [],
   "source": [
    "langchain.debug=False\n",
    "agent(\"How much is 25% of 300?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f141b75-7403-4647-83bc-e551e47c9468",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3 - Duck Duck Go (blocked by Firewall)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "873ce857-ebbc-4909-bf24-7f886f4f10e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "search = DuckDuckGoSearchRun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25e39465-c92b-4705-a14f-3109faf8188a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "search.run(\"Obama's first name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43ebbadb-1b35-495f-9c6a-8e0aa0a771e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4 - Pandas Agent  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79ccb6bc-fc3d-4eb8-a72b-5e955af06af3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../Data/Data.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e58b8820-c44c-408d-aef6-174156d55382",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "langchain.debug=False\n",
    "\n",
    "pandas_agent = create_pandas_dataframe_agent(llm=chat, \n",
    "                                             df=df, \n",
    "                                             verbose=True, \n",
    "                                             agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5824a16-7693-48e6-95e0-3fb77128ce64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_agent.run(\"how many rows are there?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a181d8db-2f73-40b5-92d4-dbf9a75bd153",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_agent.run(\"Is there a review about L'Or? Please show me it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b54198a-94e5-4d0c-a65e-2a9129f454e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define your own tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10421718-b672-4323-a252-8534fde6ec47",
     "showTitle": false,
     "title": ""
    },
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "204f6009-fa64-4727-ada5-e6b60f524ca9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def my_color(text: str) -> str:\n",
    "    \"\"\"I use different colors of T-shirts\n",
    "    depending on the day of month.\n",
    "    I use White in the even days and\n",
    "    I use Blue in the odd days\n",
    "    This function tells me which color \n",
    "    to use\"\"\"\n",
    "    day = date.today().day\n",
    "    if day%2 == 0:\n",
    "        return \"White\"\n",
    "    return \"Blue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72fdbff7-0314-40f3-8419-88bb541ea5c7",
     "showTitle": false,
     "title": ""
    },
    "height": 115,
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent= initialize_agent([my_color],\n",
    "                        chat,\n",
    "                        agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "                        handle_parsing_errors=True,\n",
    "                        verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11d7ba08-887b-4db7-9809-d39a0866b595",
     "showTitle": false,
     "title": ""
    },
    "height": 81
   },
   "outputs": [],
   "source": [
    "langchain.debug=False\n",
    "\n",
    "try:\n",
    "    result = agent(\"What t-shirt color should I use today\")\n",
    "    print(result)\n",
    "except: \n",
    "    print(\"exception on external access\")\n",
    "\n",
    "langchain.debug=False"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "6-Agents (old architecture)",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
